{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWwLze7G65JNA9F5cHmN6D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwonjihan/ML-teamproject/blob/developtemp/SeongYeomByeon/Bert_pretraining_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##개요##\n",
        "- Bookcorpus 데이터셋에서 20000개 텍스트를 추출하여 진행\n",
        "- 구조 개선의 목적이 학습의 가속화, 즉 동일한 환경에서 더 빨리, 더 작은 Loss로 수렴하는 것이기 때문에 데이터셋 크기는 크게 상관 없다고 생각했음.\n",
        "\n",
        "##결과(Test Perplexity)##\n",
        "- 오리지널 모델 / 키워드 스코어 / 임베디드 유사도\n",
        "- 1차 : 743.0725 / 773.6405 / 829.0847\n",
        "- 2차 : 813.6715 / 832.9860 / 783.4184\n",
        "- 3차 : 900.8481 / 802.1566 / 750.9805\n",
        "\n",
        "-> 테스트 결과는, pretraining task에 쓰기에는 너무 작은 데이터셋에서의 결과이기에 의미가 없다고 생각함. Accuracy를 지표로 사용하지 않은 이유도 동일\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "gjUWw3EG0and"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[실험 결과]**\n",
        "\n",
        "- 선형적 상관관계가 보이지 않음.\n",
        "- '왜?' 실패했을까\n",
        "\n",
        "# **[공통 요인]**\n",
        "\n",
        "**1. 데이터셋 크기의 한계**\n",
        "> - BERT같은 대규모 모델의 Pretraining은 통상적으로 수백만 개의 문서로 구성된 방대한 데이터셋을 필요로 함.\n",
        "- 본 연구에서는 20000개의 데이터셋을 사용하였으며, 이는 모델이 다양한 패턴과 문맥을 충분히 학습하는 데 불충분함.\n",
        "-  문맥을 학습하고 일반화하는 데 필요한 충분한 정보를 제공하지 못하여, 애초에 구조 개선의 효과가 반영될 수 없었을지도?\n",
        "\n",
        "**2. 학습 시간의 부족**\n",
        "> - BERT 모델은 일반적으로 수십에서 수백 시간의 학습을 통해 높은 성능을 달성함.\n",
        "- 본 연구에서는 제한된 학습 시간으로 인해 모델이 충분히 학습되지 못했음.\n",
        "- 구조 개선이 학습 규모에 상관 없이 학습 속도를 가속화하는데 도움이 될거라 생각했지만, 충분한 학습 시간이 주어지지 않은 상태에서 새로운 방법을 적용하는 것 자체가 그 효과를 제대로 평가할 수 없게 만든 것일지도?\n",
        "\n",
        "# **[아이디어 1: Embedding 유사도 반영]**\n",
        "**1. 임베딩 유사도 자체의 효과 미미**\n",
        ">- 상위 레이어에서의 임베딩 유사도 반영은 토큰 간의 유사성을 강조하려는 의도였음.\n",
        "- 하지만 실제로는 이미 충분히 학습된 토큰 간의 관계를 크게 변화시키지 못함.\n",
        "- 이로 인해, 임베딩 유사도를 추가하더라도 기존 attention 메커니즘에 비해 의미 있는 성능 향상을 가져오지 못한걸까?\n",
        "\n",
        "**2. 이미 높은 모델의 완성도**\n",
        "> - BERT 모델은 원래도 강한 적응력을 가지고 있어, 추가적인 유사도 반영이 없어도 충분히 문맥적 의미를 이미 학습함.\n",
        "- 그렇기에, 새로운 유사도 행렬을 반영한 것이 이미 반영된 것을 다시 반영하는거라 의미가 없었던 것 같음.\n",
        "\n",
        "# **[아이디어 2: Keyword Score 반영]**\n",
        "**1. 키워드 선택 오류**\n",
        "> - CLS 토큰과의 Similarity를 통해 Keyword Score를 계산하는 방식이, 문맥에 따라 키워드를 잘못 선택한다면?\n",
        "- 문장의 대표적인 단어가 항상 높은 Similarity를 갖는다는 가정 자체가 틀렸다면,  Keyword Score 의 계산이 부정확한 것이 당연.\n",
        "**2. CLS 토큰의 한계**\n",
        "> - CLS 토큰은 문장의 전체적인 맥락을 나타내지만, 이게 무조건 개별 단어의 문맥적 중요도를 정확하게 반영한다는 뜻은 아님.\n",
        "-  CLS 토큰과의 Similarity를 통해 계산된 Keyword Score가, 단어의 실제 중요도를 반영하는 것이 아닐지도?\n",
        "**3. 이미 강력한 Attention 메커니즘**\n",
        "> - Attention 메커니즘은 그 자체로도모든 단어의 중요도를 학습할 수 있는 강력한 능력을 갖추고 있음.\n",
        "-  이전 아이디어의 예상 실패 요인과 유사하게, Keyword Score를 추가하더라도, 모델이 본래의 Attention 메커니즘을 통해 이미 충분한 성능을 발휘하고 있었기 때문에 추가적인 가중치 적용이 큰 변화를 가져오지 못했던 걸지도?\n",
        "\n",
        "# **[결론]**\n",
        "- 본 연구에서는 BERT 모델의 Pretraining 성능을 개선하기 위한 두 가지 아이디어를 제시하고 그 효과를 평가하였음.\n",
        "- 첫 번째 아이디어는 상위 레이어의 Attention에 Embedding 유사도를 반영하는 것이었고, 두 번째 아이디어는 Attention Score에 Keyword Score를 가중치로 반영하는 것이었음.\n",
        "- 그러나 실험 결과, 기존 모델과 비교하여 성능 향상의 차이가 거의 없음을 확인할 수 있었음.\n",
        "\n",
        "- 이러한 결과는 다양한 요인이 종합적으로 작용한 결과라고 생각함.\n",
        "- 상위 레이어의 Attention에 임베딩 유사도를 반영하는 아이디어는 토큰 간의 문맥적 유사성을 강화하려는 의도였지만, 이미 강력한 적응력을 가진 BERT 모델은 이러한 변경 없이도 충분한 문맥적 의미를 학습할 수 있었을 가능성이 있음.\n",
        "- 또한, Keyword Score를 가중치로 반영하는 방법 역시 CLS 토큰과의 Similarity로 계산된 Keyword Score가 단어의 실제 중요도를 충분히 반영하지 못하여 성능 향상이 제한적이었을 수 있었다고 생각함.\n",
        "- 공통적으로도, 제한된 데이터셋 크기와 학습 시간도 우리의 생각보다 학습 동향에 지대한 영향을 미치는 요소일 가능성이 매우 큼.\n",
        "\n",
        "- 가능하다면, 보다 대규모의 데이터셋과 장기간의 학습 시간을 확보함으로써, 모델의 학습 환경을 우선적으로 개선해야 할 필요를 느낌.\n",
        "- 또한, 여러 문맥에서의 효과를 평가할 수 있는 추가적인 실험을 통해 우리가 제시했던 방법론의 유효성을 보다 정확하게 검증해야 함.\n",
        "\n",
        " # ** 결론 - 시사점 **\n",
        "- 그럼에도, BERT 모델의 구조를 깊이 이해하고 내부적인 구현을 연구하는 과정에서 다양한 아이디어를 시도해 본 경험은 전반적인 NLP 작업과 Transformer 기반 구조에 대한 이해를 증진시키는 데 큰 도움이 되었음.\n",
        "- 본 연구 과정은 단순히 모델을 사용하는 것을 넘어, 모델 성능을 최적화하고 새로운 접근 방식을 탐색하는 데 있어 중요한 기초를 제공하였으며, 이는 급변하는 AI 발전 동향 속에서 여러 모델에 대한 깊은 이해와 코드 레벨에서의 구현에 대해 유리한 시선을 갖게 해 줬다고 자부할 수 있음."
      ],
      "metadata": {
        "id": "aO1ssG_Q3bSt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH2J4DwaOQbz",
        "outputId": "d2079278-a338-4bc2-ead7-e56a649d3cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import RandomSampler, DataLoader, random_split\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
        "from typing import Optional, Tuple\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from transformers.modeling_outputs import ModelOutput, MaskedLMOutput"
      ],
      "metadata": {
        "id": "qmDH4cV4OVl5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THImrAIGOVo2",
        "outputId": "fd5bbbc5-cb22-4eb8-d53f-2d3d8dbf210c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activation function 불러오기\n",
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": torch.nn.functional.silu}"
      ],
      "metadata": {
        "id": "7wqQgqSJOV0C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    vocab_size=30522\n",
        "    hidden_size=768\n",
        "    num_hidden_layers=8\n",
        "    num_attention_heads=12\n",
        "    intermediate_size=3072\n",
        "    hidden_act=\"gelu\"\n",
        "    hidden_dropout_prob=0.1\n",
        "    attention_probs_dropout_prob=0.1\n",
        "    max_position_embeddings=512\n",
        "    type_vocab_size=2\n",
        "    initializer_range=0.02\n",
        "    layer_norm_eps=1e-12\n",
        "    pad_token_id=0\n",
        "    gradient_checkpointing=False\n",
        "    position_embedding_type=\"absolute\"\n",
        "    use_cache=True\n",
        "    is_decoder = False\n",
        "\n",
        "# BERT 입력 임베딩 생성 클래스\n",
        "class BertEmbeddings(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        # 단어 임베딩, 위치 임베딩, 토큰 타입 임베딩\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "        # 레이어 정규화와 드롭아웃\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False)\n",
        "        self.register_buffer(\"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False)\n",
        "\n",
        "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0):\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, past_key_values_length: seq_length + past_key_values_length]\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        # 입력 임베딩 생성\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "# 셀프 어텐션 구현 클래스\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config, position_embedding_type=None):\n",
        "        super().__init__()\n",
        "        # hidden_size가 num_attention_heads의 배수가 아니면 오류 발생\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        # 어텐션 헤드의 수와 각 헤드의 크기, 전체 헤드 크기 설정\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(\n",
        "            config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        # Query, Key, Value 행렬 정의\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        # 드롭아웃 레이어 정의\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        # 위치 임베딩 유형 설정\n",
        "        self.position_embedding_type = position_embedding_type or getattr(\n",
        "            config, \"position_embedding_type\", \"absolute\"\n",
        "        )\n",
        "        # 상대적 위치 임베딩을 사용하는 경우, 위치 임베딩 레이어 정의\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(\n",
        "                2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        # 디코더인지 여부 설정\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # 텐서의 크기 변환\n",
        "        new_x_shape = x.size()[\n",
        "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        # 텐서의 차원 변경 [batch_size, num_heads, seq_len, head_size]\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        # Query 레이어 계산\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # 크로스 어텐션인지 여부 확인\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # 과거의 k, v 값을 재사용 (크로스 어텐션)\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            # 인코더의 키와 값을 사용하여 크로스 어텐션 수행\n",
        "            key_layer = self.transpose_for_scores(\n",
        "                self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(\n",
        "                self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            # 과거의 k, v 값을 현재의 k, v와 결합 (디코더의 셀프 어텐션)\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            # 현재의 히든 스테이트에서 키와 값을 계산 (셀프 어텐션)\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        # Query 레이어 변환\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # 캐시를 사용할지 여부 설정\n",
        "        use_cache = past_key_value is not None\n",
        "        if self.is_decoder:\n",
        "            # 디코더인 경우, 키와 값을 캐싱\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Query와 Key의 내적(dot product)을 통해 어텐션 스코어 계산\n",
        "        attention_scores = torch.matmul(\n",
        "            query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            # 상대적 위치 임베딩을 사용하는 경우\n",
        "            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
        "            if use_cache:\n",
        "                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n",
        "                    -1, 1\n",
        "                )\n",
        "            else:\n",
        "                position_ids_l = torch.arange(\n",
        "                    query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(\n",
        "                key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "\n",
        "            # 거리 임베딩 계산\n",
        "            positional_embedding = self.distance_embedding(\n",
        "                distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(\n",
        "                dtype=query_layer.dtype)  # fp16 호환성\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                # 상대적 위치 임베딩을 쿼리에 적용\n",
        "                relative_position_scores = torch.einsum(\n",
        "                    \"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                # 상대적 위치 임베딩을 쿼리와 키에 적용\n",
        "                relative_position_scores_query = torch.einsum(\n",
        "                    \"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\n",
        "                    \"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + \\\n",
        "                    relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        # 어텐션 스코어를 정규화\n",
        "        attention_scores = attention_scores / \\\n",
        "            math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # 어텐션 마스크 적용\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # 어텐션 스코어를 확률로 변환\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # 드롭아웃 적용\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # 헤드 마스크 적용\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        # 컨텍스트 레이어 계산\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        # 텐서의 크기 변환 및 재배치\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[\n",
        "            :-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        # 출력 생성\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (\n",
        "            context_layer,)\n",
        "\n",
        "        # 디코더인 경우, past_key_value를 출력에 포함\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs\n",
        "\n",
        "# 셀프 어텐션 출력 처리 클래스\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # 드롭아웃, 레이어 정규화, 잔차 연결 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "# 어텐션 메커니즘 클래스\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        # 셀프 어텐션 및 출력 계산\n",
        "        self_outputs = self.self(\n",
        "            input_tensor,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], input_tensor)\n",
        "        outputs = (attention_output,) + self_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "# 중간 레이어 활성화 함수 클래스\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # 중간 레이어 활성화 함수 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "# 중간 레이어 출력 처리 클래스\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # 드롭아웃, 레이어 정규화, 잔차 연결 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.Layer\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # 드롭아웃, 레이어 정규화, 잔차 연결 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "# 하나의 BERT 레이어를 구현하는 클래스\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        # 어텐션과 출력 계산\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        layer_output = self.output(self.intermediate(attention_output), attention_output)\n",
        "        outputs = (layer_output,) + self_attention_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "# 여러 BERT 레이어를 포함하는 인코더 클래스\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        return (hidden_states, all_hidden_states, all_attentions)\n",
        "\n",
        "# 첫 번째 토큰의 출력을 풀링하는 클래스\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # 첫 번째 토큰의 텐서를 사용해 풀링 출력 생성\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "# 전체 BERT 모델을 구현하는 클래스\n",
        "class BertModel(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
        "        # 입력 텐서의 크기 확인\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"input_ids 혹은 inputs_embeds 둘 중 하나의 형식으로만 입력해야 합니다.\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"input_ids 또는 inputs_embeds의 형식이어야 합니다.\")\n",
        "\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(input_shape, device=device)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        # 임베딩 출력 계산\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "        # 인코더 출력 계산\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        return sequence_output, pooled_output\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "        self.LayerNorm = nn.LayerNorm(\n",
        "            config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "        self.decoder = nn.Linear(\n",
        "            config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "class BertForPreTraining(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
        "        return_dict = return_dict if return_dict is not None else True\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return MaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=None,\n",
        "            attentions=None,\n",
        "        )"
      ],
      "metadata": {
        "id": "OrISPhxjORcy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CSVDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size=512):\n",
        "        # 데이터셋 로드\n",
        "        df = pd.read_csv(file_path)\n",
        "        text = \" \".join(df[\"text\"].tolist())\n",
        "        self.examples = []\n",
        "\n",
        "        # 토크나이즈 및 블록 크기로 자르기\n",
        "        for i in tqdm(range(0, len(text) - block_size, block_size), desc=\"Tokenizing text\"):\n",
        "            chunk = text[i:i + block_size]\n",
        "            inputs = tokenizer(chunk, add_special_tokens=True, max_length=block_size, truncation=True, return_tensors=\"pt\", padding=\"max_length\")\n",
        "            inputs['labels'] = inputs.input_ids.clone()\n",
        "            self.examples.append(inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return {key: val.squeeze(0) for key, val in self.examples[i].items()}\n",
        "\n",
        "# 데이터셋 저장 함수\n",
        "def save_dataset(dataset, file_path):\n",
        "    torch.save(dataset, file_path)\n",
        "\n",
        "# 데이터셋 로드 함수\n",
        "def load_dataset(file_path):\n",
        "    return torch.load(file_path)\n",
        "\n",
        "# 메인 함수\n",
        "def main():\n",
        "    # 데이터셋 경로 지정\n",
        "    data_dir = \"/content/drive/MyDrive/bookcorpus_reduced.csv\"\n",
        "    file_path = os.path.join(data_dir)\n",
        "    processed_data_path = os.path.join(\"/content/drive/MyDrive/processed_dataset.pt\")\n",
        "\n",
        "    # 토크나이저 초기화\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # 데이터셋 전처리 및 저장\n",
        "    if not os.path.exists(processed_data_path):\n",
        "        full_dataset = CSVDataset(file_path, tokenizer)\n",
        "        save_dataset(full_dataset, processed_data_path)\n",
        "        print(\"전처리된 데이터셋 저장 완료.\")\n",
        "    else:\n",
        "        full_dataset = load_dataset(processed_data_path)\n",
        "        print(\"전처리된 데이터셋 불러오기 완료.\")\n",
        "\n",
        "    # 데이터셋 분할 (80% train, 10% validation, 10% test)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = int(0.1 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size - val_size\n",
        "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    # 데이터 로더 생성\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=8, collate_fn=data_collator)\n",
        "    validation_dataloader = DataLoader(val_dataset, batch_size=8, collate_fn=data_collator)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=data_collator)\n",
        "\n",
        "    return train_dataloader, validation_dataloader, test_dataloader, tokenizer\n",
        "\n",
        "train_dataloader, validation_dataloader, test_dataloader, tokenizer = main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4_RI9yvUWcM",
        "outputId": "a893b50a-3e9a-4de6-92d3-72099d4eb2a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing text: 100%|██████████| 2935/2935 [00:07<00:00, 393.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리된 데이터셋 저장 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(loss):\n",
        "    return math.exp(loss)\n",
        "\n",
        "def train(model, train_dataloader, validation_dataloader, tokenizer, device, epochs=3):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "    model.to(device)  # 모델을 지정된 장치로 이동\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)  # 옵티마이저 설정\n",
        "\n",
        "    # 에포크만큼 반복\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # 데이터 로더에서 미니배치를 하나씩 가져와서 학습\n",
        "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
        "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # 10번째 배치마다 현재 손실과 퍼플렉서티 출력\n",
        "            if step % 10 == 0 and step != 0:\n",
        "                current_loss = running_loss / (step + 1)\n",
        "                current_perplexity = calculate_perplexity(current_loss)\n",
        "                print(f\"Batch {step}, Loss: {current_loss:.4f}, Perplexity: {current_perplexity:.4f}\")\n",
        "\n",
        "        # 에포크의 손실과 퍼플렉서티 출력\n",
        "        epoch_loss = running_loss / len(train_dataloader)\n",
        "        epoch_perplexity = calculate_perplexity(epoch_loss)\n",
        "        print(f\"Epoch {epoch + 1} Loss: {epoch_loss:.4f}, Perplexity: {epoch_perplexity:.4f}\")\n",
        "\n",
        "        # 검증 단계\n",
        "        model.eval()  # 모델을 평가 모드로 설정\n",
        "        validation_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for step, batch in enumerate(tqdm(validation_dataloader, desc=\"Validating\")):\n",
        "                inputs = {key: val.to(device) for key, val in batch.items()}\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                validation_loss += loss.item()\n",
        "\n",
        "        # 검증 손실과 퍼플렉서티 출력\n",
        "        epoch_val_loss = validation_loss / len(validation_dataloader)\n",
        "        epoch_val_perplexity = calculate_perplexity(epoch_val_loss)\n",
        "        print(f\"Validation Loss: {epoch_val_loss:.4f}, Perplexity: {epoch_val_perplexity:.4f}\")\n",
        "        model.train()  # 평가 후에 모델을 다시 학습 모드로 설정\n",
        "\n",
        "def test(model, test_dataloader, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    test_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
        "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    # 테스트 데이터셋의 퍼플렉서티 출력\n",
        "    test_loss = test_loss / len(test_dataloader)\n",
        "    test_perplexity = calculate_perplexity(test_loss)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Perplexity: {test_perplexity:.4f}\")\n",
        "\n",
        "# 데이터셋과 데이터 로더 설정\n",
        "# (이 부분은 주어진 코드에서 제공된 데이터 로더를 사용한다고 가정합니다)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForPreTraining(Config)\n",
        "train(model, train_dataloader, validation_dataloader, tokenizer, device, epochs=3)\n",
        "test(model, test_dataloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOK6n7-_YOCz",
        "outputId": "d2571038-dcc7-476e-edbb-e05212d7b229"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▎         | 11/294 [00:07<03:05,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 10, Loss: 9.7733, Perplexity: 17558.7232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   7%|▋         | 21/294 [00:13<02:57,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 20, Loss: 9.4153, Perplexity: 12275.0556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  11%|█         | 31/294 [00:20<02:52,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 30, Loss: 9.2125, Perplexity: 10021.6876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|█▍        | 41/294 [00:26<02:46,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 40, Loss: 9.0108, Perplexity: 8191.1808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|█▋        | 51/294 [00:33<02:40,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 50, Loss: 8.8601, Perplexity: 7045.1860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|██        | 61/294 [00:40<02:34,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 60, Loss: 8.6789, Perplexity: 5877.7721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|██▍       | 71/294 [00:46<02:28,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 70, Loss: 8.5383, Perplexity: 5106.8631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|██▊       | 81/294 [00:53<02:22,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 80, Loss: 8.4033, Perplexity: 4461.5668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  31%|███       | 91/294 [01:00<02:15,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 90, Loss: 8.2781, Perplexity: 3936.8465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|███▍      | 101/294 [01:06<02:10,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100, Loss: 8.1780, Perplexity: 3561.6295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 111/294 [01:13<02:03,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 110, Loss: 8.0999, Perplexity: 3294.1925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  41%|████      | 121/294 [01:20<01:57,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 120, Loss: 8.0218, Perplexity: 3046.6627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  45%|████▍     | 131/294 [01:27<01:50,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 130, Loss: 7.9538, Perplexity: 2846.2617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|████▊     | 141/294 [01:34<01:44,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 140, Loss: 7.8981, Perplexity: 2692.1606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  51%|█████▏    | 151/294 [01:40<01:37,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 150, Loss: 7.8489, Perplexity: 2562.9714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  55%|█████▍    | 161/294 [01:47<01:31,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 160, Loss: 7.8021, Perplexity: 2445.7905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  58%|█████▊    | 171/294 [01:54<01:23,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 170, Loss: 7.7545, Perplexity: 2331.9988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  62%|██████▏   | 181/294 [02:01<01:17,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 180, Loss: 7.7147, Perplexity: 2240.9889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  65%|██████▍   | 191/294 [02:08<01:10,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 190, Loss: 7.6751, Perplexity: 2154.0768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  68%|██████▊   | 201/294 [02:15<01:04,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200, Loss: 7.6401, Perplexity: 2080.0178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|███████▏  | 211/294 [02:21<00:56,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 210, Loss: 7.6057, Perplexity: 2009.6350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  75%|███████▌  | 221/294 [02:28<00:50,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 220, Loss: 7.5767, Perplexity: 1952.2515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  79%|███████▊  | 231/294 [02:35<00:43,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 230, Loss: 7.5465, Perplexity: 1894.1768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  82%|████████▏ | 241/294 [02:42<00:37,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 240, Loss: 7.5184, Perplexity: 1841.6446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  85%|████████▌ | 251/294 [02:49<00:29,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 250, Loss: 7.4964, Perplexity: 1801.4707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  89%|████████▉ | 261/294 [02:56<00:23,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 260, Loss: 7.4735, Perplexity: 1760.8370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  92%|█████████▏| 271/294 [03:03<00:15,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 270, Loss: 7.4531, Perplexity: 1725.1581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▌| 281/294 [03:10<00:09,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 280, Loss: 7.4348, Perplexity: 1693.8510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  99%|█████████▉| 291/294 [03:17<00:02,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 290, Loss: 7.4109, Perplexity: 1653.9823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 294/294 [03:19<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 7.4038, Perplexity: 1642.1711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 37/37 [00:09<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 6.8968, Perplexity: 989.0679\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▎         | 11/294 [00:07<03:16,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 10, Loss: 6.7799, Perplexity: 879.9907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   7%|▋         | 21/294 [00:14<03:11,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 20, Loss: 6.7789, Perplexity: 879.1001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  11%|█         | 31/294 [00:21<03:04,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 30, Loss: 6.8013, Perplexity: 899.0445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|█▍        | 41/294 [00:28<02:57,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 40, Loss: 6.8015, Perplexity: 899.1852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|█▋        | 51/294 [00:35<02:50,  1.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 50, Loss: 6.8070, Perplexity: 904.1950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|██        | 61/294 [00:42<02:44,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 60, Loss: 6.8172, Perplexity: 913.3914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|██▍       | 71/294 [00:49<02:37,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 70, Loss: 6.8211, Perplexity: 917.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|██▊       | 81/294 [00:56<02:30,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 80, Loss: 6.8066, Perplexity: 903.8365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  31%|███       | 91/294 [01:03<02:22,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 90, Loss: 6.7943, Perplexity: 892.7729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|███▍      | 101/294 [01:10<02:16,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100, Loss: 6.7829, Perplexity: 882.6590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 111/294 [01:17<02:08,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 110, Loss: 6.7833, Perplexity: 882.9745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  41%|████      | 121/294 [01:25<02:02,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 120, Loss: 6.7762, Perplexity: 876.7163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  45%|████▍     | 131/294 [01:32<01:54,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 130, Loss: 6.7751, Perplexity: 875.7890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|████▊     | 141/294 [01:39<01:48,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 140, Loss: 6.7788, Perplexity: 878.9753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  51%|█████▏    | 151/294 [01:46<01:40,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 150, Loss: 6.7727, Perplexity: 873.6287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  55%|█████▍    | 161/294 [01:53<01:34,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 160, Loss: 6.7731, Perplexity: 874.0066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  58%|█████▊    | 171/294 [02:00<01:26,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 170, Loss: 6.7718, Perplexity: 872.8884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  62%|██████▏   | 181/294 [02:07<01:20,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 180, Loss: 6.7672, Perplexity: 868.8581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  65%|██████▍   | 191/294 [02:14<01:12,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 190, Loss: 6.7684, Perplexity: 869.9076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  68%|██████▊   | 201/294 [02:21<01:06,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200, Loss: 6.7643, Perplexity: 866.3201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|███████▏  | 211/294 [02:28<00:58,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 210, Loss: 6.7600, Perplexity: 862.6277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  75%|███████▌  | 221/294 [02:35<00:51,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 220, Loss: 6.7590, Perplexity: 861.7574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  79%|███████▊  | 231/294 [02:42<00:44,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 230, Loss: 6.7608, Perplexity: 863.3361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  82%|████████▏ | 241/294 [02:49<00:37,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 240, Loss: 6.7597, Perplexity: 862.3644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  85%|████████▌ | 251/294 [02:57<00:30,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 250, Loss: 6.7574, Perplexity: 860.3632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  89%|████████▉ | 261/294 [03:04<00:23,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 260, Loss: 6.7517, Perplexity: 855.4864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  92%|█████████▏| 271/294 [03:11<00:16,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 270, Loss: 6.7436, Perplexity: 848.6210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▌| 281/294 [03:18<00:09,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 280, Loss: 6.7454, Perplexity: 850.1791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  99%|█████████▉| 291/294 [03:25<00:02,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 290, Loss: 6.7433, Perplexity: 848.3474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 294/294 [03:27<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 6.7425, Perplexity: 847.6523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 37/37 [00:09<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 6.6318, Perplexity: 758.8299\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|▎         | 11/294 [00:07<03:21,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 10, Loss: 6.5702, Perplexity: 713.5328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   7%|▋         | 21/294 [00:14<03:13,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 20, Loss: 6.6195, Perplexity: 749.5392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  11%|█         | 31/294 [00:22<03:06,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 30, Loss: 6.6327, Perplexity: 759.5284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|█▍        | 41/294 [00:29<02:59,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 40, Loss: 6.6654, Perplexity: 784.7973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|█▋        | 51/294 [00:36<02:52,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 50, Loss: 6.6814, Perplexity: 797.4661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|██        | 61/294 [00:43<02:45,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 60, Loss: 6.6771, Perplexity: 793.9802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|██▍       | 71/294 [00:50<02:37,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 70, Loss: 6.6720, Perplexity: 789.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|██▊       | 81/294 [00:57<02:30,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 80, Loss: 6.6738, Perplexity: 791.4013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  31%|███       | 91/294 [01:04<02:23,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 90, Loss: 6.6723, Perplexity: 790.2359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|███▍      | 101/294 [01:11<02:17,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100, Loss: 6.6605, Perplexity: 780.9327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 111/294 [01:18<02:09,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 110, Loss: 6.6530, Perplexity: 775.0827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  41%|████      | 121/294 [01:25<02:02,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 120, Loss: 6.6496, Perplexity: 772.4427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  45%|████▍     | 131/294 [01:32<01:55,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 130, Loss: 6.6504, Perplexity: 773.0965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|████▊     | 141/294 [01:39<01:48,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 140, Loss: 6.6517, Perplexity: 774.0992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  51%|█████▏    | 151/294 [01:47<01:41,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 150, Loss: 6.6503, Perplexity: 773.0242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  55%|█████▍    | 161/294 [01:54<01:34,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 160, Loss: 6.6512, Perplexity: 773.6828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  58%|█████▊    | 171/294 [02:01<01:27,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 170, Loss: 6.6445, Perplexity: 768.5505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  62%|██████▏   | 181/294 [02:08<01:19,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 180, Loss: 6.6401, Perplexity: 765.1687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  65%|██████▍   | 191/294 [02:15<01:12,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 190, Loss: 6.6413, Perplexity: 766.1139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  68%|██████▊   | 201/294 [02:22<01:05,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 200, Loss: 6.6359, Perplexity: 761.9453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|███████▏  | 211/294 [02:29<00:58,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 210, Loss: 6.6403, Perplexity: 765.3589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  75%|███████▌  | 221/294 [02:36<00:51,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 220, Loss: 6.6387, Perplexity: 764.1174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  79%|███████▊  | 231/294 [02:43<00:44,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 230, Loss: 6.6344, Perplexity: 760.8063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  82%|████████▏ | 241/294 [02:50<00:37,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 240, Loss: 6.6247, Perplexity: 753.4462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  85%|████████▌ | 251/294 [02:57<00:30,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 250, Loss: 6.6171, Perplexity: 747.7471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  89%|████████▉ | 261/294 [03:05<00:23,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 260, Loss: 6.6175, Perplexity: 748.0995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  92%|█████████▏| 271/294 [03:12<00:16,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 270, Loss: 6.6173, Perplexity: 747.9205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▌| 281/294 [03:19<00:09,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 280, Loss: 6.6153, Perplexity: 746.4133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  99%|█████████▉| 291/294 [03:26<00:02,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 290, Loss: 6.6178, Perplexity: 748.2894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 294/294 [03:28<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 6.6172, Perplexity: 747.8553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 37/37 [00:09<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 6.6265, Perplexity: 754.8703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 37/37 [00:09<00:00,  3.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 6.6108, Perplexity: 743.0725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_cuda_memory():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "# Example usage\n",
        "clear_cuda_memory()\n"
      ],
      "metadata": {
        "id": "1DSfTLqjiP0a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/bookcorpus_reduced.csv')\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI7LyvNOElu3",
        "outputId": "935ba42d-4e62-4d2d-8212-3bd2e8a84388"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n"
          ]
        }
      ]
    }
  ]
}