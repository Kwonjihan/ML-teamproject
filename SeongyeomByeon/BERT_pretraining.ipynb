{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPqaRGGQJu91u4vLP4SqQHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwonjihan/ML-teamproject/blob/develop/SeongyeomByeon/BERT_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import RandomSampler, DataLoader\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, DataCollatorForLanguageModeling"
      ],
      "metadata": {
        "id": "AgffaJtcofIV"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IFjkmcjWzINX",
        "outputId": "86cea5cd-f488-4a88-d3c0-e78b6b143a76"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activation function 불러오기\n",
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": torch.nn.functional.silu}"
      ],
      "metadata": {
        "id": "drgT6nqeoqN5"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "c6RRMvSIoXjR"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    vocab_size = 30522\n",
        "    hidden_size = 128\n",
        "    num_hidden_layers = 4\n",
        "    num_attention_heads = 4\n",
        "    intermediate_size = 512\n",
        "    hidden_act = \"gelu\"\n",
        "    hidden_dropout_prob = 0.1\n",
        "    attention_probs_dropout_prob = 0.1\n",
        "    max_position_embeddings = 512\n",
        "    type_vocab_size = 2\n",
        "    initializer_range = 0.02\n",
        "    layer_norm_eps = 1e-12\n",
        "    pad_token_id = 0\n",
        "    gradient_checkpointing = False\n",
        "    position_embedding_type = \"absolute\"\n",
        "    use_cache = True\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(\n",
        "            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(\n",
        "            config.type_vocab_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(\n",
        "            config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.position_embedding_type = getattr(\n",
        "            config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(\n",
        "            config.max_position_embeddings).expand((1, -1)), persistent=False)\n",
        "        self.register_buffer(\"token_type_ids\", torch.zeros(\n",
        "            self.position_ids.size(), dtype=torch.long), persistent=False)\n",
        "\n",
        "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0):\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:,\n",
        "                                             past_key_values_length: seq_length + past_key_values_length]\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n",
        "                    input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(\n",
        "                    input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention heads ({config.num_attention_heads})\")\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(\n",
        "            config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[\n",
        "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        if encoder_hidden_states is not None:\n",
        "            key_layer = self.transpose_for_scores(\n",
        "                self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(\n",
        "                self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(\n",
        "            query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / \\\n",
        "            math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[\n",
        "            :-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (\n",
        "            context_layer,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(\n",
        "            config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        self_outputs = self.self(\n",
        "            input_tensor,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], input_tensor)\n",
        "        # add attentions if we output them\n",
        "        outputs = (attention_output,) + self_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(\n",
        "            config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        layer_output = self.output(self.intermediate(\n",
        "            attention_output), attention_output)\n",
        "\n",
        "        outputs = (layer_output,) + self_attention_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([BertLayer(config)\n",
        "                                   for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        return (hidden_states, all_hidden_states, all_attentions)\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "        self.LayerNorm = nn.LayerNorm(\n",
        "            config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "        self.decoder = nn.Linear(\n",
        "            config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class BertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n",
        "class BertModel(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\n",
        "                \"input_ids 혹은 inputs_embeds 둘 중 하나의 형식으로만 입력해야 합니다.\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"input_ids 또는 inputs_embeds의 형식이어야 합니다.\")\n",
        "\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(input_shape, device=device)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(\n",
        "                input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        return sequence_output, pooled_output\n",
        "\n",
        "\n",
        "class BertForPreTraining(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertPreTrainingHeads(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "\n",
        "        total_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, config.vocab_size), labels.view(-1))\n",
        "            total_loss = masked_lm_loss\n",
        "\n",
        "        return prediction_scores, seq_relationship_score, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로드 및 전처리\n",
        "class ParquetDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size=512):\n",
        "        # 데이터셋 로드\n",
        "        df = pd.read_parquet(file_path)\n",
        "        text = \" \".join(df[\"text\"].tolist())\n",
        "        self.examples = []\n",
        "\n",
        "        # 토크나이즈 및 블록 크기로 자르기\n",
        "        for i in range(0, len(text) - block_size, block_size):\n",
        "            chunk = text[i:i + block_size]\n",
        "            inputs = tokenizer(chunk, add_special_tokens=True, max_length=block_size, truncation=True, return_tensors=\"pt\", padding=\"max_length\")\n",
        "            inputs['labels'] = inputs.input_ids.clone()\n",
        "            inputs['next_sentence_label'] = torch.tensor(0)  # NSP를 위해 0 또는 1로 설정 (여기서는 임의로 0 설정)\n",
        "            self.examples.append(inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return {key: val.squeeze(0) for key, val in self.examples[i].items()}\n",
        "\n",
        "# 메인 함수\n",
        "def main():\n",
        "    # 데이터셋 경로 지정\n",
        "    data_dir = \"./\"\n",
        "    train_path = os.path.join(data_dir, \"train-00000-of-00001.parquet\")\n",
        "    validation_path = os.path.join(data_dir, \"validation-00000-of-00001.parquet\")\n",
        "    test_path = os.path.join(data_dir, \"test-00000-of-00001.parquet\")\n",
        "\n",
        "    # 토크나이저 초기화\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # 데이터셋 전처리\n",
        "    train_dataset = ParquetDataset(train_path, tokenizer)\n",
        "    validation_dataset = ParquetDataset(validation_path, tokenizer)\n",
        "    test_dataset = ParquetDataset(test_path, tokenizer)\n",
        "\n",
        "    # 데이터 로더 생성\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=8, collate_fn=data_collator)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=8, collate_fn=data_collator)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=data_collator)\n",
        "\n",
        "    return train_dataloader, validation_dataloader, test_dataloader\n",
        "\n",
        "train_dataloader, validation_dataloader, test_dataloader = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ybgHUBuwNJB",
        "outputId": "4534ed22-72cb-4ca3-e803-045e07fbcbb4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화\n",
        "config = Config()\n",
        "model = BertForPreTraining(config)\n",
        "\n",
        "# 옵티마이저 설정 (학습률 조정)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# 데이터셋 셔플링\n",
        "def get_shuffled_dataloader(dataset, batch_size=8):\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 학습 루프\n",
        "model.train()\n",
        "train_dataloader = get_shuffled_dataloader(train_dataloader.dataset, batch_size=8)\n",
        "for epoch in range(3):\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], token_type_ids=batch['token_type_ids'], labels=batch['labels'])\n",
        "        loss = outputs[2]  # total loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        if batch_idx % 10 == 0:  # 10번째 배치마다 현황 출력\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch: {epoch} finished with average loss: {avg_epoch_loss}\")\n",
        "\n",
        "# 모델 저장 경로 설정\n",
        "save_path = './my_pretrained_bert_model.pth'\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JDizczOwPIc",
        "outputId": "24305b3b-d2e3-4ee6-9993-4befc68a44e1"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Batch: 0, Loss: 10.503795623779297\n",
            "Epoch: 0, Batch: 10, Loss: 10.12758731842041\n",
            "Epoch: 0, Batch: 20, Loss: 9.712815284729004\n",
            "Epoch: 0, Batch: 30, Loss: 9.392827033996582\n",
            "Epoch: 0, Batch: 40, Loss: 9.071388244628906\n",
            "Epoch: 0, Batch: 50, Loss: 8.75230598449707\n",
            "Epoch: 0, Batch: 60, Loss: 8.491134643554688\n",
            "Epoch: 0, Batch: 70, Loss: 8.252588272094727\n",
            "Epoch: 0, Batch: 80, Loss: 7.997152328491211\n",
            "Epoch: 0, Batch: 90, Loss: 7.833266735076904\n",
            "Epoch: 0, Batch: 100, Loss: 7.605878829956055\n",
            "Epoch: 0, Batch: 110, Loss: 7.362124919891357\n",
            "Epoch: 0, Batch: 120, Loss: 7.25746488571167\n",
            "Epoch: 0, Batch: 130, Loss: 7.172574996948242\n",
            "Epoch: 0, Batch: 140, Loss: 6.983524799346924\n",
            "Epoch: 0, Batch: 150, Loss: 6.823107719421387\n",
            "Epoch: 0, Batch: 160, Loss: 6.7450714111328125\n",
            "Epoch: 0, Batch: 170, Loss: 6.60078763961792\n",
            "Epoch: 0, Batch: 180, Loss: 6.527900218963623\n",
            "Epoch: 0, Batch: 190, Loss: 6.548076629638672\n",
            "Epoch: 0, Batch: 200, Loss: 6.3798699378967285\n",
            "Epoch: 0, Batch: 210, Loss: 6.2983293533325195\n",
            "Epoch: 0, Batch: 220, Loss: 6.2205681800842285\n",
            "Epoch: 0, Batch: 230, Loss: 6.125465393066406\n",
            "Epoch: 0, Batch: 240, Loss: 6.044312953948975\n",
            "Epoch: 0, Batch: 250, Loss: 5.944174766540527\n",
            "Epoch: 0, Batch: 260, Loss: 5.999958038330078\n",
            "Epoch: 0, Batch: 270, Loss: 5.820107936859131\n",
            "Epoch: 0, Batch: 280, Loss: 5.86385440826416\n",
            "Epoch: 0, Batch: 290, Loss: 5.740509986877441\n",
            "Epoch: 0, Batch: 300, Loss: 5.761276721954346\n",
            "Epoch: 0, Batch: 310, Loss: 5.792232513427734\n",
            "Epoch: 0, Batch: 320, Loss: 5.744320869445801\n",
            "Epoch: 0, Batch: 330, Loss: 5.60226583480835\n",
            "Epoch: 0, Batch: 340, Loss: 5.67494010925293\n",
            "Epoch: 0, Batch: 350, Loss: 5.574198246002197\n",
            "Epoch: 0, Batch: 360, Loss: 5.603248119354248\n",
            "Epoch: 0, Batch: 370, Loss: 5.591782569885254\n",
            "Epoch: 0, Batch: 380, Loss: 5.450019359588623\n",
            "Epoch: 0, Batch: 390, Loss: 5.313775062561035\n",
            "Epoch: 0, Batch: 400, Loss: 5.364348411560059\n",
            "Epoch: 0, Batch: 410, Loss: 5.32045316696167\n",
            "Epoch: 0, Batch: 420, Loss: 5.296217441558838\n",
            "Epoch: 0, Batch: 430, Loss: 5.293412685394287\n",
            "Epoch: 0, Batch: 440, Loss: 5.30562686920166\n",
            "Epoch: 0, Batch: 450, Loss: 5.230557441711426\n",
            "Epoch: 0, Batch: 460, Loss: 5.160181999206543\n",
            "Epoch: 0, Batch: 470, Loss: 5.166954040527344\n",
            "Epoch: 0, Batch: 480, Loss: 5.254962921142578\n",
            "Epoch: 0, Batch: 490, Loss: 5.118310928344727\n",
            "Epoch: 0, Batch: 500, Loss: 5.0097975730896\n",
            "Epoch: 0, Batch: 510, Loss: 5.127565383911133\n",
            "Epoch: 0, Batch: 520, Loss: 5.090515613555908\n",
            "Epoch: 0, Batch: 530, Loss: 5.039437294006348\n",
            "Epoch: 0, Batch: 540, Loss: 4.995193004608154\n",
            "Epoch: 0, Batch: 550, Loss: 4.929602146148682\n",
            "Epoch: 0, Batch: 560, Loss: 4.8689165115356445\n",
            "Epoch: 0, Batch: 570, Loss: 4.984372138977051\n",
            "Epoch: 0, Batch: 580, Loss: 4.839602470397949\n",
            "Epoch: 0, Batch: 590, Loss: 4.806793689727783\n",
            "Epoch: 0, Batch: 600, Loss: 4.7624945640563965\n",
            "Epoch: 0, Batch: 610, Loss: 4.893038749694824\n",
            "Epoch: 0, Batch: 620, Loss: 4.854518413543701\n",
            "Epoch: 0, Batch: 630, Loss: 4.858242034912109\n",
            "Epoch: 0, Batch: 640, Loss: 4.761544704437256\n",
            "Epoch: 0, Batch: 650, Loss: 4.6657490730285645\n",
            "Epoch: 0, Batch: 660, Loss: 4.664810657501221\n",
            "Epoch: 0, Batch: 670, Loss: 4.740673065185547\n",
            "Epoch: 0, Batch: 680, Loss: 4.612834453582764\n",
            "Epoch: 0, Batch: 690, Loss: 4.606157302856445\n",
            "Epoch: 0, Batch: 700, Loss: 4.63857889175415\n",
            "Epoch: 0, Batch: 710, Loss: 4.506190299987793\n",
            "Epoch: 0, Batch: 720, Loss: 4.609089374542236\n",
            "Epoch: 0, Batch: 730, Loss: 4.5518574714660645\n",
            "Epoch: 0, Batch: 740, Loss: 4.5367655754089355\n",
            "Epoch: 0, Batch: 750, Loss: 4.557963848114014\n",
            "Epoch: 0, Batch: 760, Loss: 4.403926372528076\n",
            "Epoch: 0, Batch: 770, Loss: 4.413182258605957\n",
            "Epoch: 0, Batch: 780, Loss: 4.517223834991455\n",
            "Epoch: 0, Batch: 790, Loss: 4.498284339904785\n",
            "Epoch: 0, Batch: 800, Loss: 4.375775337219238\n",
            "Epoch: 0, Batch: 810, Loss: 4.387345790863037\n",
            "Epoch: 0, Batch: 820, Loss: 4.328400135040283\n",
            "Epoch: 0, Batch: 830, Loss: 4.288788318634033\n",
            "Epoch: 0, Batch: 840, Loss: 4.344259262084961\n",
            "Epoch: 0, Batch: 850, Loss: 4.290696620941162\n",
            "Epoch: 0, Batch: 860, Loss: 4.278934955596924\n",
            "Epoch: 0, Batch: 870, Loss: 4.327530384063721\n",
            "Epoch: 0, Batch: 880, Loss: 4.212005615234375\n",
            "Epoch: 0, Batch: 890, Loss: 4.272830963134766\n",
            "Epoch: 0, Batch: 900, Loss: 4.159060001373291\n",
            "Epoch: 0, Batch: 910, Loss: 4.339609146118164\n",
            "Epoch: 0, Batch: 920, Loss: 4.129444122314453\n",
            "Epoch: 0, Batch: 930, Loss: 4.113302707672119\n",
            "Epoch: 0, Batch: 940, Loss: 4.135101318359375\n",
            "Epoch: 0, Batch: 950, Loss: 4.1079020500183105\n",
            "Epoch: 0, Batch: 960, Loss: 4.113959312438965\n",
            "Epoch: 0, Batch: 970, Loss: 3.981693983078003\n",
            "Epoch: 0, Batch: 980, Loss: 4.011712074279785\n",
            "Epoch: 0, Batch: 990, Loss: 4.00253438949585\n",
            "Epoch: 0, Batch: 1000, Loss: 3.9868693351745605\n",
            "Epoch: 0, Batch: 1010, Loss: 4.027812480926514\n",
            "Epoch: 0, Batch: 1020, Loss: 4.020705223083496\n",
            "Epoch: 0, Batch: 1030, Loss: 4.060787677764893\n",
            "Epoch: 0, Batch: 1040, Loss: 3.853900909423828\n",
            "Epoch: 0, Batch: 1050, Loss: 3.8660078048706055\n",
            "Epoch: 0, Batch: 1060, Loss: 4.0684356689453125\n",
            "Epoch: 0, Batch: 1070, Loss: 3.885486364364624\n",
            "Epoch: 0, Batch: 1080, Loss: 3.82912540435791\n",
            "Epoch: 0, Batch: 1090, Loss: 3.7934069633483887\n",
            "Epoch: 0, Batch: 1100, Loss: 3.8620312213897705\n",
            "Epoch: 0, Batch: 1110, Loss: 3.787209987640381\n",
            "Epoch: 0, Batch: 1120, Loss: 3.872739315032959\n",
            "Epoch: 0, Batch: 1130, Loss: 3.7627804279327393\n",
            "Epoch: 0, Batch: 1140, Loss: 3.699160575866699\n",
            "Epoch: 0, Batch: 1150, Loss: 3.7159299850463867\n",
            "Epoch: 0, Batch: 1160, Loss: 3.7300283908843994\n",
            "Epoch: 0, Batch: 1170, Loss: 3.7036120891571045\n",
            "Epoch: 0, Batch: 1180, Loss: 3.618574380874634\n",
            "Epoch: 0, Batch: 1190, Loss: 3.6571857929229736\n",
            "Epoch: 0, Batch: 1200, Loss: 3.6599926948547363\n",
            "Epoch: 0, Batch: 1210, Loss: 3.5598106384277344\n",
            "Epoch: 0, Batch: 1220, Loss: 3.5756349563598633\n",
            "Epoch: 0, Batch: 1230, Loss: 3.561490535736084\n",
            "Epoch: 0, Batch: 1240, Loss: 3.5443620681762695\n",
            "Epoch: 0, Batch: 1250, Loss: 3.497347354888916\n",
            "Epoch: 0, Batch: 1260, Loss: 3.4711477756500244\n",
            "Epoch: 0, Batch: 1270, Loss: 3.4917232990264893\n",
            "Epoch: 0, Batch: 1280, Loss: 3.5682361125946045\n",
            "Epoch: 0, Batch: 1290, Loss: 3.58105206489563\n",
            "Epoch: 0, Batch: 1300, Loss: 3.427558422088623\n",
            "Epoch: 0, Batch: 1310, Loss: 3.3661415576934814\n",
            "Epoch: 0, Batch: 1320, Loss: 3.465665578842163\n",
            "Epoch: 0, Batch: 1330, Loss: 3.5671298503875732\n",
            "Epoch: 0, Batch: 1340, Loss: 3.4086670875549316\n",
            "Epoch: 0, Batch: 1350, Loss: 3.4018330574035645\n",
            "Epoch: 0, Batch: 1360, Loss: 3.4241955280303955\n",
            "Epoch: 0, Batch: 1370, Loss: 3.274301767349243\n",
            "Epoch: 0, Batch: 1380, Loss: 3.356571674346924\n",
            "Epoch: 0, Batch: 1390, Loss: 3.380378007888794\n",
            "Epoch: 0, Batch: 1400, Loss: 3.294599771499634\n",
            "Epoch: 0, Batch: 1410, Loss: 3.285632610321045\n",
            "Epoch: 0, Batch: 1420, Loss: 3.2563910484313965\n",
            "Epoch: 0, Batch: 1430, Loss: 3.3577513694763184\n",
            "Epoch: 0, Batch: 1440, Loss: 3.2956814765930176\n",
            "Epoch: 0, Batch: 1450, Loss: 3.2685928344726562\n",
            "Epoch: 0, Batch: 1460, Loss: 3.34385085105896\n",
            "Epoch: 0, Batch: 1470, Loss: 3.1976430416107178\n",
            "Epoch: 0, Batch: 1480, Loss: 3.2422938346862793\n",
            "Epoch: 0, Batch: 1490, Loss: 3.240004539489746\n",
            "Epoch: 0, Batch: 1500, Loss: 3.265620231628418\n",
            "Epoch: 0, Batch: 1510, Loss: 3.284959316253662\n",
            "Epoch: 0, Batch: 1520, Loss: 3.2010300159454346\n",
            "Epoch: 0, Batch: 1530, Loss: 3.1858837604522705\n",
            "Epoch: 0, Batch: 1540, Loss: 3.1311397552490234\n",
            "Epoch: 0, Batch: 1550, Loss: 3.1006739139556885\n",
            "Epoch: 0, Batch: 1560, Loss: 3.0735301971435547\n",
            "Epoch: 0, Batch: 1570, Loss: 3.152876377105713\n",
            "Epoch: 0, Batch: 1580, Loss: 3.093764305114746\n",
            "Epoch: 0, Batch: 1590, Loss: 3.060235023498535\n",
            "Epoch: 0, Batch: 1600, Loss: 2.9886205196380615\n",
            "Epoch: 0, Batch: 1610, Loss: 2.9436728954315186\n",
            "Epoch: 0, Batch: 1620, Loss: 3.079279661178589\n",
            "Epoch: 0, Batch: 1630, Loss: 2.946847677230835\n",
            "Epoch: 0, Batch: 1640, Loss: 3.076107978820801\n",
            "Epoch: 0, Batch: 1650, Loss: 2.9729459285736084\n",
            "Epoch: 0, Batch: 1660, Loss: 2.9569547176361084\n",
            "Epoch: 0, Batch: 1670, Loss: 2.9350786209106445\n",
            "Epoch: 0, Batch: 1680, Loss: 3.042604446411133\n",
            "Epoch: 0, Batch: 1690, Loss: 2.920076847076416\n",
            "Epoch: 0, Batch: 1700, Loss: 2.8854188919067383\n",
            "Epoch: 0, Batch: 1710, Loss: 2.927990674972534\n",
            "Epoch: 0, Batch: 1720, Loss: 2.8996386528015137\n",
            "Epoch: 0, Batch: 1730, Loss: 2.9130659103393555\n",
            "Epoch: 0, Batch: 1740, Loss: 2.999922275543213\n",
            "Epoch: 0, Batch: 1750, Loss: 2.9356300830841064\n",
            "Epoch: 0, Batch: 1760, Loss: 2.9234678745269775\n",
            "Epoch: 0, Batch: 1770, Loss: 2.855501651763916\n",
            "Epoch: 0, Batch: 1780, Loss: 2.770756721496582\n",
            "Epoch: 0, Batch: 1790, Loss: 2.938084840774536\n",
            "Epoch: 0, Batch: 1800, Loss: 2.762385129928589\n",
            "Epoch: 0, Batch: 1810, Loss: 2.82019305229187\n",
            "Epoch: 0, Batch: 1820, Loss: 2.815758466720581\n",
            "Epoch: 0, Batch: 1830, Loss: 2.803558588027954\n",
            "Epoch: 0, Batch: 1840, Loss: 2.941511392593384\n",
            "Epoch: 0, Batch: 1850, Loss: 2.820530652999878\n",
            "Epoch: 0, Batch: 1860, Loss: 2.7247579097747803\n",
            "Epoch: 0, Batch: 1870, Loss: 2.703683376312256\n",
            "Epoch: 0, Batch: 1880, Loss: 2.725569248199463\n",
            "Epoch: 0, Batch: 1890, Loss: 2.7390875816345215\n",
            "Epoch: 0, Batch: 1900, Loss: 2.7029335498809814\n",
            "Epoch: 0, Batch: 1910, Loss: 2.8137171268463135\n",
            "Epoch: 0, Batch: 1920, Loss: 2.7378334999084473\n",
            "Epoch: 0, Batch: 1930, Loss: 2.7500791549682617\n",
            "Epoch: 0, Batch: 1940, Loss: 2.7217280864715576\n",
            "Epoch: 0, Batch: 1950, Loss: 2.745932102203369\n",
            "Epoch: 0, Batch: 1960, Loss: 2.7088961601257324\n",
            "Epoch: 0, Batch: 1970, Loss: 2.809335470199585\n",
            "Epoch: 0, Batch: 1980, Loss: 2.6981520652770996\n",
            "Epoch: 0, Batch: 1990, Loss: 2.7497479915618896\n",
            "Epoch: 0, Batch: 2000, Loss: 2.6273012161254883\n",
            "Epoch: 0, Batch: 2010, Loss: 2.647998332977295\n",
            "Epoch: 0, Batch: 2020, Loss: 2.6447625160217285\n",
            "Epoch: 0, Batch: 2030, Loss: 2.618068218231201\n",
            "Epoch: 0, Batch: 2040, Loss: 2.5002124309539795\n",
            "Epoch: 0, Batch: 2050, Loss: 2.625247001647949\n",
            "Epoch: 0, Batch: 2060, Loss: 2.5954859256744385\n",
            "Epoch: 0, Batch: 2070, Loss: 2.499581813812256\n",
            "Epoch: 0, Batch: 2080, Loss: 2.6297738552093506\n",
            "Epoch: 0, Batch: 2090, Loss: 2.554593086242676\n",
            "Epoch: 0, Batch: 2100, Loss: 2.513559341430664\n",
            "Epoch: 0, Batch: 2110, Loss: 2.5926053524017334\n",
            "Epoch: 0, Batch: 2120, Loss: 2.4666366577148438\n",
            "Epoch: 0, Batch: 2130, Loss: 2.5082039833068848\n",
            "Epoch: 0, Batch: 2140, Loss: 2.503446578979492\n",
            "Epoch: 0, Batch: 2150, Loss: 2.5656237602233887\n",
            "Epoch: 0, Batch: 2160, Loss: 2.5195138454437256\n",
            "Epoch: 0, Batch: 2170, Loss: 2.526327133178711\n",
            "Epoch: 0, Batch: 2180, Loss: 2.454277992248535\n",
            "Epoch: 0, Batch: 2190, Loss: 2.54392671585083\n",
            "Epoch: 0, Batch: 2200, Loss: 2.4794833660125732\n",
            "Epoch: 0, Batch: 2210, Loss: 2.495649576187134\n",
            "Epoch: 0, Batch: 2220, Loss: 2.530583143234253\n",
            "Epoch: 0, Batch: 2230, Loss: 2.5014147758483887\n",
            "Epoch: 0, Batch: 2240, Loss: 2.5299978256225586\n",
            "Epoch: 0, Batch: 2250, Loss: 2.315995216369629\n",
            "Epoch: 0, Batch: 2260, Loss: 2.5631332397460938\n",
            "Epoch: 0, Batch: 2270, Loss: 2.3735342025756836\n",
            "Epoch: 0, Batch: 2280, Loss: 2.3755476474761963\n",
            "Epoch: 0, Batch: 2290, Loss: 2.468640089035034\n",
            "Epoch: 0, Batch: 2300, Loss: 2.4841485023498535\n",
            "Epoch: 0, Batch: 2310, Loss: 2.4486589431762695\n",
            "Epoch: 0, Batch: 2320, Loss: 2.548816680908203\n",
            "Epoch: 0, Batch: 2330, Loss: 2.405068874359131\n",
            "Epoch: 0, Batch: 2340, Loss: 2.4878013134002686\n",
            "Epoch: 0, Batch: 2350, Loss: 2.335515022277832\n",
            "Epoch: 0, Batch: 2360, Loss: 2.3714194297790527\n",
            "Epoch: 0, Batch: 2370, Loss: 2.559821367263794\n",
            "Epoch: 0, Batch: 2380, Loss: 2.433098554611206\n",
            "Epoch: 0, Batch: 2390, Loss: 2.4136838912963867\n",
            "Epoch: 0, Batch: 2400, Loss: 2.3774571418762207\n",
            "Epoch: 0, Batch: 2410, Loss: 2.4191300868988037\n",
            "Epoch: 0, Batch: 2420, Loss: 2.4252262115478516\n",
            "Epoch: 0, Batch: 2430, Loss: 2.383694648742676\n",
            "Epoch: 0, Batch: 2440, Loss: 2.347564697265625\n",
            "Epoch: 0, Batch: 2450, Loss: 2.304788589477539\n",
            "Epoch: 0, Batch: 2460, Loss: 2.386521100997925\n",
            "Epoch: 0, Batch: 2470, Loss: 2.401945114135742\n",
            "Epoch: 0, Batch: 2480, Loss: 2.5291783809661865\n",
            "Epoch: 0, Batch: 2490, Loss: 2.431279420852661\n",
            "Epoch: 0, Batch: 2500, Loss: 2.32330322265625\n",
            "Epoch: 0, Batch: 2510, Loss: 2.22281551361084\n",
            "Epoch: 0, Batch: 2520, Loss: 2.305454969406128\n",
            "Epoch: 0, Batch: 2530, Loss: 2.3636462688446045\n",
            "Epoch: 0, Batch: 2540, Loss: 2.299293041229248\n",
            "Epoch: 0, Batch: 2550, Loss: 2.2636301517486572\n",
            "Epoch: 0, Batch: 2560, Loss: 2.3044309616088867\n",
            "Epoch: 0, Batch: 2570, Loss: 2.3497636318206787\n",
            "Epoch: 0, Batch: 2580, Loss: 2.2839369773864746\n",
            "Epoch: 0, Batch: 2590, Loss: 2.3416550159454346\n",
            "Epoch: 0, Batch: 2600, Loss: 2.30794620513916\n",
            "Epoch: 0, Batch: 2610, Loss: 2.2832000255584717\n",
            "Epoch: 0, Batch: 2620, Loss: 2.3668649196624756\n",
            "Epoch: 0, Batch: 2630, Loss: 2.2466683387756348\n",
            "Epoch: 0 finished with average loss: 3.915376698405286\n",
            "Epoch: 1, Batch: 0, Loss: 2.3018767833709717\n",
            "Epoch: 1, Batch: 10, Loss: 2.292386531829834\n",
            "Epoch: 1, Batch: 20, Loss: 2.3910765647888184\n",
            "Epoch: 1, Batch: 30, Loss: 2.380866050720215\n",
            "Epoch: 1, Batch: 40, Loss: 2.296454429626465\n",
            "Epoch: 1, Batch: 50, Loss: 2.392915725708008\n",
            "Epoch: 1, Batch: 60, Loss: 2.2371933460235596\n",
            "Epoch: 1, Batch: 70, Loss: 2.23289155960083\n",
            "Epoch: 1, Batch: 80, Loss: 2.2696990966796875\n",
            "Epoch: 1, Batch: 90, Loss: 2.2678680419921875\n",
            "Epoch: 1, Batch: 100, Loss: 2.2886123657226562\n",
            "Epoch: 1, Batch: 110, Loss: 2.263654947280884\n",
            "Epoch: 1, Batch: 120, Loss: 2.3238525390625\n",
            "Epoch: 1, Batch: 130, Loss: 2.2100446224212646\n",
            "Epoch: 1, Batch: 140, Loss: 2.2778241634368896\n",
            "Epoch: 1, Batch: 150, Loss: 2.190096378326416\n",
            "Epoch: 1, Batch: 160, Loss: 2.1677002906799316\n",
            "Epoch: 1, Batch: 170, Loss: 2.2999534606933594\n",
            "Epoch: 1, Batch: 180, Loss: 2.1946053504943848\n",
            "Epoch: 1, Batch: 190, Loss: 2.1745617389678955\n",
            "Epoch: 1, Batch: 200, Loss: 2.294691324234009\n",
            "Epoch: 1, Batch: 210, Loss: 2.1931893825531006\n",
            "Epoch: 1, Batch: 220, Loss: 2.1934244632720947\n",
            "Epoch: 1, Batch: 230, Loss: 2.2320706844329834\n",
            "Epoch: 1, Batch: 240, Loss: 2.149653673171997\n",
            "Epoch: 1, Batch: 250, Loss: 2.130476236343384\n",
            "Epoch: 1, Batch: 260, Loss: 2.1864025592803955\n",
            "Epoch: 1, Batch: 270, Loss: 2.136845588684082\n",
            "Epoch: 1, Batch: 280, Loss: 2.14941143989563\n",
            "Epoch: 1, Batch: 290, Loss: 2.2116055488586426\n",
            "Epoch: 1, Batch: 300, Loss: 2.122192144393921\n",
            "Epoch: 1, Batch: 310, Loss: 2.2059333324432373\n",
            "Epoch: 1, Batch: 320, Loss: 2.1084072589874268\n",
            "Epoch: 1, Batch: 330, Loss: 2.2344045639038086\n",
            "Epoch: 1, Batch: 340, Loss: 2.295750141143799\n",
            "Epoch: 1, Batch: 350, Loss: 2.1483325958251953\n",
            "Epoch: 1, Batch: 360, Loss: 2.253817319869995\n",
            "Epoch: 1, Batch: 370, Loss: 2.1527957916259766\n",
            "Epoch: 1, Batch: 380, Loss: 2.1166250705718994\n",
            "Epoch: 1, Batch: 390, Loss: 2.130342483520508\n",
            "Epoch: 1, Batch: 400, Loss: 2.077538251876831\n",
            "Epoch: 1, Batch: 410, Loss: 2.0458922386169434\n",
            "Epoch: 1, Batch: 420, Loss: 2.0749621391296387\n",
            "Epoch: 1, Batch: 430, Loss: 2.0959692001342773\n",
            "Epoch: 1, Batch: 440, Loss: 2.1729793548583984\n",
            "Epoch: 1, Batch: 450, Loss: 2.1496028900146484\n",
            "Epoch: 1, Batch: 460, Loss: 2.1073660850524902\n",
            "Epoch: 1, Batch: 470, Loss: 2.2549378871917725\n",
            "Epoch: 1, Batch: 480, Loss: 2.0728702545166016\n",
            "Epoch: 1, Batch: 490, Loss: 1.9302624464035034\n",
            "Epoch: 1, Batch: 500, Loss: 2.057481527328491\n",
            "Epoch: 1, Batch: 510, Loss: 2.051138162612915\n",
            "Epoch: 1, Batch: 520, Loss: 2.0194902420043945\n",
            "Epoch: 1, Batch: 530, Loss: 2.1019961833953857\n",
            "Epoch: 1, Batch: 540, Loss: 2.094170331954956\n",
            "Epoch: 1, Batch: 550, Loss: 2.0037362575531006\n",
            "Epoch: 1, Batch: 560, Loss: 1.9481070041656494\n",
            "Epoch: 1, Batch: 570, Loss: 2.038111686706543\n",
            "Epoch: 1, Batch: 580, Loss: 2.0245184898376465\n",
            "Epoch: 1, Batch: 590, Loss: 1.9930787086486816\n",
            "Epoch: 1, Batch: 600, Loss: 1.999219536781311\n",
            "Epoch: 1, Batch: 610, Loss: 1.9334298372268677\n",
            "Epoch: 1, Batch: 620, Loss: 2.056297779083252\n",
            "Epoch: 1, Batch: 630, Loss: 2.0459253787994385\n",
            "Epoch: 1, Batch: 640, Loss: 1.9871840476989746\n",
            "Epoch: 1, Batch: 650, Loss: 1.947222352027893\n",
            "Epoch: 1, Batch: 660, Loss: 2.0018062591552734\n",
            "Epoch: 1, Batch: 670, Loss: 1.8748935461044312\n",
            "Epoch: 1, Batch: 680, Loss: 1.9330005645751953\n",
            "Epoch: 1, Batch: 690, Loss: 2.02386736869812\n",
            "Epoch: 1, Batch: 700, Loss: 1.96360445022583\n",
            "Epoch: 1, Batch: 710, Loss: 1.9515806436538696\n",
            "Epoch: 1, Batch: 720, Loss: 2.0070273876190186\n",
            "Epoch: 1, Batch: 730, Loss: 2.0185039043426514\n",
            "Epoch: 1, Batch: 740, Loss: 1.9146612882614136\n",
            "Epoch: 1, Batch: 750, Loss: 1.9023447036743164\n",
            "Epoch: 1, Batch: 760, Loss: 1.8923951387405396\n",
            "Epoch: 1, Batch: 770, Loss: 2.0703980922698975\n",
            "Epoch: 1, Batch: 780, Loss: 1.9172309637069702\n",
            "Epoch: 1, Batch: 790, Loss: 1.86142897605896\n",
            "Epoch: 1, Batch: 800, Loss: 1.9467581510543823\n",
            "Epoch: 1, Batch: 810, Loss: 1.8605707883834839\n",
            "Epoch: 1, Batch: 820, Loss: 1.853034257888794\n",
            "Epoch: 1, Batch: 830, Loss: 1.9444619417190552\n",
            "Epoch: 1, Batch: 840, Loss: 1.9254467487335205\n",
            "Epoch: 1, Batch: 850, Loss: 1.8463056087493896\n",
            "Epoch: 1, Batch: 860, Loss: 1.853987693786621\n",
            "Epoch: 1, Batch: 870, Loss: 1.8974615335464478\n",
            "Epoch: 1, Batch: 880, Loss: 1.8881871700286865\n",
            "Epoch: 1, Batch: 890, Loss: 1.8843356370925903\n",
            "Epoch: 1, Batch: 900, Loss: 1.8664039373397827\n",
            "Epoch: 1, Batch: 910, Loss: 1.8935760259628296\n",
            "Epoch: 1, Batch: 920, Loss: 1.9295165538787842\n",
            "Epoch: 1, Batch: 930, Loss: 1.8453353643417358\n",
            "Epoch: 1, Batch: 940, Loss: 1.8956471681594849\n",
            "Epoch: 1, Batch: 950, Loss: 1.8761544227600098\n",
            "Epoch: 1, Batch: 960, Loss: 1.8083845376968384\n",
            "Epoch: 1, Batch: 970, Loss: 1.7635382413864136\n",
            "Epoch: 1, Batch: 980, Loss: 1.8663214445114136\n",
            "Epoch: 1, Batch: 990, Loss: 1.766296625137329\n",
            "Epoch: 1, Batch: 1000, Loss: 1.8181838989257812\n",
            "Epoch: 1, Batch: 1010, Loss: 1.823644757270813\n",
            "Epoch: 1, Batch: 1020, Loss: 1.9124443531036377\n",
            "Epoch: 1, Batch: 1030, Loss: 1.8415184020996094\n",
            "Epoch: 1, Batch: 1040, Loss: 1.7946076393127441\n",
            "Epoch: 1, Batch: 1050, Loss: 1.7916921377182007\n",
            "Epoch: 1, Batch: 1060, Loss: 1.8057514429092407\n",
            "Epoch: 1, Batch: 1070, Loss: 1.8423668146133423\n",
            "Epoch: 1, Batch: 1080, Loss: 1.7763898372650146\n",
            "Epoch: 1, Batch: 1090, Loss: 1.7588517665863037\n",
            "Epoch: 1, Batch: 1100, Loss: 1.820031762123108\n",
            "Epoch: 1, Batch: 1110, Loss: 1.8553760051727295\n",
            "Epoch: 1, Batch: 1120, Loss: 1.712416410446167\n",
            "Epoch: 1, Batch: 1130, Loss: 1.7763375043869019\n",
            "Epoch: 1, Batch: 1140, Loss: 1.770003080368042\n",
            "Epoch: 1, Batch: 1150, Loss: 1.782084345817566\n",
            "Epoch: 1, Batch: 1160, Loss: 1.777047872543335\n",
            "Epoch: 1, Batch: 1170, Loss: 1.815527081489563\n",
            "Epoch: 1, Batch: 1180, Loss: 1.7701644897460938\n",
            "Epoch: 1, Batch: 1190, Loss: 1.7941440343856812\n",
            "Epoch: 1, Batch: 1200, Loss: 1.7417157888412476\n",
            "Epoch: 1, Batch: 1210, Loss: 1.799256443977356\n",
            "Epoch: 1, Batch: 1220, Loss: 1.79497230052948\n",
            "Epoch: 1, Batch: 1230, Loss: 1.8109909296035767\n",
            "Epoch: 1, Batch: 1240, Loss: 1.716802954673767\n",
            "Epoch: 1, Batch: 1250, Loss: 1.7601522207260132\n",
            "Epoch: 1, Batch: 1260, Loss: 1.8140326738357544\n",
            "Epoch: 1, Batch: 1270, Loss: 1.7513799667358398\n",
            "Epoch: 1, Batch: 1280, Loss: 1.7849482297897339\n",
            "Epoch: 1, Batch: 1290, Loss: 1.6906182765960693\n",
            "Epoch: 1, Batch: 1300, Loss: 1.7162367105484009\n",
            "Epoch: 1, Batch: 1310, Loss: 1.6956136226654053\n",
            "Epoch: 1, Batch: 1320, Loss: 1.6714962720870972\n",
            "Epoch: 1, Batch: 1330, Loss: 1.7925249338150024\n",
            "Epoch: 1, Batch: 1340, Loss: 1.7381311655044556\n",
            "Epoch: 1, Batch: 1350, Loss: 1.7198766469955444\n",
            "Epoch: 1, Batch: 1360, Loss: 1.7568907737731934\n",
            "Epoch: 1, Batch: 1370, Loss: 1.8688808679580688\n",
            "Epoch: 1, Batch: 1380, Loss: 1.6655861139297485\n",
            "Epoch: 1, Batch: 1390, Loss: 1.7435308694839478\n",
            "Epoch: 1, Batch: 1400, Loss: 1.658489465713501\n",
            "Epoch: 1, Batch: 1410, Loss: 1.6631335020065308\n",
            "Epoch: 1, Batch: 1420, Loss: 1.723596453666687\n",
            "Epoch: 1, Batch: 1430, Loss: 1.7195639610290527\n",
            "Epoch: 1, Batch: 1440, Loss: 1.6625654697418213\n",
            "Epoch: 1, Batch: 1450, Loss: 1.6498740911483765\n",
            "Epoch: 1, Batch: 1460, Loss: 1.7126762866973877\n",
            "Epoch: 1, Batch: 1470, Loss: 1.6613508462905884\n",
            "Epoch: 1, Batch: 1480, Loss: 1.6108876466751099\n",
            "Epoch: 1, Batch: 1490, Loss: 1.6648979187011719\n",
            "Epoch: 1, Batch: 1500, Loss: 1.7216262817382812\n",
            "Epoch: 1, Batch: 1510, Loss: 1.6798913478851318\n",
            "Epoch: 1, Batch: 1520, Loss: 1.6202583312988281\n",
            "Epoch: 1, Batch: 1530, Loss: 1.6310981512069702\n",
            "Epoch: 1, Batch: 1540, Loss: 1.5876164436340332\n",
            "Epoch: 1, Batch: 1550, Loss: 1.694676399230957\n",
            "Epoch: 1, Batch: 1560, Loss: 1.634021520614624\n",
            "Epoch: 1, Batch: 1570, Loss: 1.6156206130981445\n",
            "Epoch: 1, Batch: 1580, Loss: 1.6107505559921265\n",
            "Epoch: 1, Batch: 1590, Loss: 1.6393157243728638\n",
            "Epoch: 1, Batch: 1600, Loss: 1.6677772998809814\n",
            "Epoch: 1, Batch: 1610, Loss: 1.686524748802185\n",
            "Epoch: 1, Batch: 1620, Loss: 1.6118717193603516\n",
            "Epoch: 1, Batch: 1630, Loss: 1.6649692058563232\n",
            "Epoch: 1, Batch: 1640, Loss: 1.5804959535598755\n",
            "Epoch: 1, Batch: 1650, Loss: 1.5897337198257446\n",
            "Epoch: 1, Batch: 1660, Loss: 1.608338713645935\n",
            "Epoch: 1, Batch: 1670, Loss: 1.6557824611663818\n",
            "Epoch: 1, Batch: 1680, Loss: 1.6170982122421265\n",
            "Epoch: 1, Batch: 1690, Loss: 1.600853681564331\n",
            "Epoch: 1, Batch: 1700, Loss: 1.6071677207946777\n",
            "Epoch: 1, Batch: 1710, Loss: 1.5535469055175781\n",
            "Epoch: 1, Batch: 1720, Loss: 1.6518715620040894\n",
            "Epoch: 1, Batch: 1730, Loss: 1.5556679964065552\n",
            "Epoch: 1, Batch: 1740, Loss: 1.7302625179290771\n",
            "Epoch: 1, Batch: 1750, Loss: 1.5479226112365723\n",
            "Epoch: 1, Batch: 1760, Loss: 1.5127921104431152\n",
            "Epoch: 1, Batch: 1770, Loss: 1.5557332038879395\n",
            "Epoch: 1, Batch: 1780, Loss: 1.648363709449768\n",
            "Epoch: 1, Batch: 1790, Loss: 1.5701528787612915\n",
            "Epoch: 1, Batch: 1800, Loss: 1.5296281576156616\n",
            "Epoch: 1, Batch: 1810, Loss: 1.5708194971084595\n",
            "Epoch: 1, Batch: 1820, Loss: 1.5683021545410156\n",
            "Epoch: 1, Batch: 1830, Loss: 1.6423221826553345\n",
            "Epoch: 1, Batch: 1840, Loss: 1.6274436712265015\n",
            "Epoch: 1, Batch: 1850, Loss: 1.4959691762924194\n",
            "Epoch: 1, Batch: 1860, Loss: 1.5600696802139282\n",
            "Epoch: 1, Batch: 1870, Loss: 1.5561286211013794\n",
            "Epoch: 1, Batch: 1880, Loss: 1.564128041267395\n",
            "Epoch: 1, Batch: 1890, Loss: 1.594504952430725\n",
            "Epoch: 1, Batch: 1900, Loss: 1.510753870010376\n",
            "Epoch: 1, Batch: 1910, Loss: 1.507016897201538\n",
            "Epoch: 1, Batch: 1920, Loss: 1.490359902381897\n",
            "Epoch: 1, Batch: 1930, Loss: 1.5579231977462769\n",
            "Epoch: 1, Batch: 1940, Loss: 1.4368789196014404\n",
            "Epoch: 1, Batch: 1950, Loss: 1.53875732421875\n",
            "Epoch: 1, Batch: 1960, Loss: 1.5540744066238403\n",
            "Epoch: 1, Batch: 1970, Loss: 1.4795775413513184\n",
            "Epoch: 1, Batch: 1980, Loss: 1.5378953218460083\n",
            "Epoch: 1, Batch: 1990, Loss: 1.4882633686065674\n",
            "Epoch: 1, Batch: 2000, Loss: 1.439591407775879\n",
            "Epoch: 1, Batch: 2010, Loss: 1.5782101154327393\n",
            "Epoch: 1, Batch: 2020, Loss: 1.5081506967544556\n",
            "Epoch: 1, Batch: 2030, Loss: 1.4768565893173218\n",
            "Epoch: 1, Batch: 2040, Loss: 1.5515596866607666\n",
            "Epoch: 1, Batch: 2050, Loss: 1.5147143602371216\n",
            "Epoch: 1, Batch: 2060, Loss: 1.4601808786392212\n",
            "Epoch: 1, Batch: 2070, Loss: 1.4086424112319946\n",
            "Epoch: 1, Batch: 2080, Loss: 1.5062174797058105\n",
            "Epoch: 1, Batch: 2090, Loss: 1.5168136358261108\n",
            "Epoch: 1, Batch: 2100, Loss: 1.4604277610778809\n",
            "Epoch: 1, Batch: 2110, Loss: 1.5240848064422607\n",
            "Epoch: 1, Batch: 2120, Loss: 1.4711384773254395\n",
            "Epoch: 1, Batch: 2130, Loss: 1.4645252227783203\n",
            "Epoch: 1, Batch: 2140, Loss: 1.3965421915054321\n",
            "Epoch: 1, Batch: 2150, Loss: 1.4622106552124023\n",
            "Epoch: 1, Batch: 2160, Loss: 1.4405235052108765\n",
            "Epoch: 1, Batch: 2170, Loss: 1.4788894653320312\n",
            "Epoch: 1, Batch: 2180, Loss: 1.4462324380874634\n",
            "Epoch: 1, Batch: 2190, Loss: 1.4948376417160034\n",
            "Epoch: 1, Batch: 2200, Loss: 1.436708688735962\n",
            "Epoch: 1, Batch: 2210, Loss: 1.4379749298095703\n",
            "Epoch: 1, Batch: 2220, Loss: 1.3615370988845825\n",
            "Epoch: 1, Batch: 2230, Loss: 1.4421601295471191\n",
            "Epoch: 1, Batch: 2240, Loss: 1.4472101926803589\n",
            "Epoch: 1, Batch: 2250, Loss: 1.449478030204773\n",
            "Epoch: 1, Batch: 2260, Loss: 1.409738302230835\n",
            "Epoch: 1, Batch: 2270, Loss: 1.392763376235962\n",
            "Epoch: 1, Batch: 2280, Loss: 1.4871841669082642\n",
            "Epoch: 1, Batch: 2290, Loss: 1.4749211072921753\n",
            "Epoch: 1, Batch: 2300, Loss: 1.485329270362854\n",
            "Epoch: 1, Batch: 2310, Loss: 1.40230393409729\n",
            "Epoch: 1, Batch: 2320, Loss: 1.3782066106796265\n",
            "Epoch: 1, Batch: 2330, Loss: 1.3967756032943726\n",
            "Epoch: 1, Batch: 2340, Loss: 1.4368454217910767\n",
            "Epoch: 1, Batch: 2350, Loss: 1.4125351905822754\n",
            "Epoch: 1, Batch: 2360, Loss: 1.4372223615646362\n",
            "Epoch: 1, Batch: 2370, Loss: 1.4174202680587769\n",
            "Epoch: 1, Batch: 2380, Loss: 1.3557822704315186\n",
            "Epoch: 1, Batch: 2390, Loss: 1.3472245931625366\n",
            "Epoch: 1, Batch: 2400, Loss: 1.4271727800369263\n",
            "Epoch: 1, Batch: 2410, Loss: 1.3463078737258911\n",
            "Epoch: 1, Batch: 2420, Loss: 1.3407524824142456\n",
            "Epoch: 1, Batch: 2430, Loss: 1.3536452054977417\n",
            "Epoch: 1, Batch: 2440, Loss: 1.4030735492706299\n",
            "Epoch: 1, Batch: 2450, Loss: 1.3784078359603882\n",
            "Epoch: 1, Batch: 2460, Loss: 1.4400588274002075\n",
            "Epoch: 1, Batch: 2470, Loss: 1.4364917278289795\n",
            "Epoch: 1, Batch: 2480, Loss: 1.3977786302566528\n",
            "Epoch: 1, Batch: 2490, Loss: 1.435458779335022\n",
            "Epoch: 1, Batch: 2500, Loss: 1.4425430297851562\n",
            "Epoch: 1, Batch: 2510, Loss: 1.3672072887420654\n",
            "Epoch: 1, Batch: 2520, Loss: 1.3872591257095337\n",
            "Epoch: 1, Batch: 2530, Loss: 1.3838281631469727\n",
            "Epoch: 1, Batch: 2540, Loss: 1.380092740058899\n",
            "Epoch: 1, Batch: 2550, Loss: 1.3728638887405396\n",
            "Epoch: 1, Batch: 2560, Loss: 1.4096667766571045\n",
            "Epoch: 1, Batch: 2570, Loss: 1.4083149433135986\n",
            "Epoch: 1, Batch: 2580, Loss: 1.3201476335525513\n",
            "Epoch: 1, Batch: 2590, Loss: 1.3953032493591309\n",
            "Epoch: 1, Batch: 2600, Loss: 1.335929274559021\n",
            "Epoch: 1, Batch: 2610, Loss: 1.3551709651947021\n",
            "Epoch: 1, Batch: 2620, Loss: 1.362430214881897\n",
            "Epoch: 1, Batch: 2630, Loss: 1.3576622009277344\n",
            "Epoch: 1 finished with average loss: 1.76272407021423\n",
            "Epoch: 2, Batch: 0, Loss: 1.468442678451538\n",
            "Epoch: 2, Batch: 10, Loss: 1.3789926767349243\n",
            "Epoch: 2, Batch: 20, Loss: 1.3670973777770996\n",
            "Epoch: 2, Batch: 30, Loss: 1.3436962366104126\n",
            "Epoch: 2, Batch: 40, Loss: 1.3440217971801758\n",
            "Epoch: 2, Batch: 50, Loss: 1.3597160577774048\n",
            "Epoch: 2, Batch: 60, Loss: 1.4383735656738281\n",
            "Epoch: 2, Batch: 70, Loss: 1.3135590553283691\n",
            "Epoch: 2, Batch: 80, Loss: 1.4678486585617065\n",
            "Epoch: 2, Batch: 90, Loss: 1.2945895195007324\n",
            "Epoch: 2, Batch: 100, Loss: 1.3706578016281128\n",
            "Epoch: 2, Batch: 110, Loss: 1.384353756904602\n",
            "Epoch: 2, Batch: 120, Loss: 1.348878026008606\n",
            "Epoch: 2, Batch: 130, Loss: 1.3608111143112183\n",
            "Epoch: 2, Batch: 140, Loss: 1.262223720550537\n",
            "Epoch: 2, Batch: 150, Loss: 1.3172452449798584\n",
            "Epoch: 2, Batch: 160, Loss: 1.4241750240325928\n",
            "Epoch: 2, Batch: 170, Loss: 1.278033971786499\n",
            "Epoch: 2, Batch: 180, Loss: 1.3094985485076904\n",
            "Epoch: 2, Batch: 190, Loss: 1.403020977973938\n",
            "Epoch: 2, Batch: 200, Loss: 1.2176707983016968\n",
            "Epoch: 2, Batch: 210, Loss: 1.2523808479309082\n",
            "Epoch: 2, Batch: 220, Loss: 1.347663164138794\n",
            "Epoch: 2, Batch: 230, Loss: 1.2794063091278076\n",
            "Epoch: 2, Batch: 240, Loss: 1.266552209854126\n",
            "Epoch: 2, Batch: 250, Loss: 1.2636624574661255\n",
            "Epoch: 2, Batch: 260, Loss: 1.298553466796875\n",
            "Epoch: 2, Batch: 270, Loss: 1.314237356185913\n",
            "Epoch: 2, Batch: 280, Loss: 1.3003966808319092\n",
            "Epoch: 2, Batch: 290, Loss: 1.2646549940109253\n",
            "Epoch: 2, Batch: 300, Loss: 1.3187589645385742\n",
            "Epoch: 2, Batch: 310, Loss: 1.2904846668243408\n",
            "Epoch: 2, Batch: 320, Loss: 1.3605157136917114\n",
            "Epoch: 2, Batch: 330, Loss: 1.3811225891113281\n",
            "Epoch: 2, Batch: 340, Loss: 1.2865208387374878\n",
            "Epoch: 2, Batch: 350, Loss: 1.2817803621292114\n",
            "Epoch: 2, Batch: 360, Loss: 1.2596780061721802\n",
            "Epoch: 2, Batch: 370, Loss: 1.3178566694259644\n",
            "Epoch: 2, Batch: 380, Loss: 1.233850121498108\n",
            "Epoch: 2, Batch: 390, Loss: 1.226083517074585\n",
            "Epoch: 2, Batch: 400, Loss: 1.2914342880249023\n",
            "Epoch: 2, Batch: 410, Loss: 1.3517755270004272\n",
            "Epoch: 2, Batch: 420, Loss: 1.269135594367981\n",
            "Epoch: 2, Batch: 430, Loss: 1.293614149093628\n",
            "Epoch: 2, Batch: 440, Loss: 1.2404142618179321\n",
            "Epoch: 2, Batch: 450, Loss: 1.2571603059768677\n",
            "Epoch: 2, Batch: 460, Loss: 1.3019238710403442\n",
            "Epoch: 2, Batch: 470, Loss: 1.257266879081726\n",
            "Epoch: 2, Batch: 480, Loss: 1.2378162145614624\n",
            "Epoch: 2, Batch: 490, Loss: 1.2758893966674805\n",
            "Epoch: 2, Batch: 500, Loss: 1.2485270500183105\n",
            "Epoch: 2, Batch: 510, Loss: 1.2908645868301392\n",
            "Epoch: 2, Batch: 520, Loss: 1.3227020502090454\n",
            "Epoch: 2, Batch: 530, Loss: 1.3614857196807861\n",
            "Epoch: 2, Batch: 540, Loss: 1.2191208600997925\n",
            "Epoch: 2, Batch: 550, Loss: 1.2765471935272217\n",
            "Epoch: 2, Batch: 560, Loss: 1.2918211221694946\n",
            "Epoch: 2, Batch: 570, Loss: 1.319236159324646\n",
            "Epoch: 2, Batch: 580, Loss: 1.2935147285461426\n",
            "Epoch: 2, Batch: 590, Loss: 1.2465150356292725\n",
            "Epoch: 2, Batch: 600, Loss: 1.2455825805664062\n",
            "Epoch: 2, Batch: 610, Loss: 1.2289913892745972\n",
            "Epoch: 2, Batch: 620, Loss: 1.3126741647720337\n",
            "Epoch: 2, Batch: 630, Loss: 1.2263104915618896\n",
            "Epoch: 2, Batch: 640, Loss: 1.2369269132614136\n",
            "Epoch: 2, Batch: 650, Loss: 1.2157188653945923\n",
            "Epoch: 2, Batch: 660, Loss: 1.258638858795166\n",
            "Epoch: 2, Batch: 670, Loss: 1.2259814739227295\n",
            "Epoch: 2, Batch: 680, Loss: 1.3494858741760254\n",
            "Epoch: 2, Batch: 690, Loss: 1.2636992931365967\n",
            "Epoch: 2, Batch: 700, Loss: 1.178255319595337\n",
            "Epoch: 2, Batch: 710, Loss: 1.2058898210525513\n",
            "Epoch: 2, Batch: 720, Loss: 1.2404500246047974\n",
            "Epoch: 2, Batch: 730, Loss: 1.2391166687011719\n",
            "Epoch: 2, Batch: 740, Loss: 1.2821545600891113\n",
            "Epoch: 2, Batch: 750, Loss: 1.2400823831558228\n",
            "Epoch: 2, Batch: 760, Loss: 1.296277642250061\n",
            "Epoch: 2, Batch: 770, Loss: 1.2467249631881714\n",
            "Epoch: 2, Batch: 780, Loss: 1.302119493484497\n",
            "Epoch: 2, Batch: 790, Loss: 1.2916805744171143\n",
            "Epoch: 2, Batch: 800, Loss: 1.2178239822387695\n",
            "Epoch: 2, Batch: 810, Loss: 1.2552406787872314\n",
            "Epoch: 2, Batch: 820, Loss: 1.2043958902359009\n",
            "Epoch: 2, Batch: 830, Loss: 1.3335641622543335\n",
            "Epoch: 2, Batch: 840, Loss: 1.183814525604248\n",
            "Epoch: 2, Batch: 850, Loss: 1.211805820465088\n",
            "Epoch: 2, Batch: 860, Loss: 1.2123174667358398\n",
            "Epoch: 2, Batch: 870, Loss: 1.2251248359680176\n",
            "Epoch: 2, Batch: 880, Loss: 1.2650662660598755\n",
            "Epoch: 2, Batch: 890, Loss: 1.3060473203659058\n",
            "Epoch: 2, Batch: 900, Loss: 1.2308140993118286\n",
            "Epoch: 2, Batch: 910, Loss: 1.2171999216079712\n",
            "Epoch: 2, Batch: 920, Loss: 1.2009652853012085\n",
            "Epoch: 2, Batch: 930, Loss: 1.373868465423584\n",
            "Epoch: 2, Batch: 940, Loss: 1.1649718284606934\n",
            "Epoch: 2, Batch: 950, Loss: 1.2074930667877197\n",
            "Epoch: 2, Batch: 960, Loss: 1.2157469987869263\n",
            "Epoch: 2, Batch: 970, Loss: 1.2162320613861084\n",
            "Epoch: 2, Batch: 980, Loss: 1.2619205713272095\n",
            "Epoch: 2, Batch: 990, Loss: 1.1922165155410767\n",
            "Epoch: 2, Batch: 1000, Loss: 1.249226689338684\n",
            "Epoch: 2, Batch: 1010, Loss: 1.1750584840774536\n",
            "Epoch: 2, Batch: 1020, Loss: 1.1657181978225708\n",
            "Epoch: 2, Batch: 1030, Loss: 1.22347092628479\n",
            "Epoch: 2, Batch: 1040, Loss: 1.1625680923461914\n",
            "Epoch: 2, Batch: 1050, Loss: 1.218885898590088\n",
            "Epoch: 2, Batch: 1060, Loss: 1.1401290893554688\n",
            "Epoch: 2, Batch: 1070, Loss: 1.135292410850525\n",
            "Epoch: 2, Batch: 1080, Loss: 1.2059193849563599\n",
            "Epoch: 2, Batch: 1090, Loss: 1.2086741924285889\n",
            "Epoch: 2, Batch: 1100, Loss: 1.1788781881332397\n",
            "Epoch: 2, Batch: 1110, Loss: 1.2412526607513428\n",
            "Epoch: 2, Batch: 1120, Loss: 1.1803680658340454\n",
            "Epoch: 2, Batch: 1130, Loss: 1.1678624153137207\n",
            "Epoch: 2, Batch: 1140, Loss: 1.170480489730835\n",
            "Epoch: 2, Batch: 1150, Loss: 1.2611472606658936\n",
            "Epoch: 2, Batch: 1160, Loss: 1.2607935667037964\n",
            "Epoch: 2, Batch: 1170, Loss: 1.1787266731262207\n",
            "Epoch: 2, Batch: 1180, Loss: 1.1779282093048096\n",
            "Epoch: 2, Batch: 1190, Loss: 1.2096867561340332\n",
            "Epoch: 2, Batch: 1200, Loss: 1.1867026090621948\n",
            "Epoch: 2, Batch: 1210, Loss: 1.232340693473816\n",
            "Epoch: 2, Batch: 1220, Loss: 1.0839149951934814\n",
            "Epoch: 2, Batch: 1230, Loss: 1.231271743774414\n",
            "Epoch: 2, Batch: 1240, Loss: 1.1854965686798096\n",
            "Epoch: 2, Batch: 1250, Loss: 1.205405831336975\n",
            "Epoch: 2, Batch: 1260, Loss: 1.1698791980743408\n",
            "Epoch: 2, Batch: 1270, Loss: 1.1728907823562622\n",
            "Epoch: 2, Batch: 1280, Loss: 1.1243900060653687\n",
            "Epoch: 2, Batch: 1290, Loss: 1.161621332168579\n",
            "Epoch: 2, Batch: 1300, Loss: 1.2052953243255615\n",
            "Epoch: 2, Batch: 1310, Loss: 1.1923695802688599\n",
            "Epoch: 2, Batch: 1320, Loss: 1.1972224712371826\n",
            "Epoch: 2, Batch: 1330, Loss: 1.194618821144104\n",
            "Epoch: 2, Batch: 1340, Loss: 1.1629455089569092\n",
            "Epoch: 2, Batch: 1350, Loss: 1.1820402145385742\n",
            "Epoch: 2, Batch: 1360, Loss: 1.1676503419876099\n",
            "Epoch: 2, Batch: 1370, Loss: 1.1289737224578857\n",
            "Epoch: 2, Batch: 1380, Loss: 1.1330145597457886\n",
            "Epoch: 2, Batch: 1390, Loss: 1.1669195890426636\n",
            "Epoch: 2, Batch: 1400, Loss: 1.1713930368423462\n",
            "Epoch: 2, Batch: 1410, Loss: 1.1304317712783813\n",
            "Epoch: 2, Batch: 1420, Loss: 1.2124220132827759\n",
            "Epoch: 2, Batch: 1430, Loss: 1.2434403896331787\n",
            "Epoch: 2, Batch: 1440, Loss: 1.1676428318023682\n",
            "Epoch: 2, Batch: 1450, Loss: 1.1899745464324951\n",
            "Epoch: 2, Batch: 1460, Loss: 1.1626819372177124\n",
            "Epoch: 2, Batch: 1470, Loss: 1.1612036228179932\n",
            "Epoch: 2, Batch: 1480, Loss: 1.1657934188842773\n",
            "Epoch: 2, Batch: 1490, Loss: 1.1360970735549927\n",
            "Epoch: 2, Batch: 1500, Loss: 1.2456899881362915\n",
            "Epoch: 2, Batch: 1510, Loss: 1.0992485284805298\n",
            "Epoch: 2, Batch: 1520, Loss: 1.1222494840621948\n",
            "Epoch: 2, Batch: 1530, Loss: 1.1812156438827515\n",
            "Epoch: 2, Batch: 1540, Loss: 1.071764349937439\n",
            "Epoch: 2, Batch: 1550, Loss: 1.128682255744934\n",
            "Epoch: 2, Batch: 1560, Loss: 1.1158150434494019\n",
            "Epoch: 2, Batch: 1570, Loss: 1.1274694204330444\n",
            "Epoch: 2, Batch: 1580, Loss: 1.1694438457489014\n",
            "Epoch: 2, Batch: 1590, Loss: 1.1968865394592285\n",
            "Epoch: 2, Batch: 1600, Loss: 1.1100969314575195\n",
            "Epoch: 2, Batch: 1610, Loss: 1.1921693086624146\n",
            "Epoch: 2, Batch: 1620, Loss: 1.1134283542633057\n",
            "Epoch: 2, Batch: 1630, Loss: 1.2459442615509033\n",
            "Epoch: 2, Batch: 1640, Loss: 1.094908595085144\n",
            "Epoch: 2, Batch: 1650, Loss: 1.1764456033706665\n",
            "Epoch: 2, Batch: 1660, Loss: 1.1191636323928833\n",
            "Epoch: 2, Batch: 1670, Loss: 1.1326791048049927\n",
            "Epoch: 2, Batch: 1680, Loss: 1.094333529472351\n",
            "Epoch: 2, Batch: 1690, Loss: 1.1416467428207397\n",
            "Epoch: 2, Batch: 1700, Loss: 1.1859437227249146\n",
            "Epoch: 2, Batch: 1710, Loss: 1.1066710948944092\n",
            "Epoch: 2, Batch: 1720, Loss: 1.15036141872406\n",
            "Epoch: 2, Batch: 1730, Loss: 1.1174912452697754\n",
            "Epoch: 2, Batch: 1740, Loss: 1.173862338066101\n",
            "Epoch: 2, Batch: 1750, Loss: 1.1263728141784668\n",
            "Epoch: 2, Batch: 1760, Loss: 1.1896212100982666\n",
            "Epoch: 2, Batch: 1770, Loss: 1.138928771018982\n",
            "Epoch: 2, Batch: 1780, Loss: 1.1420350074768066\n",
            "Epoch: 2, Batch: 1790, Loss: 1.1316357851028442\n",
            "Epoch: 2, Batch: 1800, Loss: 1.2084639072418213\n",
            "Epoch: 2, Batch: 1810, Loss: 1.1073991060256958\n",
            "Epoch: 2, Batch: 1820, Loss: 1.1512006521224976\n",
            "Epoch: 2, Batch: 1830, Loss: 1.1463885307312012\n",
            "Epoch: 2, Batch: 1840, Loss: 1.1388190984725952\n",
            "Epoch: 2, Batch: 1850, Loss: 1.1587846279144287\n",
            "Epoch: 2, Batch: 1860, Loss: 1.1294305324554443\n",
            "Epoch: 2, Batch: 1870, Loss: 1.0627208948135376\n",
            "Epoch: 2, Batch: 1880, Loss: 1.1238079071044922\n",
            "Epoch: 2, Batch: 1890, Loss: 1.150345802307129\n",
            "Epoch: 2, Batch: 1900, Loss: 1.0921790599822998\n",
            "Epoch: 2, Batch: 1910, Loss: 1.1350364685058594\n",
            "Epoch: 2, Batch: 1920, Loss: 1.1582757234573364\n",
            "Epoch: 2, Batch: 1930, Loss: 1.119553804397583\n",
            "Epoch: 2, Batch: 1940, Loss: 1.0739022493362427\n",
            "Epoch: 2, Batch: 1950, Loss: 1.138346791267395\n",
            "Epoch: 2, Batch: 1960, Loss: 1.1567341089248657\n",
            "Epoch: 2, Batch: 1970, Loss: 1.0735925436019897\n",
            "Epoch: 2, Batch: 1980, Loss: 1.161697268486023\n",
            "Epoch: 2, Batch: 1990, Loss: 1.1456685066223145\n",
            "Epoch: 2, Batch: 2000, Loss: 1.1463648080825806\n",
            "Epoch: 2, Batch: 2010, Loss: 1.17954683303833\n",
            "Epoch: 2, Batch: 2020, Loss: 1.1344541311264038\n",
            "Epoch: 2, Batch: 2030, Loss: 1.1143207550048828\n",
            "Epoch: 2, Batch: 2040, Loss: 1.0849195718765259\n",
            "Epoch: 2, Batch: 2050, Loss: 1.0251274108886719\n",
            "Epoch: 2, Batch: 2060, Loss: 1.093249797821045\n",
            "Epoch: 2, Batch: 2070, Loss: 1.0818324089050293\n",
            "Epoch: 2, Batch: 2080, Loss: 1.107337474822998\n",
            "Epoch: 2, Batch: 2090, Loss: 1.140588641166687\n",
            "Epoch: 2, Batch: 2100, Loss: 1.072307825088501\n",
            "Epoch: 2, Batch: 2110, Loss: 1.072218656539917\n",
            "Epoch: 2, Batch: 2120, Loss: 1.1475003957748413\n",
            "Epoch: 2, Batch: 2130, Loss: 1.1358860731124878\n",
            "Epoch: 2, Batch: 2140, Loss: 1.0654929876327515\n",
            "Epoch: 2, Batch: 2150, Loss: 1.0997792482376099\n",
            "Epoch: 2, Batch: 2160, Loss: 1.1343202590942383\n",
            "Epoch: 2, Batch: 2170, Loss: 1.1031321287155151\n",
            "Epoch: 2, Batch: 2180, Loss: 1.1112210750579834\n",
            "Epoch: 2, Batch: 2190, Loss: 1.1333799362182617\n",
            "Epoch: 2, Batch: 2200, Loss: 1.1683173179626465\n",
            "Epoch: 2, Batch: 2210, Loss: 1.0449211597442627\n",
            "Epoch: 2, Batch: 2220, Loss: 1.1032679080963135\n",
            "Epoch: 2, Batch: 2230, Loss: 1.0839958190917969\n",
            "Epoch: 2, Batch: 2240, Loss: 1.0704034566879272\n",
            "Epoch: 2, Batch: 2250, Loss: 1.0838806629180908\n",
            "Epoch: 2, Batch: 2260, Loss: 1.0605398416519165\n",
            "Epoch: 2, Batch: 2270, Loss: 1.0834978818893433\n",
            "Epoch: 2, Batch: 2280, Loss: 1.0662355422973633\n",
            "Epoch: 2, Batch: 2290, Loss: 1.1284254789352417\n",
            "Epoch: 2, Batch: 2300, Loss: 1.133360505104065\n",
            "Epoch: 2, Batch: 2310, Loss: 1.0550224781036377\n",
            "Epoch: 2, Batch: 2320, Loss: 1.0944558382034302\n",
            "Epoch: 2, Batch: 2330, Loss: 1.1005613803863525\n",
            "Epoch: 2, Batch: 2340, Loss: 1.1181905269622803\n",
            "Epoch: 2, Batch: 2350, Loss: 1.1702991724014282\n",
            "Epoch: 2, Batch: 2360, Loss: 1.093760371208191\n",
            "Epoch: 2, Batch: 2370, Loss: 1.0767017602920532\n",
            "Epoch: 2, Batch: 2380, Loss: 1.0410455465316772\n",
            "Epoch: 2, Batch: 2390, Loss: 1.13893461227417\n",
            "Epoch: 2, Batch: 2400, Loss: 1.09670090675354\n",
            "Epoch: 2, Batch: 2410, Loss: 1.17213773727417\n",
            "Epoch: 2, Batch: 2420, Loss: 1.0915534496307373\n",
            "Epoch: 2, Batch: 2430, Loss: 1.132728934288025\n",
            "Epoch: 2, Batch: 2440, Loss: 1.1268999576568604\n",
            "Epoch: 2, Batch: 2450, Loss: 1.1235806941986084\n",
            "Epoch: 2, Batch: 2460, Loss: 1.0486584901809692\n",
            "Epoch: 2, Batch: 2470, Loss: 1.0347391366958618\n",
            "Epoch: 2, Batch: 2480, Loss: 1.1410247087478638\n",
            "Epoch: 2, Batch: 2490, Loss: 1.1196262836456299\n",
            "Epoch: 2, Batch: 2500, Loss: 1.0922154188156128\n",
            "Epoch: 2, Batch: 2510, Loss: 1.083991289138794\n",
            "Epoch: 2, Batch: 2520, Loss: 1.0527149438858032\n",
            "Epoch: 2, Batch: 2530, Loss: 1.0721654891967773\n",
            "Epoch: 2, Batch: 2540, Loss: 1.1633559465408325\n",
            "Epoch: 2, Batch: 2550, Loss: 1.061252236366272\n",
            "Epoch: 2, Batch: 2560, Loss: 1.1178843975067139\n",
            "Epoch: 2, Batch: 2570, Loss: 1.057930827140808\n",
            "Epoch: 2, Batch: 2580, Loss: 1.035618543624878\n",
            "Epoch: 2, Batch: 2590, Loss: 1.0410958528518677\n",
            "Epoch: 2, Batch: 2600, Loss: 1.1173322200775146\n",
            "Epoch: 2, Batch: 2610, Loss: 1.0559759140014648\n",
            "Epoch: 2, Batch: 2620, Loss: 1.0920453071594238\n",
            "Epoch: 2, Batch: 2630, Loss: 1.1027079820632935\n",
            "Epoch: 2 finished with average loss: 1.1925456987612615\n",
            "Model saved to ./my_pretrained_bert_model.pth\n"
          ]
        }
      ]
    }
  ]
}