{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0302064c163f41bf889dc633937bcf9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_878f8ae129174430988b055805122203",
              "IPY_MODEL_eef2a68ba5564b9f83f65b7a0cbb52c4",
              "IPY_MODEL_fe4566cd6c464b3caa2094effadb5786"
            ],
            "layout": "IPY_MODEL_e14373525b1a4a959a5589b6ab68a59b"
          }
        },
        "878f8ae129174430988b055805122203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c407e323f4a745e3ab27da2734ae1b05",
            "placeholder": "​",
            "style": "IPY_MODEL_2d8bfc412d3a42e2abe4b2363db14c8a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "eef2a68ba5564b9f83f65b7a0cbb52c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b9c6acef47c44e1aab5efcc4b0138f7",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f065fca2233a41fa96c156fdc5e7498e",
            "value": 48
          }
        },
        "fe4566cd6c464b3caa2094effadb5786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_854c805085314339ba77013a45e31777",
            "placeholder": "​",
            "style": "IPY_MODEL_1fc5cd79911f44a6b50631454be875b1",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.37kB/s]"
          }
        },
        "e14373525b1a4a959a5589b6ab68a59b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c407e323f4a745e3ab27da2734ae1b05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d8bfc412d3a42e2abe4b2363db14c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b9c6acef47c44e1aab5efcc4b0138f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f065fca2233a41fa96c156fdc5e7498e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "854c805085314339ba77013a45e31777": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fc5cd79911f44a6b50631454be875b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76adb4520c664c92bed6b42f1c6e63bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ca095f5d15245c2b7ed90d3d09b1c1d",
              "IPY_MODEL_2cf947dbd5ec4881af5a4e19a74b8453",
              "IPY_MODEL_aeb26709e5934ec7af5fe8a87efc87ff"
            ],
            "layout": "IPY_MODEL_74c28e50f4c848aaa900d0a98b09f098"
          }
        },
        "8ca095f5d15245c2b7ed90d3d09b1c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_335bee19f8d442b39464358ea8e41ffd",
            "placeholder": "​",
            "style": "IPY_MODEL_142b408585cf49688fa8b594fe1f2f65",
            "value": "vocab.txt: 100%"
          }
        },
        "2cf947dbd5ec4881af5a4e19a74b8453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_669da10101de4500b7ae33d9805b6599",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17c1b63db475474c9e8633a0cb9d5e98",
            "value": 231508
          }
        },
        "aeb26709e5934ec7af5fe8a87efc87ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4e4dbbcf3a34237a64e71d279f5b8cb",
            "placeholder": "​",
            "style": "IPY_MODEL_dab1544249cb44da93de512c53f5cd93",
            "value": " 232k/232k [00:00&lt;00:00, 4.82MB/s]"
          }
        },
        "74c28e50f4c848aaa900d0a98b09f098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "335bee19f8d442b39464358ea8e41ffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "142b408585cf49688fa8b594fe1f2f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "669da10101de4500b7ae33d9805b6599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17c1b63db475474c9e8633a0cb9d5e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4e4dbbcf3a34237a64e71d279f5b8cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dab1544249cb44da93de512c53f5cd93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eca873460ee4421b67b09dcf9a8ddea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c440e041b8344a2ea33454e45ad817c6",
              "IPY_MODEL_decac242034e436b9ed3d6c15d3a13a4",
              "IPY_MODEL_dd77aa9a0f5049459dce27e208f2a7ad"
            ],
            "layout": "IPY_MODEL_d15400201ff14ad5a777b5258cbc3148"
          }
        },
        "c440e041b8344a2ea33454e45ad817c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e36e82bef1924778b599c1050a51fe9f",
            "placeholder": "​",
            "style": "IPY_MODEL_67b0adcc4f394804a1c528be2135189d",
            "value": "tokenizer.json: 100%"
          }
        },
        "decac242034e436b9ed3d6c15d3a13a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_116d3614c5d84a579b44e14e83787122",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11e664dca26f473c96570a2df6f9b89e",
            "value": 466062
          }
        },
        "dd77aa9a0f5049459dce27e208f2a7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff9bac4e87345ebb5cff28ad0b98286",
            "placeholder": "​",
            "style": "IPY_MODEL_6ac4a31a5cb649babf33ec7a930cee92",
            "value": " 466k/466k [00:00&lt;00:00, 24.3MB/s]"
          }
        },
        "d15400201ff14ad5a777b5258cbc3148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e36e82bef1924778b599c1050a51fe9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67b0adcc4f394804a1c528be2135189d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "116d3614c5d84a579b44e14e83787122": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11e664dca26f473c96570a2df6f9b89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aff9bac4e87345ebb5cff28ad0b98286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ac4a31a5cb649babf33ec7a930cee92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ef803de2a0f4cc98ec4dbf18dd346b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_819aa74e54bd473c9ea5522402ea23a2",
              "IPY_MODEL_1d45b3a7d7a1492fa21d1453af86c609",
              "IPY_MODEL_d905bd2344d540259e4caa988ffde820"
            ],
            "layout": "IPY_MODEL_5b384e6f3b284e439075602b31c125be"
          }
        },
        "819aa74e54bd473c9ea5522402ea23a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_206183d1c4f2442991f61528c7721e61",
            "placeholder": "​",
            "style": "IPY_MODEL_130b125f278044e3816bc92740cc092e",
            "value": "config.json: 100%"
          }
        },
        "1d45b3a7d7a1492fa21d1453af86c609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_398c510771ab4f108ce2a5c905477f5f",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6061cb5e1a2c4cbb927003c1bb19ab03",
            "value": 570
          }
        },
        "d905bd2344d540259e4caa988ffde820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79b039108f04404faecbc83acc666462",
            "placeholder": "​",
            "style": "IPY_MODEL_7b57d4acc9994a16ab23f8dfba0bfcdd",
            "value": " 570/570 [00:00&lt;00:00, 48.1kB/s]"
          }
        },
        "5b384e6f3b284e439075602b31c125be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "206183d1c4f2442991f61528c7721e61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "130b125f278044e3816bc92740cc092e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "398c510771ab4f108ce2a5c905477f5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6061cb5e1a2c4cbb927003c1bb19ab03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79b039108f04404faecbc83acc666462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b57d4acc9994a16ab23f8dfba0bfcdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfc8b943ab5346f78c23f7dcdacf9ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86c714eae7a54e89b03c1f4ec392e5c1",
              "IPY_MODEL_f57592b13cff4afbba4e54e5963469af",
              "IPY_MODEL_f9a1136f3e854de6be9668f87a6e672a"
            ],
            "layout": "IPY_MODEL_56980aadb7d24f66b4b7a7c910f15405"
          }
        },
        "86c714eae7a54e89b03c1f4ec392e5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0b54ce82f164e979f02fd76cb6209d7",
            "placeholder": "​",
            "style": "IPY_MODEL_3b962fc22a1342999defb170196e1044",
            "value": "model.safetensors: 100%"
          }
        },
        "f57592b13cff4afbba4e54e5963469af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47aaefaebbf342ef9f4e2bc85c41684b",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92e520bebaa64e3e9e246c6213ddfa5e",
            "value": 440449768
          }
        },
        "f9a1136f3e854de6be9668f87a6e672a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2ab601273354a9f88da487fb181862a",
            "placeholder": "​",
            "style": "IPY_MODEL_6410f0b6f6dc49d0b3ae25d55fdd4327",
            "value": " 440M/440M [00:02&lt;00:00, 243MB/s]"
          }
        },
        "56980aadb7d24f66b4b7a7c910f15405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0b54ce82f164e979f02fd76cb6209d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b962fc22a1342999defb170196e1044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47aaefaebbf342ef9f4e2bc85c41684b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92e520bebaa64e3e9e246c6213ddfa5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2ab601273354a9f88da487fb181862a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6410f0b6f6dc49d0b3ae25d55fdd4327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwonjihan/ML-teamproject/blob/develop/BERT_Embedding_Modified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path='/content/drive/MyDrive/Colab Notebooks/6000_IMDB_Dataset.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfRNbEvV-8tp",
        "outputId": "691ca072-f12c-4fca-808e-c0c24970c911"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from typing import Optional, Tuple\n",
        "import re"
      ],
      "metadata": {
        "id": "woLaLsvCPFAi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NRCLex\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "CgRXfpJiLsAz",
        "outputId": "3a1b16b3-a721-4b63-8799-daa66d60b03e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting NRCLex\n",
            "  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from NRCLex) (0.17.1)\n",
            "INFO: pip is looking at multiple versions of nrclex to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob->NRCLex) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob->NRCLex) (4.66.4)\n",
            "Building wheels for collected packages: NRCLex\n",
            "  Building wheel for NRCLex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43309 sha256=d5576e5f4d58a787c81c08b6b48794f788c819b00483b15b7e84061282d8e438\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/10/44/6abfb1234298806a145fd6bcaec8cbc712e88dd1cd6cb242fa\n",
            "Successfully built NRCLex\n",
            "Installing collected packages: NRCLex\n",
            "Successfully installed NRCLex-3.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 감정에 해당하는 토큰 임베딩에 큰 포션을 취하는 방법\n",
        "- 1. 우선 문장을 토큰화하기 전에 감정이 드러난 단어를 찾는다\n",
        "- 2. 해당 단어를 버트 토크나이저를 통해 토크나이즈 된 결과를 저장해둔다\n",
        "- 3. 만약 감정이 있다면 해당 단어가 벡터로 표현되고 합해져서 임베딩 될 때 더 크게 영향을 주도록 한다."
      ],
      "metadata": {
        "id": "px00_B1UNyv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 문장에서 단어 감정 추출 예시 ##\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def analyze_sentence(sentence):\n",
        "    # 문장을 토큰화\n",
        "    words = word_tokenize(sentence)\n",
        "\n",
        "    # VADER 초기화\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # 감정이 포함된 단어들 저장\n",
        "    positive_words = []\n",
        "    negative_words = []\n",
        "\n",
        "    # 각 단어의 감정 점수 분석\n",
        "    for word in words:\n",
        "        score = analyzer.polarity_scores(word)['compound']\n",
        "        if score > 0:\n",
        "            positive_words.append(word)\n",
        "        elif score < 0:\n",
        "            negative_words.append(word)\n",
        "\n",
        "    return positive_words, negative_words\n",
        "\n",
        "# 테스트 문장\n",
        "sentence = \"Petter Mattei's Love in the Time of Money is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter.\"\n",
        "positive_words, negative_words = analyze_sentence(sentence)\n",
        "\n",
        "print(\"Positive words:\", positive_words)\n",
        "print(\"Negative words:\", negative_words)\n"
      ],
      "metadata": {
        "id": "rGZfCrtcI7Ba",
        "outputId": "793755f6-c559-46df-8345-f6cf79e889df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive words: ['Love', 'stunning', 'success']\n",
            "Negative words: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.우선 문장을 토큰화하기 전에 감정이 드러난 단어를 찾는다\n",
        "## 2.해당 단어를 버트 토크나이저를 통해 토크나이즈 된 결과를 저장해둔다"
      ],
      "metadata": {
        "id": "ZzetDrS4Op6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 #\n",
        "import itertools\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = re.sub(r'<[^>]+>', ' ', self.texts[idx])  # HTML 태그 제거\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # 문장을 토큰화하고 감정 점수를 분석\n",
        "        words = word_tokenize(text)\n",
        "        sentiment_words = []\n",
        "        for word in words:\n",
        "            score = self.analyzer.polarity_scores(word)['compound']\n",
        "            if score > 0:\n",
        "                sentiment_words.append(word)\n",
        "            elif score < 0:\n",
        "                sentiment_words.append(word)\n",
        "\n",
        "\n",
        "        # 감정 단어들을 토큰화\n",
        "        tokenized_sentiment_words = []\n",
        "        for word in sentiment_words:\n",
        "            tokenized = self.tokenizer(word, add_special_tokens=False)['input_ids']\n",
        "            tokenized_sentiment_words.append(tokenized) # 토큰화된 감정단어들\n",
        "\n",
        "        # '##' 붙은 서브워드 제거\n",
        "        filtered_tokens = []\n",
        "        for token_ids in tokenized_sentiment_words:\n",
        "            tokens = self.tokenizer.convert_ids_to_tokens(token_ids)\n",
        "            filtered_tokens.extend([token for token in tokens if not token.startswith('##')])\n",
        "\n",
        "        # 토큰들을 하나의 텍스트로 이어붙이기\n",
        "        result_text = ' '.join(filtered_tokens)\n",
        "\n",
        "        # 토큰화 및 인코딩\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "        token_type_ids = encoding['token_type_ids'].squeeze()\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            result_text,\n",
        "            add_special_tokens=False,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        sentiment_tokens = encoding['input_ids'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'token_type_ids': token_type_ids,\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            #'sentiment_words' :  sentiment_words,\n",
        "            #'tokenized_sentiment_words' : tokenized_sentiment_words,\n",
        "            \"sentiment_tokens\" :  sentiment_tokens,\n",
        "        }\n",
        "\n",
        "def load_data(file_path, tokenizer, max_length=128):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "    texts = df['review'].tolist()   # 리스트 요소 하나에 풀 문장이 들어있음\n",
        "    labels = df['sentiment'].tolist()\n",
        "    print(texts[0])\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
        "    print(train_texts[0])\n",
        "\n",
        "    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "    return train_dataset, val_dataset\n"
      ],
      "metadata": {
        "id": "CIQbjs9wOs-p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CPKCGzikOsnP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zuK6Nz5JO1ip",
        "outputId": "b76cabb6-9e3e-4e69-895b-01f68ca8b0a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352,
          "referenced_widgets": [
            "0302064c163f41bf889dc633937bcf9a",
            "878f8ae129174430988b055805122203",
            "eef2a68ba5564b9f83f65b7a0cbb52c4",
            "fe4566cd6c464b3caa2094effadb5786",
            "e14373525b1a4a959a5589b6ab68a59b",
            "c407e323f4a745e3ab27da2734ae1b05",
            "2d8bfc412d3a42e2abe4b2363db14c8a",
            "5b9c6acef47c44e1aab5efcc4b0138f7",
            "f065fca2233a41fa96c156fdc5e7498e",
            "854c805085314339ba77013a45e31777",
            "1fc5cd79911f44a6b50631454be875b1",
            "76adb4520c664c92bed6b42f1c6e63bb",
            "8ca095f5d15245c2b7ed90d3d09b1c1d",
            "2cf947dbd5ec4881af5a4e19a74b8453",
            "aeb26709e5934ec7af5fe8a87efc87ff",
            "74c28e50f4c848aaa900d0a98b09f098",
            "335bee19f8d442b39464358ea8e41ffd",
            "142b408585cf49688fa8b594fe1f2f65",
            "669da10101de4500b7ae33d9805b6599",
            "17c1b63db475474c9e8633a0cb9d5e98",
            "f4e4dbbcf3a34237a64e71d279f5b8cb",
            "dab1544249cb44da93de512c53f5cd93",
            "3eca873460ee4421b67b09dcf9a8ddea",
            "c440e041b8344a2ea33454e45ad817c6",
            "decac242034e436b9ed3d6c15d3a13a4",
            "dd77aa9a0f5049459dce27e208f2a7ad",
            "d15400201ff14ad5a777b5258cbc3148",
            "e36e82bef1924778b599c1050a51fe9f",
            "67b0adcc4f394804a1c528be2135189d",
            "116d3614c5d84a579b44e14e83787122",
            "11e664dca26f473c96570a2df6f9b89e",
            "aff9bac4e87345ebb5cff28ad0b98286",
            "6ac4a31a5cb649babf33ec7a930cee92",
            "9ef803de2a0f4cc98ec4dbf18dd346b1",
            "819aa74e54bd473c9ea5522402ea23a2",
            "1d45b3a7d7a1492fa21d1453af86c609",
            "d905bd2344d540259e4caa988ffde820",
            "5b384e6f3b284e439075602b31c125be",
            "206183d1c4f2442991f61528c7721e61",
            "130b125f278044e3816bc92740cc092e",
            "398c510771ab4f108ce2a5c905477f5f",
            "6061cb5e1a2c4cbb927003c1bb19ab03",
            "79b039108f04404faecbc83acc666462",
            "7b57d4acc9994a16ab23f8dfba0bfcdd"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0302064c163f41bf889dc633937bcf9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76adb4520c664c92bed6b42f1c6e63bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3eca873460ee4421b67b09dcf9a8ddea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ef803de2a0f4cc98ec4dbf18dd346b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "Wow, what's this on the video rental store's shelf in front of me? Nothing other than a questionable \"sequel\" to 8MM. It wasn't a very good sequel to a movie that had a very definitive end and an abundance of emotional depth far greater than this movie.<br /><br />Basically, from the plot outline verbatim, an American diplomat, David Huxley, and his fiancée, Tish Harrington, venture into the sordid underworld of sex and pornography in Budapest, Hungary to find out who is blackmailing them with a porno video taken of them with a prostitute, Risa. The entire story is based around the various characters who make up these various sex clubs and strip joints throughout the city. The mystery is solved when, in the end, Tish finds out that the ransom money for the video and (essentially later on in the story) her fiancée which came out of her trust fund money is basically going back to her future husband as the story unfolds til the bitter end.<br /><br />I didn't like how this had nothing to do with the original 8MM at all. The only thing close to the original is the type of thriller that it was, the fact that David ends up with some kind of bondage contraption over him to keep him prisoner looks like the kinky world of the first film, and the fact that the entire movie has sex emblazoned throughout almost every key scene. Otherwise, its a totally different movie. It made for a lousy love story, even before the end is known, which makes the ending more of a possibility because I didn't believe the words coming out of David's mouth the whole time. But we were warned that he was a liar about most things that might get him in trouble.<br /><br />There were ridiculous nude scenes in most of the \"shocking\" moments of the film, which were trying to stir emotions in the audience to cheer for Tish to figure out the plot so she can leave this \"hellish\" sex debauchery. I counted at least 11 ridiculously filmed sequences when there was nothing but sex to be shown. Even the menu screen on the DVD is nothing but film with naked women on it to make the DVD seem totally provocative.<br /><br />David was no heroic person throughout the film. You could guess he was the main problem long before the end. The actors who played each role were all new to me, which might explain how they got so many of them to strip down to gain acting \"respect.\" There were plot holes (How did David and Richard finally impress Tish's father by getting the lease they wanted when David was so wrapped up in this damn investigation to try and find a prostitute?) There were cheesy technology moments (like the talking email program dressed up like a bondage queen), and a gay brother character which did nothing but show how the director was trying to get a Joaquin Phoenix knockoff to play this character. The tag line featured on this profile for this video is complete BS, too, because it wasn't even about a last breath. Nobody really dies. But they did have a good car crash sequence that came out of nowhere...but that was a good 10 seconds long out of an hour and 3/4 long movie.<br /><br />Go rent (and maybe buy) the original. It's one of Joel Schumacher's better and more original films. It has everything better about it from this film. I wouldn't recommend seeing this unless you want to compare apples to oranges.\n"
          ]
        }
      ],
      "source": [
        "# 토크나이저 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# 데이터 로드\n",
        "train_dataset, val_dataset = load_data(file_path, tokenizer)\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    ## 예시 확인하기 ##\n",
        "df = pd.read_csv(file_path)\n",
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "texts = df['review'].tolist()\n",
        "labels = df['sentiment'].tolist()\n",
        "dataset = SentimentDataset(texts, labels, tokenizer, 128)\n",
        "\"\"\"\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            token_type_ids=batch['token_type_ids'],\n",
        "            labels=batch['labels'],\n",
        "            sentiment_tokens=batch['sentiment_tokens']\n",
        "\"\"\"\n",
        "print(\"Sample text:\", texts[3])\n",
        "sample_item = dataset[19]\n",
        "print(\"Tokenized input IDs:\", sample_item['input_ids'].tolist())\n",
        "print(\"Tokenized tokens:\", tokenizer.convert_ids_to_tokens(sample_item['input_ids'].tolist()))\n",
        "print(\"Tokenized attention mask:\", sample_item['attention_mask'].tolist())\n",
        "print(\"Tokenized token type IDs:\", sample_item['token_type_ids'].tolist())\n",
        "print(\"Label:\", sample_item['labels'].item())\n",
        "print('sentiment_words:', sample_item['sentiment_words'])\n",
        "print('Tokenized sentiment words:',sample_item['tokenized_sentiment_words'])\n",
        "\n",
        "# 토큰화된 감정 단어들 출력\n",
        "tokenized_sentiment_words = sample_item['tokenized_sentiment_words']\n",
        "all_tokens = []\n",
        "filtered_all_tokens = []\n",
        "\n",
        "for token_ids in tokenized_sentiment_words:\n",
        "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "        filtered_tokens = [token for token in tokens if not token.startswith('##')]\n",
        "        filtered_all_tokens.append(filtered_tokens)\n",
        "        all_tokens.append(tokens)\n",
        "\n",
        "print('## removed Tokenized sentiment words :', filtered_all_tokens)\n",
        "print(len(sample_item['input_ids']))\n",
        "print(len(sample_item['attention_mask']))\n",
        "print(len(sample_item['token_type_ids']))\n",
        "print(len(sample_item[\"attention_mask\"]))\n",
        "print(len(sample_item[\"sentiment_tokens\"]))\n",
        "    ##===============##"
      ],
      "metadata": {
        "id": "xnF-HxEmERu7",
        "outputId": "a6c9001a-bd63-436b-9236-b7419f6a4f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample text: Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.\n",
            "Tokenized input IDs: [101, 2023, 3185, 2003, 2241, 2006, 1996, 2338, 1010, 1000, 1037, 2116, 11867, 7770, 7983, 2098, 2518, 1000, 2011, 7658, 10514, 25811, 1998, 10455, 3314, 1997, 2679, 4262, 2090, 4004, 2015, 1998, 12461, 1010, 1037, 8476, 2008, 3310, 2013, 7658, 1005, 1055, 3167, 6322, 2004, 2019, 23399, 3652, 2039, 1999, 2859, 1012, 2008, 4281, 1010, 1998, 1996, 3376, 4291, 4290, 10906, 1010, 3957, 2023, 2293, 2466, 1037, 4310, 1998, 2738, 15236, 7224, 2005, 2049, 2051, 1012, 2060, 2084, 2008, 1010, 1996, 2466, 2003, 1037, 12991, 27086, 7472, 2007, 1037, 13432, 2299, 2008, 2003, 3383, 2062, 4622, 2084, 1996, 3185, 2993, 1012, 1996, 3376, 7673, 3557, 3504, 1996, 2112, 1998, 3957, 1037, 6919, 1010, 7436, 4222, 2836, 2004, 1037, 3460, 1997, 3816, 8843, 2076, 1996, 13896, 1997, 15523, 102]\n",
            "Tokenized tokens: ['[CLS]', 'this', 'movie', 'is', 'based', 'on', 'the', 'book', ',', '\"', 'a', 'many', 'sp', '##len', '##dor', '##ed', 'thing', '\"', 'by', 'han', 'su', '##yin', 'and', 'tackles', 'issues', 'of', 'race', 'relations', 'between', 'asian', '##s', 'and', 'whites', ',', 'a', 'topic', 'that', 'comes', 'from', 'han', \"'\", 's', 'personal', 'experiences', 'as', 'an', 'eurasian', 'growing', 'up', 'in', 'china', '.', 'that', 'background', ',', 'and', 'the', 'beautiful', 'hong', 'kong', 'settings', ',', 'gives', 'this', 'love', 'story', 'a', 'unique', 'and', 'rather', 'daring', 'atmosphere', 'for', 'its', 'time', '.', 'other', 'than', 'that', ',', 'the', 'story', 'is', 'a', 'stereo', '##typical', 'romance', 'with', 'a', 'memorable', 'song', 'that', 'is', 'perhaps', 'more', 'remembered', 'than', 'the', 'movie', 'itself', '.', 'the', 'beautiful', 'jennifer', 'jones', 'looks', 'the', 'part', 'and', 'gives', 'a', 'wonderful', ',', 'oscar', 'nominated', 'performance', 'as', 'a', 'doctor', 'of', 'mixed', 'breed', 'during', 'the', 'advent', 'of', 'communism', '[SEP]']\n",
            "Tokenized attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Tokenized token type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Label: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'sentiment_words'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-635b32e5e1d1>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokenized token type IDs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment_words:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tokenized sentiment words:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_sentiment_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sentiment_words'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [모델 준비] #\n",
        "\n",
        "# 활성화 함수\n",
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "# 활성화 함수 매핑\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": torch.nn.functional.silu}\n",
        "\n",
        "# 모델 설정\n",
        "class Config:\n",
        "    vocab_size = 30522\n",
        "    hidden_size = 128\n",
        "    num_hidden_layers = 4\n",
        "    num_attention_heads = 4\n",
        "    intermediate_size = 512\n",
        "    hidden_act = \"gelu\"\n",
        "    hidden_dropout_prob = 0.1\n",
        "    attention_probs_dropout_prob = 0.1\n",
        "    max_position_embeddings = 512\n",
        "    type_vocab_size = 2\n",
        "    initializer_range = 0.02\n",
        "    layer_norm_eps = 1e-12\n",
        "    pad_token_id = 0\n",
        "    gradient_checkpointing = False\n",
        "    position_embedding_type = \"absolute\"\n",
        "    use_cache = True\n",
        "    is_decoder = False\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "    #def __init__(self, config: Config, sentiment_ratio_init: float = 1.0):\n",
        "        super().__init__()\n",
        "        # 단어 임베딩, 위치 임베딩, 토큰 타입 임베딩\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # 레이어 정규화와 드롭아웃\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False)\n",
        "        self.register_buffer(\"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False)\n",
        "\n",
        "        # 학습 가능한 sentiment_ratio 파라미터 추가 및 초기값 설정\n",
        "        #self.sentiment_ratio = nn.Parameter(torch.tensor(sentiment_ratio_init))\n",
        "\n",
        "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0, sentiment_tokens=None):\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, past_key_values_length: seq_length + past_key_values_length]\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        # 입력 임베딩 생성\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "\n",
        "        for i in range(input_ids.size(0)):  # 배치의 각 문장에 대해\n",
        "            sentiment_token = sentiment_tokens[i]\n",
        "            sentiment_token_filtered = sentiment_token[(sentiment_token != 0) & (sentiment_token != 101) & (sentiment_token != 102)]\n",
        "            for j in range(input_ids.size(1)):  # 각 문장의 각 토큰에 대해\n",
        "                if input_ids[i, j] in sentiment_token_filtered:\n",
        "                    # 파라미터로 사용시\n",
        "                    # embeddings[i, j] = self.sentiment_ratio * inputs_embeds[i, j] + token_type_embeddings[i, j]\n",
        "                    # print(\"sentiment token ratio : \",self.sentiment_ratio)\n",
        "                    # 정해줄 때\n",
        "                    embeddings[i, j] = 2 * inputs_embeds[i, j] + token_type_embeddings[i, j]\n",
        "\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# 셀프 어텐션 구현 클래스\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config, position_embedding_type=None):\n",
        "        super().__init__()\n",
        "        # hidden_size가 num_attention_heads의 배수가 아니면 오류 발생\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        # 어텐션 헤드의 수와 각 헤드의 크기, 전체 헤드 크기 설정\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(\n",
        "            config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        # Query, Key, Value 행렬 정의\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        # 드롭아웃 레이어 정의\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        # 위치 임베딩 유형 설정\n",
        "        self.position_embedding_type = position_embedding_type or getattr(\n",
        "            config, \"position_embedding_type\", \"absolute\"\n",
        "        )\n",
        "        # 상대적 위치 임베딩을 사용하는 경우, 위치 임베딩 레이어 정의\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(\n",
        "                2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        # 디코더인지 여부 설정\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # 텐서의 크기 변환\n",
        "        new_x_shape = x.size()[\n",
        "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        # 텐서의 차원 변경 [batch_size, num_heads, seq_len, head_size]\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        # Query 레이어 계산\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # 크로스 어텐션인지 여부 확인\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # 과거의 k, v 값을 재사용 (크로스 어텐션)\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            # 인코더의 키와 값을 사용하여 크로스 어텐션 수행\n",
        "            key_layer = self.transpose_for_scores(\n",
        "                self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(\n",
        "                self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            # 과거의 k, v 값을 현재의 k, v와 결합 (디코더의 셀프 어텐션)\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            # 현재의 히든 스테이트에서 키와 값을 계산 (셀프 어텐션)\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        # Query 레이어 변환\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # 캐시를 사용할지 여부 설정\n",
        "        use_cache = past_key_value is not None\n",
        "        if self.is_decoder:\n",
        "            # 디코더인 경우, 키와 값을 캐싱\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Query와 Key의 내적(dot product)을 통해 어텐션 스코어 계산\n",
        "        attention_scores = torch.matmul(\n",
        "            query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            # 상대적 위치 임베딩을 사용하는 경우\n",
        "            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
        "            if use_cache:\n",
        "                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n",
        "                    -1, 1\n",
        "                )\n",
        "            else:\n",
        "                position_ids_l = torch.arange(\n",
        "                    query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(\n",
        "                key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "\n",
        "            # 거리 임베딩 계산\n",
        "            positional_embedding = self.distance_embedding(\n",
        "                distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(\n",
        "                dtype=query_layer.dtype)  # fp16 호환성\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                # 상대적 위치 임베딩을 쿼리에 적용\n",
        "                relative_position_scores = torch.einsum(\n",
        "                    \"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                # 상대적 위치 임베딩을 쿼리와 키에 적용\n",
        "                relative_position_scores_query = torch.einsum(\n",
        "                    \"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\n",
        "                    \"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + \\\n",
        "                    relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        # 어텐션 스코어를 정규화\n",
        "        attention_scores = attention_scores / \\\n",
        "            math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # 어텐션 마스크 적용\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # 어텐션 스코어를 확률로 변환\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # 드롭아웃 적용\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # 헤드 마스크 적용\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        # 컨텍스트 레이어 계산\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        # 텐서의 크기 변환 및 재배치\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[\n",
        "            :-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        # 출력 생성\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (\n",
        "            context_layer,)\n",
        "\n",
        "        # 디코더인 경우, past_key_value를 출력에 포함\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# 셀프 어텐션 출력 처리 클래스\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # 드롭아웃, 레이어 정규화, 잔차 연결 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "# 어텐션 메커니즘 클래스\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        # 셀프 어텐션 및 출력 계산\n",
        "        self_outputs = self.self(\n",
        "            input_tensor,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], input_tensor)\n",
        "        outputs = (attention_output,) + self_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "# 중간 레이어 활성화 함수 클래스\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # 중간 레이어 활성화 함수 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "# 중간 레이어 출력 처리 클래스\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # 드롭아웃, 레이어 정규화, 잔차 연결 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.Layer\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # 드롭아웃, 레이어 정규화, 잔차 연결 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "# 하나의 BERT 레이어를 구현하는 클래스\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        # 어텐션과 출력 계산\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        layer_output = self.output(self.intermediate(attention_output), attention_output)\n",
        "        outputs = (layer_output,) + self_attention_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "# 여러 BERT 레이어를 포함하는 인코더 클래스\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        return (hidden_states, all_hidden_states, all_attentions)\n",
        "\n",
        "# 첫 번째 토큰의 출력을 풀링하는 클래스\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # 첫 번째 토큰의 텐서를 사용해 풀링 출력 생성\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "# 전체 BERT 모델을 구현하는 클래스\n",
        "class  userBertModel(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None, sentiment_tokens=None):\n",
        "        # 입력 텐서의 크기 확인\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"input_ids 혹은 inputs_embeds 둘 중 하나의 형식으로만 입력해야 합니다.\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"input_ids 또는 inputs_embeds의 형식이어야 합니다.\")\n",
        "\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(input_shape, device=device)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        # 임베딩 출력 계산\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            sentiment_tokens= sentiment_tokens,\n",
        "        )\n",
        "        # 인코더 출력 계산\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        return sequence_output, pooled_output"
      ],
      "metadata": {
        "id": "8RxEd2bIQyQ1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nAXhMfNkYbth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "from tqdm import tqdm\n",
        "# 위 구조를 취합하여 만든 이진 감정 분류를 위한 클래스\n",
        "class BertForSequenceClassification(nn.Module):\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.bert = userBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, sentiment_tokens=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            sentiment_tokens= sentiment_tokens,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# Pretrained BERT 모델 로드\n",
        "\n",
        "pretrained_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "pretrained_state_dict = pretrained_model.state_dict()\n",
        "\n",
        "# Custom 모델 초기화\n",
        "custom_config = BertConfig(\n",
        "    vocab_size=30522,\n",
        "    hidden_size=768,\n",
        "    num_hidden_layers=8,\n",
        "    num_attention_heads=12, # 수정된 부분\n",
        "    intermediate_size=3072, # 수정된 부분\n",
        "    hidden_act=\"gelu\",\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    max_position_embeddings=512,\n",
        "    type_vocab_size=2,\n",
        "    initializer_range=0.02,\n",
        "    layer_norm_eps=1e-12,\n",
        "    pad_token_id=0,\n",
        "    gradient_checkpointing=False,\n",
        "    position_embedding_type=\"absolute\",\n",
        "    use_cache=True\n",
        ")\n",
        "model = BertForSequenceClassification(custom_config, num_labels=2)\n",
        "\n",
        "# Pretrained 모델의 state_dict를 Custom 모델로 로드\n",
        "model.bert.load_state_dict(pretrained_state_dict, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# 옵티마이저 설정\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "# 학습 진행\n",
        "model.train()\n",
        "for epoch in tqdm(range(3)):\n",
        "    epoch_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            token_type_ids=batch['token_type_ids'],\n",
        "            labels=batch['labels'],\n",
        "            sentiment_tokens=batch['sentiment_tokens']\n",
        "        )\n",
        "        loss, logits = outputs\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # 예측값 계산\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct_predictions += (predicted == batch['labels']).sum().item()\n",
        "        total_predictions += batch['labels'].size(0)\n",
        "\n",
        "        if batch_idx % 10 == 0:  # 10번째 배치마다 현황 출력\n",
        "            batch_accuracy = correct_predictions / total_predictions\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}, Accuracy: {batch_accuracy}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Epoch: {epoch} finished with average loss: {avg_epoch_loss}, Accuracy: {epoch_accuracy}\")\n",
        "\n",
        "model.eval()\n",
        "epoch_loss = 0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(val_dataloader):\n",
        "        outputs = model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            token_type_ids=batch['token_type_ids'],\n",
        "            labels=batch['labels'],\n",
        "            sentiment_tokens=batch['sentiment_tokens']\n",
        "        )\n",
        "        loss, logits = outputs\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # 예측값 계산\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct_predictions += (predicted == batch['labels']).sum().item()\n",
        "        total_predictions += batch['labels'].size(0)\n",
        "\n",
        "        if batch_idx % 10 == 0:  # 10번째 배치마다 현황 출력\n",
        "            batch_accuracy = correct_predictions / total_predictions\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}, Accuracy: {batch_accuracy}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Epoch: {epoch} finished with average loss: {avg_epoch_loss}, Accuracy: {epoch_accuracy}\")\n",
        "\n",
        "# 모델 저장 경로 설정\n",
        "save_path = './my_finetuned_bert_model.pth'\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")"
      ],
      "metadata": {
        "id": "5aTojxNkPIxO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cfc8b943ab5346f78c23f7dcdacf9ebd",
            "86c714eae7a54e89b03c1f4ec392e5c1",
            "f57592b13cff4afbba4e54e5963469af",
            "f9a1136f3e854de6be9668f87a6e672a",
            "56980aadb7d24f66b4b7a7c910f15405",
            "f0b54ce82f164e979f02fd76cb6209d7",
            "3b962fc22a1342999defb170196e1044",
            "47aaefaebbf342ef9f4e2bc85c41684b",
            "92e520bebaa64e3e9e246c6213ddfa5e",
            "a2ab601273354a9f88da487fb181862a",
            "6410f0b6f6dc49d0b3ae25d55fdd4327"
          ]
        },
        "outputId": "bf46fac5-dd0e-497b-946c-49f817595384"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfc8b943ab5346f78c23f7dcdacf9ebd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Batch: 0, Loss: 0.7367975115776062, Accuracy: 0.375\n",
            "Epoch: 0, Batch: 10, Loss: 0.6932107210159302, Accuracy: 0.48863636363636365\n",
            "Epoch: 0, Batch: 20, Loss: 0.6595125794410706, Accuracy: 0.5089285714285714\n",
            "Epoch: 0, Batch: 30, Loss: 0.7328212857246399, Accuracy: 0.5120967741935484\n",
            "Epoch: 0, Batch: 40, Loss: 0.6903248429298401, Accuracy: 0.5152439024390244\n",
            "Epoch: 0, Batch: 50, Loss: 0.6620938181877136, Accuracy: 0.5330882352941176\n",
            "Epoch: 0, Batch: 60, Loss: 0.7016817927360535, Accuracy: 0.5491803278688525\n",
            "Epoch: 0, Batch: 70, Loss: 0.6376954317092896, Accuracy: 0.551056338028169\n",
            "Epoch: 0, Batch: 80, Loss: 0.777674674987793, Accuracy: 0.5617283950617284\n",
            "Epoch: 0, Batch: 90, Loss: 0.6732926368713379, Accuracy: 0.5693681318681318\n",
            "Epoch: 0, Batch: 100, Loss: 0.5564017295837402, Accuracy: 0.5798267326732673\n",
            "Epoch: 0, Batch: 110, Loss: 0.7119823098182678, Accuracy: 0.5822072072072072\n",
            "Epoch: 0, Batch: 120, Loss: 0.6333842873573303, Accuracy: 0.5883264462809917\n",
            "Epoch: 0, Batch: 130, Loss: 0.6545757055282593, Accuracy: 0.5949427480916031\n",
            "Epoch: 0, Batch: 140, Loss: 0.6381262540817261, Accuracy: 0.5997340425531915\n",
            "Epoch: 0, Batch: 150, Loss: 0.4858632981777191, Accuracy: 0.6117549668874173\n",
            "Epoch: 0, Batch: 160, Loss: 0.5527454614639282, Accuracy: 0.6203416149068323\n",
            "Epoch: 0, Batch: 170, Loss: 0.6587789058685303, Accuracy: 0.6293859649122807\n",
            "Epoch: 0, Batch: 180, Loss: 0.44246596097946167, Accuracy: 0.6353591160220995\n",
            "Epoch: 0, Batch: 190, Loss: 0.4529198706150055, Accuracy: 0.6410340314136126\n",
            "Epoch: 0, Batch: 200, Loss: 0.4733371138572693, Accuracy: 0.6492537313432836\n",
            "Epoch: 0, Batch: 210, Loss: 0.42553216218948364, Accuracy: 0.6558056872037915\n",
            "Epoch: 0, Batch: 220, Loss: 0.3531499207019806, Accuracy: 0.6628959276018099\n",
            "Epoch: 0, Batch: 230, Loss: 0.276647686958313, Accuracy: 0.6693722943722944\n",
            "Epoch: 0, Batch: 240, Loss: 0.5614747405052185, Accuracy: 0.674792531120332\n",
            "Epoch: 0, Batch: 250, Loss: 0.49101966619491577, Accuracy: 0.6805278884462151\n",
            "Epoch: 0, Batch: 260, Loss: 0.4690324366092682, Accuracy: 0.6839080459770115\n",
            "Epoch: 0, Batch: 270, Loss: 0.6178739070892334, Accuracy: 0.6872693726937269\n",
            "Epoch: 0, Batch: 280, Loss: 0.5841687321662903, Accuracy: 0.6923932384341637\n",
            "Epoch: 0, Batch: 290, Loss: 0.5599668622016907, Accuracy: 0.6952319587628866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 1/3 [07:10<14:20, 430.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 finished with average loss: 0.5684144148230552, Accuracy: 0.6972916666666666\n",
            "Epoch: 1, Batch: 0, Loss: 0.3797096014022827, Accuracy: 0.875\n",
            "Epoch: 1, Batch: 10, Loss: 0.321513831615448, Accuracy: 0.8522727272727273\n",
            "Epoch: 1, Batch: 20, Loss: 0.3777894675731659, Accuracy: 0.8005952380952381\n",
            "Epoch: 1, Batch: 30, Loss: 0.3373524844646454, Accuracy: 0.8024193548387096\n",
            "Epoch: 1, Batch: 40, Loss: 0.4150125980377197, Accuracy: 0.8155487804878049\n",
            "Epoch: 1, Batch: 50, Loss: 0.4075225293636322, Accuracy: 0.8223039215686274\n",
            "Epoch: 1, Batch: 60, Loss: 0.42076489329338074, Accuracy: 0.819672131147541\n",
            "Epoch: 1, Batch: 70, Loss: 0.6770987510681152, Accuracy: 0.8133802816901409\n",
            "Epoch: 1, Batch: 80, Loss: 0.47743210196495056, Accuracy: 0.8101851851851852\n",
            "Epoch: 1, Batch: 90, Loss: 0.6658422946929932, Accuracy: 0.8090659340659341\n",
            "Epoch: 1, Batch: 100, Loss: 0.2458597719669342, Accuracy: 0.8087871287128713\n",
            "Epoch: 1, Batch: 110, Loss: 0.4581979513168335, Accuracy: 0.8108108108108109\n",
            "Epoch: 1, Batch: 120, Loss: 0.6293175220489502, Accuracy: 0.8125\n",
            "Epoch: 1, Batch: 130, Loss: 0.3980604112148285, Accuracy: 0.8148854961832062\n",
            "Epoch: 1, Batch: 140, Loss: 0.4717215299606323, Accuracy: 0.8151595744680851\n",
            "Epoch: 1, Batch: 150, Loss: 0.5235276818275452, Accuracy: 0.8174668874172185\n",
            "Epoch: 1, Batch: 160, Loss: 0.3085724115371704, Accuracy: 0.8198757763975155\n",
            "Epoch: 1, Batch: 170, Loss: 0.5811845660209656, Accuracy: 0.8216374269005848\n",
            "Epoch: 1, Batch: 180, Loss: 0.22598284482955933, Accuracy: 0.8242403314917127\n",
            "Epoch: 1, Batch: 190, Loss: 0.3249839246273041, Accuracy: 0.824607329842932\n",
            "Epoch: 1, Batch: 200, Loss: 0.2576484978199005, Accuracy: 0.826181592039801\n",
            "Epoch: 1, Batch: 210, Loss: 0.33096230030059814, Accuracy: 0.8287914691943128\n",
            "Epoch: 1, Batch: 220, Loss: 0.14852586388587952, Accuracy: 0.829185520361991\n",
            "Epoch: 1, Batch: 230, Loss: 0.18798910081386566, Accuracy: 0.8298160173160173\n",
            "Epoch: 1, Batch: 240, Loss: 0.40292972326278687, Accuracy: 0.8298755186721992\n",
            "Epoch: 1, Batch: 250, Loss: 0.4416704475879669, Accuracy: 0.8316733067729084\n",
            "Epoch: 1, Batch: 260, Loss: 0.39748063683509827, Accuracy: 0.8314176245210728\n",
            "Epoch: 1, Batch: 270, Loss: 0.4934917688369751, Accuracy: 0.8332564575645757\n",
            "Epoch: 1, Batch: 280, Loss: 0.5169877409934998, Accuracy: 0.8354092526690391\n",
            "Epoch: 1, Batch: 290, Loss: 0.46943745017051697, Accuracy: 0.8348367697594502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [14:22<07:11, 431.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 finished with average loss: 0.3862891927361488, Accuracy: 0.8341666666666666\n",
            "Epoch: 2, Batch: 0, Loss: 0.2173270732164383, Accuracy: 0.9375\n",
            "Epoch: 2, Batch: 10, Loss: 0.21041731536388397, Accuracy: 0.8863636363636364\n",
            "Epoch: 2, Batch: 20, Loss: 0.1944224238395691, Accuracy: 0.8720238095238095\n",
            "Epoch: 2, Batch: 30, Loss: 0.3589649498462677, Accuracy: 0.8629032258064516\n",
            "Epoch: 2, Batch: 40, Loss: 0.2096933126449585, Accuracy: 0.8658536585365854\n",
            "Epoch: 2, Batch: 50, Loss: 0.2507512867450714, Accuracy: 0.8688725490196079\n",
            "Epoch: 2, Batch: 60, Loss: 0.2954183518886566, Accuracy: 0.8688524590163934\n",
            "Epoch: 2, Batch: 70, Loss: 0.6129778623580933, Accuracy: 0.8626760563380281\n",
            "Epoch: 2, Batch: 80, Loss: 0.4631611704826355, Accuracy: 0.8634259259259259\n",
            "Epoch: 2, Batch: 90, Loss: 0.6231528520584106, Accuracy: 0.8653846153846154\n",
            "Epoch: 2, Batch: 100, Loss: 0.1384209543466568, Accuracy: 0.8620049504950495\n",
            "Epoch: 2, Batch: 110, Loss: 0.3889522850513458, Accuracy: 0.8603603603603603\n",
            "Epoch: 2, Batch: 120, Loss: 0.6270550489425659, Accuracy: 0.8610537190082644\n",
            "Epoch: 2, Batch: 130, Loss: 0.2703450620174408, Accuracy: 0.8625954198473282\n",
            "Epoch: 2, Batch: 140, Loss: 0.34780818223953247, Accuracy: 0.863031914893617\n",
            "Epoch: 2, Batch: 150, Loss: 0.3707413673400879, Accuracy: 0.8646523178807947\n",
            "Epoch: 2, Batch: 160, Loss: 0.2683970034122467, Accuracy: 0.8672360248447205\n",
            "Epoch: 2, Batch: 170, Loss: 0.6523663401603699, Accuracy: 0.866593567251462\n",
            "Epoch: 2, Batch: 180, Loss: 0.2958042323589325, Accuracy: 0.8691298342541437\n",
            "Epoch: 2, Batch: 190, Loss: 0.3555930554866791, Accuracy: 0.8694371727748691\n",
            "Epoch: 2, Batch: 200, Loss: 0.13177545368671417, Accuracy: 0.8700248756218906\n",
            "Epoch: 2, Batch: 210, Loss: 0.22123682498931885, Accuracy: 0.8729265402843602\n",
            "Epoch: 2, Batch: 220, Loss: 0.1155809536576271, Accuracy: 0.872737556561086\n",
            "Epoch: 2, Batch: 230, Loss: 0.1547481119632721, Accuracy: 0.872564935064935\n",
            "Epoch: 2, Batch: 240, Loss: 0.3612156808376312, Accuracy: 0.8734439834024896\n",
            "Epoch: 2, Batch: 250, Loss: 0.3651118576526642, Accuracy: 0.8742529880478087\n",
            "Epoch: 2, Batch: 260, Loss: 0.17130978405475616, Accuracy: 0.8752394636015326\n",
            "Epoch: 2, Batch: 270, Loss: 0.45780885219573975, Accuracy: 0.8761531365313653\n",
            "Epoch: 2, Batch: 280, Loss: 0.40778905153274536, Accuracy: 0.8770017793594306\n",
            "Epoch: 2, Batch: 290, Loss: 0.34256812930107117, Accuracy: 0.876073883161512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [21:30<00:00, 430.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 finished with average loss: 0.30733124854664007, Accuracy: 0.8758333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Batch: 0, Loss: 0.32300978899002075, Accuracy: 0.875\n",
            "Epoch: 2, Batch: 10, Loss: 0.675106942653656, Accuracy: 0.8636363636363636\n",
            "Epoch: 2, Batch: 20, Loss: 0.4350636303424835, Accuracy: 0.8601190476190477\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-74dbaff7fbf6>\u001b[0m in \u001b[0;36m<cell line: 101>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-74dbaff7fbf6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, labels, sentiment_tokens)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-12bbd48bd483>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, sentiment_tokens)\u001b[0m\n\u001b[1;32m    443\u001b[0m         )\n\u001b[1;32m    444\u001b[0m         \u001b[0;31m# 인코더 출력 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-12bbd48bd483>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    375\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-12bbd48bd483>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    352\u001b[0m         )\n\u001b[1;32m    353\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-12bbd48bd483>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# 중간 레이어 활성화 함수 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZsualEUMGMgq"
      }
    }
  ]
}