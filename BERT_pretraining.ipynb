{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwonjihan/ML-teamproject/blob/develop/BERT_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import RandomSampler, DataLoader\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, DataCollatorForLanguageModeling"
      ],
      "metadata": {
        "id": "AgffaJtcofIV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IFjkmcjWzINX",
        "outputId": "4cc7c9b8-733f-41ad-9511-9b208a7b463a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activation function 불러오기\n",
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": torch.nn.functional.silu}"
      ],
      "metadata": {
        "id": "drgT6nqeoqN5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c6RRMvSIoXjR"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    vocab_size = 30522  # Vocabulary size of the BERT model.\n",
        "    hidden_size = 768  # Size of the hidden layers.\n",
        "    num_hidden_layers = 12  # Number of hidden layers in the transformer encoder.\n",
        "    num_attention_heads = 12  # Number of attention heads.\n",
        "    intermediate_size = 3072  # Size of the \"intermediate\" (i.e., feed-forward) layer in the transformer.\n",
        "    hidden_act = \"gelu\"  # Activation function to use.\n",
        "    hidden_dropout_prob = 0.1  # Dropout probability for the hidden layers.\n",
        "    attention_probs_dropout_prob = 0.1  # Dropout probability for the attention probabilities.\n",
        "    max_position_embeddings = 512  # Maximum number of position embeddings.\n",
        "    type_vocab_size = 2  # The vocabulary size of the token type ids.\n",
        "    initializer_range = 0.02  # The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
        "    layer_norm_eps = 1e-12  # The epsilon used by the layer normalization layers.\n",
        "    pad_token_id = 0  # The ID of the padding token.\n",
        "    gradient_checkpointing = False  # Whether to use gradient checkpointing to save memory.\n",
        "    position_embedding_type = \"absolute\"  # The type of position embeddings.\n",
        "    use_cache = True  # Whether to use caching during generation.\n",
        "\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(\n",
        "            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(\n",
        "            config.type_vocab_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(\n",
        "            config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.position_embedding_type = getattr(\n",
        "            config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(\n",
        "            config.max_position_embeddings).expand((1, -1)), persistent=False)\n",
        "        self.register_buffer(\"token_type_ids\", torch.zeros(\n",
        "            self.position_ids.size(), dtype=torch.long), persistent=False)\n",
        "\n",
        "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0):\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:,\n",
        "                                             past_key_values_length: seq_length + past_key_values_length]\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n",
        "                    input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(\n",
        "                    input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention heads ({config.num_attention_heads})\")\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(\n",
        "            config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[\n",
        "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        if encoder_hidden_states is not None:\n",
        "            key_layer = self.transpose_for_scores(\n",
        "                self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(\n",
        "                self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(\n",
        "            query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / \\\n",
        "            math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[\n",
        "            :-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (\n",
        "            context_layer,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(\n",
        "            config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        self_outputs = self.self(\n",
        "            input_tensor,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], input_tensor)\n",
        "        # add attentions if we output them\n",
        "        outputs = (attention_output,) + self_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(\n",
        "            config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        layer_output = self.output(self.intermediate(\n",
        "            attention_output), attention_output)\n",
        "\n",
        "        outputs = (layer_output,) + self_attention_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([BertLayer(config)\n",
        "                                   for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        return (hidden_states, all_hidden_states, all_attentions)\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "        self.LayerNorm = nn.LayerNorm(\n",
        "            config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "        self.decoder = nn.Linear(\n",
        "            config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class BertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n",
        "class BertModel(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\n",
        "                \"input_ids 혹은 inputs_embeds 둘 중 하나의 형식으로만 입력해야 합니다.\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"input_ids 또는 inputs_embeds의 형식이어야 합니다.\")\n",
        "\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(input_shape, device=device)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(\n",
        "                input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        return sequence_output, pooled_output\n",
        "\n",
        "\n",
        "class BertForPreTraining(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertPreTrainingHeads(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "\n",
        "        total_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, config.vocab_size), labels.view(-1))\n",
        "            total_loss = masked_lm_loss\n",
        "\n",
        "        return prediction_scores, seq_relationship_score, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0UdnkBz6WGS7",
        "outputId": "c7be8125-2abb-420e-b8ed-12a90c444afb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Tokenization classes.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import unicodedata\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
        "  \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n",
        "\n",
        "  # The casing has to be passed in by the user and there is no explicit check\n",
        "  # as to whether it matches the checkpoint. The casing information probably\n",
        "  # should have been stored in the bert_config.json file, but it's not, so\n",
        "  # we have to heuristically detect it to validate.\n",
        "\n",
        "  if not init_checkpoint:\n",
        "    return\n",
        "\n",
        "  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
        "  if m is None:\n",
        "    return\n",
        "\n",
        "  model_name = m.group(1)\n",
        "\n",
        "  lower_models = [\n",
        "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
        "      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  cased_models = [\n",
        "      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
        "      \"multi_cased_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  is_bad_config = False\n",
        "  if model_name in lower_models and not do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"False\"\n",
        "    case_name = \"lowercased\"\n",
        "    opposite_flag = \"True\"\n",
        "\n",
        "  if model_name in cased_models and do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"True\"\n",
        "    case_name = \"cased\"\n",
        "    opposite_flag = \"False\"\n",
        "\n",
        "  if is_bad_config:\n",
        "    raise ValueError(\n",
        "        \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
        "        \"However, `%s` seems to be a %s model, so you \"\n",
        "        \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
        "        \"how the model was pre-training. If this error is wrong, please \"\n",
        "        \"just comment out this check.\" % (actual_flag, init_checkpoint,\n",
        "                                          model_name, case_name, opposite_flag))\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    elif isinstance(text, unicode):\n",
        "      return text\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "  # These functions want `str` for both Python2 and Python3, but in one case\n",
        "  # it's a Unicode string and in the other it's a byte string.\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, unicode):\n",
        "      return text.encode(\"utf-8\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "  vocab = collections.OrderedDict()\n",
        "  index = 0\n",
        "  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
        "    while True:\n",
        "      token = convert_to_unicode(reader.readline())\n",
        "      if not token:\n",
        "        break\n",
        "      token = token.strip()\n",
        "      vocab[token] = index\n",
        "      index += 1\n",
        "  return vocab\n",
        "\n",
        "\n",
        "def convert_by_vocab(vocab, items):\n",
        "  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
        "  output = []\n",
        "  for item in items:\n",
        "    output.append(vocab[item])\n",
        "  return output\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "  return convert_by_vocab(vocab, tokens)\n",
        "\n",
        "\n",
        "def convert_ids_to_tokens(inv_vocab, ids):\n",
        "  return convert_by_vocab(inv_vocab, ids)\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "  text = text.strip()\n",
        "  if not text:\n",
        "    return []\n",
        "  tokens = text.split()\n",
        "  return tokens\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, do_lower_case=True):\n",
        "    self.vocab = load_vocab(vocab_file)\n",
        "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    split_tokens = []\n",
        "    for token in self.basic_tokenizer.tokenize(text):\n",
        "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "        split_tokens.append(sub_token)\n",
        "\n",
        "    return split_tokens\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    return convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    return convert_by_vocab(self.inv_vocab, ids)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "  def __init__(self, do_lower_case=True):\n",
        "    \"\"\"Constructs a BasicTokenizer.\n",
        "\n",
        "    Args:\n",
        "      do_lower_case: Whether to lower case the input.\n",
        "    \"\"\"\n",
        "    self.do_lower_case = do_lower_case\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "    text = convert_to_unicode(text)\n",
        "    text = self._clean_text(text)\n",
        "\n",
        "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "    # models. This is also applied to the English models now, but it doesn't\n",
        "    # matter since the English models were not trained on any Chinese data\n",
        "    # and generally don't have any Chinese data in them (there are Chinese\n",
        "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "    # words in the English Wikipedia.).\n",
        "    text = self._tokenize_chinese_chars(text)\n",
        "\n",
        "    orig_tokens = whitespace_tokenize(text)\n",
        "    split_tokens = []\n",
        "    for token in orig_tokens:\n",
        "      if self.do_lower_case:\n",
        "        token = token.lower()\n",
        "        token = self._run_strip_accents(token)\n",
        "      split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "    return output_tokens\n",
        "\n",
        "  def _run_strip_accents(self, text):\n",
        "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "    text = unicodedata.normalize(\"NFD\", text)\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cat = unicodedata.category(char)\n",
        "      if cat == \"Mn\":\n",
        "        continue\n",
        "      output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _run_split_on_punc(self, text):\n",
        "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "    chars = list(text)\n",
        "    i = 0\n",
        "    start_new_word = True\n",
        "    output = []\n",
        "    while i < len(chars):\n",
        "      char = chars[i]\n",
        "      if _is_punctuation(char):\n",
        "        output.append([char])\n",
        "        start_new_word = True\n",
        "      else:\n",
        "        if start_new_word:\n",
        "          output.append([])\n",
        "        start_new_word = False\n",
        "        output[-1].append(char)\n",
        "      i += 1\n",
        "\n",
        "    return [\"\".join(x) for x in output]\n",
        "\n",
        "  def _tokenize_chinese_chars(self, text):\n",
        "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if self._is_chinese_char(cp):\n",
        "        output.append(\" \")\n",
        "        output.append(char)\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _is_chinese_char(self, cp):\n",
        "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "    #\n",
        "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "    # space-separated words, so they are not treated specially and handled\n",
        "    # like the all of the other languages.\n",
        "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def _clean_text(self, text):\n",
        "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "        continue\n",
        "      if _is_whitespace(char):\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "  \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
        "    self.vocab = vocab\n",
        "    self.unk_token = unk_token\n",
        "    self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "\n",
        "    This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "    using the given vocabulary.\n",
        "\n",
        "    For example:\n",
        "      input = \"unaffable\"\n",
        "      output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "    Args:\n",
        "      text: A single token or whitespace separated tokens. This should have\n",
        "        already been passed through `BasicTokenizer.\n",
        "\n",
        "    Returns:\n",
        "      A list of wordpiece tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    text = convert_to_unicode(text)\n",
        "\n",
        "    output_tokens = []\n",
        "    for token in whitespace_tokenize(text):\n",
        "      chars = list(token)\n",
        "      if len(chars) > self.max_input_chars_per_word:\n",
        "        output_tokens.append(self.unk_token)\n",
        "        continue\n",
        "\n",
        "      is_bad = False\n",
        "      start = 0\n",
        "      sub_tokens = []\n",
        "      while start < len(chars):\n",
        "        end = len(chars)\n",
        "        cur_substr = None\n",
        "        while start < end:\n",
        "          substr = \"\".join(chars[start:end])\n",
        "          if start > 0:\n",
        "            substr = \"##\" + substr\n",
        "          if substr in self.vocab:\n",
        "            cur_substr = substr\n",
        "            break\n",
        "          end -= 1\n",
        "        if cur_substr is None:\n",
        "          is_bad = True\n",
        "          break\n",
        "        sub_tokens.append(cur_substr)\n",
        "        start = end\n",
        "\n",
        "      if is_bad:\n",
        "        output_tokens.append(self.unk_token)\n",
        "      else:\n",
        "        output_tokens.extend(sub_tokens)\n",
        "    return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "  # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "  # as whitespace since they are generally considered as such.\n",
        "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat == \"Zs\":\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "  \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "  # These are technically control characters but we count them as whitespace\n",
        "  # characters.\n",
        "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return False\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat in (\"Cc\", \"Cf\"):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "  cp = ord(char)\n",
        "  # We treat all non-letter/number ASCII as punctuation.\n",
        "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "  # Punctuation class but we treat them as punctuation anyways, for\n",
        "  # consistency.\n",
        "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat.startswith(\"P\"):\n",
        "    return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "Tg9dkvoOaTif"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright (c) 2019 NVIDIA CORPORATION. All rights reserved.\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Create masked LM/next sentence masked_lm TF examples for BERT.\"\"\"\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "from io import open\n",
        "import h5py\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import random\n",
        "import collections\n",
        "\n",
        "from google.colab import drive\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "class TrainingInstance(object):\n",
        "  \"\"\"A single training instance (sentence pair).\"\"\"\n",
        "\n",
        "  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n",
        "               is_random_next):\n",
        "    self.tokens = tokens\n",
        "    self.segment_ids = segment_ids\n",
        "    self.is_random_next = is_random_next\n",
        "    self.masked_lm_positions = masked_lm_positions\n",
        "    self.masked_lm_labels = masked_lm_labels\n",
        "\n",
        "  def __str__(self):\n",
        "    s = \"\"\n",
        "    s += \"tokens: %s\\n\" % (\" \".join(\n",
        "        [printable_text(x) for x in self.tokens]))\n",
        "    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
        "    s += \"is_random_next: %s\\n\" % self.is_random_next\n",
        "    s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
        "        [str(x) for x in self.masked_lm_positions]))\n",
        "    s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
        "        [printable_text(x) for x in self.masked_lm_labels]))\n",
        "    s += \"\\n\"\n",
        "    return s\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.__str__()\n",
        "\n",
        "\n",
        "def write_instance_to_example_file(instances, tokenizer, max_seq_length,\n",
        "                                    max_predictions_per_seq, output_file):\n",
        "  \"\"\"Create TF example files from `TrainingInstance`s.\"\"\"\n",
        "\n",
        "\n",
        "  total_written = 0\n",
        "  features = collections.OrderedDict()\n",
        "\n",
        "  num_instances = len(instances)\n",
        "  features[\"input_ids\"] = np.zeros([num_instances, max_seq_length], dtype=\"int32\")\n",
        "  features[\"input_mask\"] = np.zeros([num_instances, max_seq_length], dtype=\"int32\")\n",
        "  features[\"segment_ids\"] = np.zeros([num_instances, max_seq_length], dtype=\"int32\")\n",
        "  features[\"masked_lm_positions\"] =  np.zeros([num_instances, max_predictions_per_seq], dtype=\"int32\")\n",
        "  features[\"masked_lm_ids\"] = np.zeros([num_instances, max_predictions_per_seq], dtype=\"int32\")\n",
        "  features[\"next_sentence_labels\"] = np.zeros(num_instances, dtype=\"int32\")\n",
        "\n",
        "\n",
        "  for inst_index, instance in enumerate(tqdm(instances)):\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "    segment_ids = list(instance.segment_ids)\n",
        "    assert len(input_ids) <= max_seq_length\n",
        "\n",
        "    while len(input_ids) < max_seq_length:\n",
        "      input_ids.append(0)\n",
        "      input_mask.append(0)\n",
        "      segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    masked_lm_positions = list(instance.masked_lm_positions)\n",
        "    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
        "    masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
        "\n",
        "    while len(masked_lm_positions) < max_predictions_per_seq:\n",
        "      masked_lm_positions.append(0)\n",
        "      masked_lm_ids.append(0)\n",
        "      masked_lm_weights.append(0.0)\n",
        "\n",
        "    next_sentence_label = 1 if instance.is_random_next else 0\n",
        "\n",
        "\n",
        "\n",
        "    features[\"input_ids\"][inst_index] = input_ids\n",
        "    features[\"input_mask\"][inst_index] = input_mask\n",
        "    features[\"segment_ids\"][inst_index] = segment_ids\n",
        "    features[\"masked_lm_positions\"][inst_index] = masked_lm_positions\n",
        "    features[\"masked_lm_ids\"][inst_index] = masked_lm_ids\n",
        "    features[\"next_sentence_labels\"][inst_index] = next_sentence_label\n",
        "\n",
        "    total_written += 1\n",
        "\n",
        "    # if inst_index < 20:\n",
        "    #   tf.logging.info(\"*** Example ***\")\n",
        "    #   tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "    #       [tokenization.printable_text(x) for x in instance.tokens]))\n",
        "\n",
        "    #   for feature_name in features.keys():\n",
        "    #     feature = features[feature_name]\n",
        "    #     values = []\n",
        "    #     if feature.int64_list.value:\n",
        "    #       values = feature.int64_list.value\n",
        "    #     elif feature.float_list.value:\n",
        "    #       values = feature.float_list.value\n",
        "    #     tf.logging.info(\n",
        "    #         \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
        "\n",
        "\n",
        "  print(\"saving data\")\n",
        "  f= h5py.File(output_file, 'w')\n",
        "  f.create_dataset(\"input_ids\", data=features[\"input_ids\"], dtype='i4', compression='gzip')\n",
        "  f.create_dataset(\"input_mask\", data=features[\"input_mask\"], dtype='i1', compression='gzip')\n",
        "  f.create_dataset(\"segment_ids\", data=features[\"segment_ids\"], dtype='i1', compression='gzip')\n",
        "  f.create_dataset(\"masked_lm_positions\", data=features[\"masked_lm_positions\"], dtype='i4', compression='gzip')\n",
        "  f.create_dataset(\"masked_lm_ids\", data=features[\"masked_lm_ids\"], dtype='i4', compression='gzip')\n",
        "  f.create_dataset(\"next_sentence_labels\", data=features[\"next_sentence_labels\"], dtype='i1', compression='gzip')\n",
        "  f.flush()\n",
        "  f.close()\n",
        "\n",
        "def create_training_instances(input_files, tokenizer, max_seq_length,\n",
        "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
        "                              max_predictions_per_seq, rng):\n",
        "  \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
        "  all_documents = [[]]\n",
        "\n",
        "  # Input file format:\n",
        "  # (1) One sentence per line. These should ideally be actual sentences, not\n",
        "  # entire paragraphs or arbitrary spans of text. (Because we use the\n",
        "  # sentence boundaries for the \"next sentence prediction\" task).\n",
        "  # (2) Blank lines between documents. Document boundaries are needed so\n",
        "  # that the \"next sentence prediction\" task doesn't span between documents.\n",
        "  for input_file in input_files:\n",
        "    print(\"creating instance from {}\".format(input_file))\n",
        "    with open(input_file, \"r\") as reader:\n",
        "      while True:\n",
        "        line = convert_to_unicode(reader.readline())\n",
        "        if not line:\n",
        "          break\n",
        "        line = line.strip()\n",
        "\n",
        "        # Empty lines are used as document delimiters\n",
        "        if not line:\n",
        "          all_documents.append([])\n",
        "        tokens = tokenizer.tokenize(line)\n",
        "        if tokens:\n",
        "          all_documents[-1].append(tokens)\n",
        "\n",
        "  # Remove empty documents\n",
        "  all_documents = [x for x in all_documents if x]\n",
        "  rng.shuffle(all_documents)\n",
        "\n",
        "  vocab_words = list(tokenizer.vocab.keys())\n",
        "  instances = []\n",
        "  for _ in range(dupe_factor):\n",
        "    for document_index in range(len(all_documents)):\n",
        "      instances.extend(\n",
        "          create_instances_from_document(\n",
        "              all_documents, document_index, max_seq_length, short_seq_prob,\n",
        "              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
        "\n",
        "  rng.shuffle(instances)\n",
        "  return instances\n",
        "\n",
        "\n",
        "def create_instances_from_document(\n",
        "    all_documents, document_index, max_seq_length, short_seq_prob,\n",
        "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
        "  \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
        "  document = all_documents[document_index]\n",
        "\n",
        "  # Account for [CLS], [SEP], [SEP]\n",
        "  max_num_tokens = max_seq_length - 3\n",
        "\n",
        "  # We *usually* want to fill up the entire sequence since we are padding\n",
        "  # to `max_seq_length` anyways, so short sequences are generally wasted\n",
        "  # computation. However, we *sometimes*\n",
        "  # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
        "  # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
        "  # The `target_seq_length` is just a rough target however, whereas\n",
        "  # `max_seq_length` is a hard limit.\n",
        "  target_seq_length = max_num_tokens\n",
        "  if rng.random() < short_seq_prob:\n",
        "    target_seq_length = rng.randint(2, max_num_tokens)\n",
        "\n",
        "  # We DON'T just concatenate all of the tokens from a document into a long\n",
        "  # sequence and choose an arbitrary split point because this would make the\n",
        "  # next sentence prediction task too easy. Instead, we split the input into\n",
        "  # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
        "  # input.\n",
        "  instances = []\n",
        "  current_chunk = []\n",
        "  current_length = 0\n",
        "  i = 0\n",
        "  while i < len(document):\n",
        "    segment = document[i]\n",
        "    current_chunk.append(segment)\n",
        "    current_length += len(segment)\n",
        "    if i == len(document) - 1 or current_length >= target_seq_length:\n",
        "      if current_chunk:\n",
        "        # `a_end` is how many segments from `current_chunk` go into the `A`\n",
        "        # (first) sentence.\n",
        "        a_end = 1\n",
        "        if len(current_chunk) >= 2:\n",
        "          a_end = rng.randint(1, len(current_chunk) - 1)\n",
        "\n",
        "        tokens_a = []\n",
        "        for j in range(a_end):\n",
        "          tokens_a.extend(current_chunk[j])\n",
        "\n",
        "        tokens_b = []\n",
        "        # Random next\n",
        "        is_random_next = False\n",
        "        if len(current_chunk) == 1 or rng.random() < 0.5:\n",
        "          is_random_next = True\n",
        "          target_b_length = target_seq_length - len(tokens_a)\n",
        "\n",
        "          # This should rarely go for more than one iteration for large\n",
        "          # corpora. However, just to be careful, we try to make sure that\n",
        "          # the random document is not the same as the document\n",
        "          # we're processing.\n",
        "          for _ in range(10):\n",
        "            random_document_index = rng.randint(0, len(all_documents) - 1)\n",
        "            if random_document_index != document_index:\n",
        "              break\n",
        "\n",
        "          #If picked random document is the same as the current document\n",
        "          if random_document_index == document_index:\n",
        "            is_random_next = False\n",
        "\n",
        "          random_document = all_documents[random_document_index]\n",
        "          random_start = rng.randint(0, len(random_document) - 1)\n",
        "          for j in range(random_start, len(random_document)):\n",
        "            tokens_b.extend(random_document[j])\n",
        "            if len(tokens_b) >= target_b_length:\n",
        "              break\n",
        "          # We didn't actually use these segments so we \"put them back\" so\n",
        "          # they don't go to waste.\n",
        "          num_unused_segments = len(current_chunk) - a_end\n",
        "          i -= num_unused_segments\n",
        "        # Actual next\n",
        "        else:\n",
        "          is_random_next = False\n",
        "          for j in range(a_end, len(current_chunk)):\n",
        "            tokens_b.extend(current_chunk[j])\n",
        "        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
        "\n",
        "        assert len(tokens_a) >= 1\n",
        "        assert len(tokens_b) >= 1\n",
        "\n",
        "        tokens = []\n",
        "        segment_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        segment_ids.append(0)\n",
        "        for token in tokens_a:\n",
        "          tokens.append(token)\n",
        "          segment_ids.append(0)\n",
        "\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(0)\n",
        "\n",
        "        for token in tokens_b:\n",
        "          tokens.append(token)\n",
        "          segment_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(1)\n",
        "\n",
        "        (tokens, masked_lm_positions,\n",
        "         masked_lm_labels) = create_masked_lm_predictions(\n",
        "             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
        "        instance = TrainingInstance(\n",
        "            tokens=tokens,\n",
        "            segment_ids=segment_ids,\n",
        "            is_random_next=is_random_next,\n",
        "            masked_lm_positions=masked_lm_positions,\n",
        "            masked_lm_labels=masked_lm_labels)\n",
        "        instances.append(instance)\n",
        "      current_chunk = []\n",
        "      current_length = 0\n",
        "    i += 1\n",
        "\n",
        "  return instances\n",
        "\n",
        "\n",
        "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
        "                                          [\"index\", \"label\"])\n",
        "\n",
        "\n",
        "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
        "                                 max_predictions_per_seq, vocab_words, rng):\n",
        "  \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
        "\n",
        "  cand_indexes = []\n",
        "  for (i, token) in enumerate(tokens):\n",
        "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
        "      continue\n",
        "    cand_indexes.append(i)\n",
        "\n",
        "  rng.shuffle(cand_indexes)\n",
        "\n",
        "  output_tokens = list(tokens)\n",
        "\n",
        "  num_to_predict = min(max_predictions_per_seq,\n",
        "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
        "\n",
        "  masked_lms = []\n",
        "  covered_indexes = set()\n",
        "  for index in cand_indexes:\n",
        "    if len(masked_lms) >= num_to_predict:\n",
        "      break\n",
        "    if index in covered_indexes:\n",
        "      continue\n",
        "    covered_indexes.add(index)\n",
        "\n",
        "    masked_token = None\n",
        "    # 80% of the time, replace with [MASK]\n",
        "    if rng.random() < 0.8:\n",
        "      masked_token = \"[MASK]\"\n",
        "    else:\n",
        "      # 10% of the time, keep original\n",
        "      if rng.random() < 0.5:\n",
        "        masked_token = tokens[index]\n",
        "      # 10% of the time, replace with random word\n",
        "      else:\n",
        "        masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
        "\n",
        "    output_tokens[index] = masked_token\n",
        "\n",
        "    masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
        "\n",
        "  masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
        "\n",
        "  masked_lm_positions = []\n",
        "  masked_lm_labels = []\n",
        "  for p in masked_lms:\n",
        "    masked_lm_positions.append(p.index)\n",
        "    masked_lm_labels.append(p.label)\n",
        "\n",
        "  return (output_tokens, masked_lm_positions, masked_lm_labels)\n",
        "\n",
        "\n",
        "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
        "  \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_num_tokens:\n",
        "      break\n",
        "\n",
        "    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
        "    assert len(trunc_tokens) >= 1\n",
        "\n",
        "    # We want to sometimes truncate from the front and sometimes from the\n",
        "    # back to add more randomness and avoid biases.\n",
        "    if rng.random() < 0.5:\n",
        "      del trunc_tokens[0]\n",
        "    else:\n",
        "      trunc_tokens.pop()\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/6000_IMDB_Dataset.csv'\n",
        "tempdf = pd.read_csv(path)\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    pattern = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    return pattern.sub('', text)\n",
        "\n",
        "df = tempdf.copy()\n",
        "df['review'] = tempdf['review'].apply(remove_html_tags)\n",
        "file_path = \"/content/drive/MyDrive/input.txt\"\n",
        "\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    for item in df['review']:\n",
        "        file.write(item + '\\n')\n",
        "\n",
        "vocab_file = '/content/drive/MyDrive/vocab.txt'\n",
        "input_file = file_path\n",
        "output_file = '/content/drive/MyDrive/output.h5'\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--vocab_file\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The vocabulary the BERT model will train on.\")\n",
        "    parser.add_argument(\"--input_file\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The input train corpus. can be directory with .txt files or a path to a single file\")\n",
        "    parser.add_argument(\"--output_file\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The output file where the model checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "\n",
        "    # str\n",
        "    parser.add_argument(\"--bert_model\", default=\"bert-large-uncased\", type=str, required=False,\n",
        "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
        "                              \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
        "\n",
        "    #int\n",
        "    parser.add_argument(\"--max_seq_length\",\n",
        "                        default=128,\n",
        "                        type=int,\n",
        "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
        "                             \"than this will be padded.\")\n",
        "    parser.add_argument(\"--dupe_factor\",\n",
        "                        default=10,\n",
        "                        type=int,\n",
        "                        help=\"Number of times to duplicate the input data (with different masks).\")\n",
        "    parser.add_argument(\"--max_predictions_per_seq\",\n",
        "                        default=20,\n",
        "                        type=int,\n",
        "                        help=\"Maximum sequence length.\")\n",
        "\n",
        "\n",
        "    # floats\n",
        "\n",
        "    parser.add_argument(\"--masked_lm_prob\",\n",
        "                        default=0.15,\n",
        "                        type=float,\n",
        "                        help=\"Masked LM probability.\")\n",
        "\n",
        "    parser.add_argument(\"--short_seq_prob\",\n",
        "                        default=0.1,\n",
        "                        type=float,\n",
        "                        help=\"Probability to create a sequence shorter than maximum sequence length\")\n",
        "\n",
        "    parser.add_argument(\"--do_lower_case\",\n",
        "                        action='store_true',\n",
        "                        default=True,\n",
        "                        help=\"Whether to lower case the input text. True for uncased models, False for cased models.\")\n",
        "    parser.add_argument('--random_seed',\n",
        "                        type=int,\n",
        "                        default=12345,\n",
        "                        help=\"random seed for initialization\")\n",
        "\n",
        "\n",
        "    args = argparse.Namespace(\n",
        "    vocab_file='/content/drive/MyDrive/vocab.txt',\n",
        "    input_file=\"/content/drive/MyDrive/input.txt\",\n",
        "    output_file='/content/drive/MyDrive/output.h5',\n",
        "    max_seq_length=128,\n",
        "    dupe_factor=10,\n",
        "    max_predictions_per_seq=20,\n",
        "    masked_lm_prob=0.15,\n",
        "    short_seq_prob=0.1,\n",
        "    do_lower_case=True,\n",
        "    random_seed=12345\n",
        "    )\n",
        "\n",
        "    tokenizer = BertTokenizer(args.vocab_file, do_lower_case=args.do_lower_case, max_len=512)\n",
        "\n",
        "    input_files = []\n",
        "    if os.path.isfile(args.input_file):\n",
        "      input_files.append(args.input_file)\n",
        "    elif os.path.isdir(args.input_file):\n",
        "      input_files = [os.path.join(args.input_file, f) for f in os.listdir(args.input_file) if (os.path.isfile(os.path.join(args.input_file, f)) and f.endswith('.txt') )]\n",
        "    else:\n",
        "      raise ValueError(\"{} is not a valid path\".format(args.input_file))\n",
        "\n",
        "    rng = random.Random(args.random_seed)\n",
        "    instances = create_training_instances(\n",
        "        input_files, tokenizer, args.max_seq_length, args.dupe_factor,\n",
        "        args.short_seq_prob, args.masked_lm_prob, args.max_predictions_per_seq,\n",
        "        rng)\n",
        "\n",
        "    output_file = args.output_file\n",
        "\n",
        "\n",
        "    write_instance_to_example_file(instances, tokenizer, args.max_seq_length,\n",
        "                                    args.max_predictions_per_seq, output_file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "PqUV_X10aL_6",
        "outputId": "7d8b87f0-3997-4cf3-ee00-00fac050a385",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "creating instance from /content/drive/MyDrive/input.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58138/58138 [00:06<00:00, 8989.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class H5Dataset(Dataset):\n",
        "    def __init__(self, h5_file):\n",
        "        self.h5_file = h5_file\n",
        "        with h5py.File(h5_file, 'r') as f:\n",
        "            self.input_ids = f['input_ids'][:]\n",
        "            self.input_mask = f['input_mask'][:]\n",
        "            self.segment_ids = f['segment_ids'][:]\n",
        "            self.masked_lm_positions = f['masked_lm_positions'][:]\n",
        "            self.masked_lm_ids = f['masked_lm_ids'][:]\n",
        "            self.next_sentence_labels = f['next_sentence_labels'][:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'input_mask': torch.tensor(self.input_mask[idx], dtype=torch.long),\n",
        "            'segment_ids': torch.tensor(self.segment_ids[idx], dtype=torch.long),\n",
        "            'masked_lm_positions': torch.tensor(self.masked_lm_positions[idx], dtype=torch.long),\n",
        "            'masked_lm_ids': torch.tensor(self.masked_lm_ids[idx], dtype=torch.long),\n",
        "            'next_sentence_labels': torch.tensor(self.next_sentence_labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "dataset = H5Dataset('/content/drive/MyDrive/output.h5')\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "_JDizczOwPIc"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 전처리 데이터 로드\n",
        "with h5py.File('/content/drive/MyDrive/output.h5', 'r') as f:\n",
        "    input_ids = f['input_ids'][:]\n",
        "    input_mask = f['input_mask'][:]\n",
        "    masked_lm_ids = f['masked_lm_ids'][:]\n",
        "    masked_lm_positions = f['masked_lm_positions'][:]\n",
        "    next_sentence_labels = f['next_sentence_labels'][:]\n",
        "    segment_ids = f['segment_ids'][:]"
      ],
      "metadata": {
        "id": "5LjlRzg7iGrs"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f'input_ids shape: {input_ids.shape}')\n",
        "print(f'input_mask shape: {input_mask.shape}')\n",
        "print(f'masked_lm_ids shape: {masked_lm_ids.shape}')\n",
        "print(f'masked_lm_positions shape: {masked_lm_positions.shape}')\n",
        "print(f'next_sentence_labels shape: {next_sentence_labels.shape}')\n",
        "print(f'segment_ids shape: {segment_ids.shape}')"
      ],
      "metadata": {
        "id": "5Un7HDIig_8u",
        "outputId": "19bede15-b2f1-4bdb-90d9-a9245651aa03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids shape: (58138, 128)\n",
            "input_mask shape: (58138, 128)\n",
            "masked_lm_ids shape: (58138, 20)\n",
            "masked_lm_positions shape: (58138, 20)\n",
            "next_sentence_labels shape: (58138,)\n",
            "segment_ids shape: (58138, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertForPreTraining, BertConfig\n",
        "import os\n",
        "\n",
        "# 데이터 로드\n",
        "with h5py.File('/content/drive/MyDrive/output.h5', 'r') as f:\n",
        "    input_ids = f['input_ids'][:]\n",
        "    input_mask = f['input_mask'][:]\n",
        "    segment_ids = f['segment_ids'][:]\n",
        "    masked_lm_positions = f['masked_lm_positions'][:]\n",
        "    masked_lm_ids = f['masked_lm_ids'][:]\n",
        "    next_sentence_labels = f['next_sentence_labels'][:]\n",
        "\n",
        "# 모든 텐서가 동일한 첫 번째 차원을 가지도록 확인\n",
        "assert input_ids.shape[0] == input_mask.shape[0] == segment_ids.shape[0] == next_sentence_labels.shape[0]\n",
        "\n",
        "# TensorDataset 생성\n",
        "dataset = TensorDataset(torch.tensor(input_ids, dtype=torch.long),\n",
        "                        torch.tensor(input_mask, dtype=torch.long),\n",
        "                        torch.tensor(segment_ids, dtype=torch.long),\n",
        "                        torch.tensor(masked_lm_positions, dtype=torch.long),\n",
        "                        torch.tensor(masked_lm_ids, dtype=torch.long),\n",
        "                        torch.tensor(next_sentence_labels, dtype=torch.long))\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)  # 배치 사이즈 줄이기\n",
        "\n",
        "# 모델 설정\n",
        "config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining(config)\n",
        "\n",
        "# 옵티마이저 설정\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# 모델 학습\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    for batch in tqdm(dataloader):\n",
        "        input_ids, input_mask, segment_ids, masked_lm_positions, masked_lm_ids, next_sentence_labels = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        attention_mask=input_mask,\n",
        "                        token_type_ids=segment_ids)\n",
        "\n",
        "        prediction_scores = outputs.prediction_logits\n",
        "        seq_relationship_score = outputs.seq_relationship_logits\n",
        "\n",
        "        # prediction_scores와 masked_lm_labels의 크기 맞추기\n",
        "        batch_size, seq_length, vocab_size = prediction_scores.size()\n",
        "        prediction_scores = prediction_scores.view(-1, vocab_size)\n",
        "\n",
        "        # masked_lm_labels를 input_ids와 동일한 형태로 변환\n",
        "        flat_masked_lm_positions = masked_lm_positions.view(-1)\n",
        "        flat_masked_lm_ids = masked_lm_ids.view(-1)\n",
        "\n",
        "        # 유효한 마스킹 위치 필터링\n",
        "        mask = flat_masked_lm_positions != 0\n",
        "        valid_positions = flat_masked_lm_positions[mask]\n",
        "        valid_labels = flat_masked_lm_ids[mask]\n",
        "\n",
        "        prediction_scores = prediction_scores.index_select(0, valid_positions)\n",
        "\n",
        "        # next_sentence_labels와 seq_relationship_score의 크기 맞추기\n",
        "        next_sentence_labels = next_sentence_labels.view(-1)\n",
        "        seq_relationship_score = seq_relationship_score.view(-1, 2)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        masked_lm_loss = loss_fct(prediction_scores, valid_labels)\n",
        "        next_sentence_loss = loss_fct(seq_relationship_score, next_sentence_labels)\n",
        "        loss = masked_lm_loss + next_sentence_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss: {loss.item()}')\n",
        "\n",
        "# 모델 저장\n",
        "if not os.path.exists('bert'):\n",
        "    os.mkdir('bert')\n",
        "model.save_pretrained('bert/pretrained')\n"
      ],
      "metadata": {
        "id": "G2I2qUwcfm-O",
        "outputId": "670da101-3f92-461e-825d-fb5146a69b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/14535 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-f350cc6f2a1b>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnext_sentence_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}