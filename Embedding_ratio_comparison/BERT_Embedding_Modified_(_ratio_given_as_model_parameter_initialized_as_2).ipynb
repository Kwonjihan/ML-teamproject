{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62f8962ea0694223aa7be1c112aa0cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71c646605367470d817c3ab40fb6e05a",
              "IPY_MODEL_6c1cf0d058a84e609973420907fa966a",
              "IPY_MODEL_b0a7b89510014bd0a3cef49b040b636f"
            ],
            "layout": "IPY_MODEL_8679672a43b3469c9e2d7bd3fbd159f4"
          }
        },
        "71c646605367470d817c3ab40fb6e05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a72dee5d6574ceaac2239c0ee63fc97",
            "placeholder": "​",
            "style": "IPY_MODEL_29e37ea5b859456eb383fd160281cda9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6c1cf0d058a84e609973420907fa966a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7640ba8c113f4db0b8c751da09c7eb42",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20001e6300124931bca08c55ecb690ef",
            "value": 48
          }
        },
        "b0a7b89510014bd0a3cef49b040b636f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90f46a329a2243a8a8a9dd052ecf1713",
            "placeholder": "​",
            "style": "IPY_MODEL_dc658a7f2d2b46f18c20c799a3aefa46",
            "value": " 48.0/48.0 [00:00&lt;00:00, 4.61kB/s]"
          }
        },
        "8679672a43b3469c9e2d7bd3fbd159f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a72dee5d6574ceaac2239c0ee63fc97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e37ea5b859456eb383fd160281cda9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7640ba8c113f4db0b8c751da09c7eb42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20001e6300124931bca08c55ecb690ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90f46a329a2243a8a8a9dd052ecf1713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc658a7f2d2b46f18c20c799a3aefa46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c318f848a4274861ac1857a817cd5989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_629eeff9bfad460fb3cea3af69eb7046",
              "IPY_MODEL_f577f38837024a4eb622519c29bc67c5",
              "IPY_MODEL_2480bf26b32a42c4b2ca28ecf1d42729"
            ],
            "layout": "IPY_MODEL_fd4fc4db807044e4a471ed363cfc1971"
          }
        },
        "629eeff9bfad460fb3cea3af69eb7046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffeb85601ea547379bc65a86dd925e53",
            "placeholder": "​",
            "style": "IPY_MODEL_425c4254d0ac4a579973b751bbec2982",
            "value": "vocab.txt: 100%"
          }
        },
        "f577f38837024a4eb622519c29bc67c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31d5ddaa4dcd4b7fa8ea94784e3eb4db",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7226e4e6c9a9446fb09d639b250d3142",
            "value": 231508
          }
        },
        "2480bf26b32a42c4b2ca28ecf1d42729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f783d85d2f994a06ae683c0fb3c8f238",
            "placeholder": "​",
            "style": "IPY_MODEL_2b4faf0d6a8b44e4a0885e8566a8e57f",
            "value": " 232k/232k [00:00&lt;00:00, 5.59MB/s]"
          }
        },
        "fd4fc4db807044e4a471ed363cfc1971": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffeb85601ea547379bc65a86dd925e53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "425c4254d0ac4a579973b751bbec2982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31d5ddaa4dcd4b7fa8ea94784e3eb4db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7226e4e6c9a9446fb09d639b250d3142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f783d85d2f994a06ae683c0fb3c8f238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b4faf0d6a8b44e4a0885e8566a8e57f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26ce14b3d4e7468082b91e1bc51ef2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a53c6d547a754acebc433b2228f78eb7",
              "IPY_MODEL_193b1d2d153c44368791d307ba2a00ae",
              "IPY_MODEL_123f67e906954320a0eb8b33fd733072"
            ],
            "layout": "IPY_MODEL_7657bb5d7c7949398e40f30f27b05fe7"
          }
        },
        "a53c6d547a754acebc433b2228f78eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7ebe1209a5744e289aa6a7e1078cebe",
            "placeholder": "​",
            "style": "IPY_MODEL_09169452a2394c0a9617479f5f824ef7",
            "value": "tokenizer.json: 100%"
          }
        },
        "193b1d2d153c44368791d307ba2a00ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c035ffe6e8364d388a7c595b02ee2f36",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_baf1a4a44ea24542977ffc2a58fb9ee2",
            "value": 466062
          }
        },
        "123f67e906954320a0eb8b33fd733072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89759078cb4d47ab9c575e894d47e4c5",
            "placeholder": "​",
            "style": "IPY_MODEL_dea076b99f1c475586353d956a5aeedb",
            "value": " 466k/466k [00:00&lt;00:00, 27.3MB/s]"
          }
        },
        "7657bb5d7c7949398e40f30f27b05fe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7ebe1209a5744e289aa6a7e1078cebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09169452a2394c0a9617479f5f824ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c035ffe6e8364d388a7c595b02ee2f36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baf1a4a44ea24542977ffc2a58fb9ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89759078cb4d47ab9c575e894d47e4c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dea076b99f1c475586353d956a5aeedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d92f358011e4881a6925e596ba719a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fac62a2c1013405595c840c338942c9c",
              "IPY_MODEL_2bffb103d9ab45039d26bbeb77849834",
              "IPY_MODEL_af807567c76440d687f7a5ec693c22f6"
            ],
            "layout": "IPY_MODEL_96dba14eeb2446988105809ed4ffe4bc"
          }
        },
        "fac62a2c1013405595c840c338942c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a34075e37b4187bb682e940301327c",
            "placeholder": "​",
            "style": "IPY_MODEL_f5a70856bde24dd487d4fc6b7dd6057c",
            "value": "config.json: 100%"
          }
        },
        "2bffb103d9ab45039d26bbeb77849834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e00e26744f74c22aff3b2f0e6a99639",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dedee8d8040a4c8497dd3a029445cf66",
            "value": 570
          }
        },
        "af807567c76440d687f7a5ec693c22f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1797467e0b74584ae4aaa22831d2a7d",
            "placeholder": "​",
            "style": "IPY_MODEL_2966ae172cf7437a9ab08a9f833a707a",
            "value": " 570/570 [00:00&lt;00:00, 56.7kB/s]"
          }
        },
        "96dba14eeb2446988105809ed4ffe4bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6a34075e37b4187bb682e940301327c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a70856bde24dd487d4fc6b7dd6057c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e00e26744f74c22aff3b2f0e6a99639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dedee8d8040a4c8497dd3a029445cf66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1797467e0b74584ae4aaa22831d2a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2966ae172cf7437a9ab08a9f833a707a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwonjihan/ML-teamproject/blob/develop/Embedding_ratio_comparison/BERT_Embedding_Modified_(_ratio_given_as_model_parameter_initialized_as_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path='/content/drive/MyDrive/6000_IMDB_Dataset.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfRNbEvV-8tp",
        "outputId": "b3f05b80-9207-44fd-b364-920f0850892c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from typing import Optional, Tuple\n",
        "import re"
      ],
      "metadata": {
        "id": "woLaLsvCPFAi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NRCLex\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "CgRXfpJiLsAz",
        "outputId": "45fe1d7f-4e15-4277-ba62-f81529eb9b01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting NRCLex\n",
            "  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\n",
            "Collecting textblob (from NRCLex)\n",
            "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of nrclex to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting NRCLex\n",
            "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.10/dist-packages (from textblob->NRCLex) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob->NRCLex) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob->NRCLex) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob->NRCLex) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->textblob->NRCLex) (4.66.4)\n",
            "Building wheels for collected packages: NRCLex\n",
            "  Building wheel for NRCLex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43309 sha256=e0861d63f164301fecb6787516e906756d2597c522dddc879d46fc5eb956ada4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/10/44/6abfb1234298806a145fd6bcaec8cbc712e88dd1cd6cb242fa\n",
            "Successfully built NRCLex\n",
            "Installing collected packages: textblob, NRCLex\n",
            "Successfully installed NRCLex-3.0.0 textblob-0.18.0.post0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 감정에 해당하는 토큰 임베딩에 큰 포션을 취하는 방법\n",
        "- 1. 우선 문장을 토큰화하기 전에 감정이 드러난 단어를 찾는다\n",
        "- 2. 해당 단어를 버트 토크나이저를 통해 토크나이즈 된 결과를 저장해둔다\n",
        "- 3. 만약 감정이 있다면 해당 단어가 벡터로 표현되고 합해져서 임베딩 될 때 더 크게 영향을 주도록 한다."
      ],
      "metadata": {
        "id": "px00_B1UNyv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 문장에서 단어 감정 추출 예시 ##\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def analyze_sentence(sentence):\n",
        "    # 문장을 토큰화\n",
        "    words = word_tokenize(sentence)\n",
        "\n",
        "    # VADER 초기화\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # 감정이 포함된 단어들 저장\n",
        "    positive_words = []\n",
        "    negative_words = []\n",
        "\n",
        "    # 각 단어의 감정 점수 분석\n",
        "    for word in words:\n",
        "        score = analyzer.polarity_scores(word)['compound']\n",
        "        if score > 0:\n",
        "            positive_words.append(word)\n",
        "        elif score < 0:\n",
        "            negative_words.append(word)\n",
        "\n",
        "    return positive_words, negative_words\n",
        "\n",
        "# 테스트 문장\n",
        "sentence = \"Petter Mattei's Love in the Time of Money is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter.\"\n",
        "positive_words, negative_words = analyze_sentence(sentence)\n",
        "\n",
        "print(\"Positive words:\", positive_words)\n",
        "print(\"Negative words:\", negative_words)\n"
      ],
      "metadata": {
        "id": "rGZfCrtcI7Ba",
        "outputId": "2ad15185-84d8-4a37-a274-e9e554216322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive words: ['Love', 'stunning', 'success']\n",
            "Negative words: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.우선 문장을 토큰화하기 전에 감정이 드러난 단어를 찾는다\n",
        "## 2.해당 단어를 버트 토크나이저를 통해 토크나이즈 된 결과를 저장해둔다"
      ],
      "metadata": {
        "id": "ZzetDrS4Op6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 #\n",
        "import itertools\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = re.sub(r'<[^>]+>', ' ', self.texts[idx])  # HTML 태그 제거\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # 문장을 토큰화하고 감정 점수를 분석\n",
        "        words = word_tokenize(text)\n",
        "        sentiment_words = []\n",
        "        for word in words:\n",
        "            score = self.analyzer.polarity_scores(word)['compound']\n",
        "            if score > 0:\n",
        "                sentiment_words.append(word)\n",
        "            elif score < 0:\n",
        "                sentiment_words.append(word)\n",
        "\n",
        "\n",
        "        # 감정 단어들을 토큰화\n",
        "        tokenized_sentiment_words = []\n",
        "        for word in sentiment_words:\n",
        "            tokenized = self.tokenizer(word, add_special_tokens=False)['input_ids']\n",
        "            tokenized_sentiment_words.append(tokenized) # 토큰화된 감정단어들\n",
        "\n",
        "        # '##' 붙은 서브워드 제거\n",
        "        filtered_tokens = []\n",
        "        for token_ids in tokenized_sentiment_words:\n",
        "            tokens = self.tokenizer.convert_ids_to_tokens(token_ids)\n",
        "            filtered_tokens.extend([token for token in tokens if not token.startswith('##')])\n",
        "\n",
        "        # 토큰들을 하나의 텍스트로 이어붙이기\n",
        "        result_text = ' '.join(filtered_tokens)\n",
        "\n",
        "        # 토큰화 및 인코딩\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "        token_type_ids = encoding['token_type_ids'].squeeze()\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            result_text,\n",
        "            add_special_tokens=False,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        sentiment_tokens = encoding['input_ids'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'token_type_ids': token_type_ids,\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            #'sentiment_words' :  sentiment_words,\n",
        "            #'tokenized_sentiment_words' : tokenized_sentiment_words,\n",
        "            \"sentiment_tokens\" :  sentiment_tokens,\n",
        "        }\n",
        "\n",
        "def load_data(file_path, tokenizer, max_length=128):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "    texts = df['review'].tolist()   # 리스트 요소 하나에 풀 문장이 들어있음\n",
        "    labels = df['sentiment'].tolist()\n",
        "    print(texts[0])\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
        "    print(train_texts[0])\n",
        "\n",
        "    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "    return train_dataset, val_dataset\n"
      ],
      "metadata": {
        "id": "CIQbjs9wOs-p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CPKCGzikOsnP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zuK6Nz5JO1ip",
        "outputId": "9bc339f9-3def-4836-81cc-b3aee25ea02e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349,
          "referenced_widgets": [
            "62f8962ea0694223aa7be1c112aa0cea",
            "71c646605367470d817c3ab40fb6e05a",
            "6c1cf0d058a84e609973420907fa966a",
            "b0a7b89510014bd0a3cef49b040b636f",
            "8679672a43b3469c9e2d7bd3fbd159f4",
            "0a72dee5d6574ceaac2239c0ee63fc97",
            "29e37ea5b859456eb383fd160281cda9",
            "7640ba8c113f4db0b8c751da09c7eb42",
            "20001e6300124931bca08c55ecb690ef",
            "90f46a329a2243a8a8a9dd052ecf1713",
            "dc658a7f2d2b46f18c20c799a3aefa46",
            "c318f848a4274861ac1857a817cd5989",
            "629eeff9bfad460fb3cea3af69eb7046",
            "f577f38837024a4eb622519c29bc67c5",
            "2480bf26b32a42c4b2ca28ecf1d42729",
            "fd4fc4db807044e4a471ed363cfc1971",
            "ffeb85601ea547379bc65a86dd925e53",
            "425c4254d0ac4a579973b751bbec2982",
            "31d5ddaa4dcd4b7fa8ea94784e3eb4db",
            "7226e4e6c9a9446fb09d639b250d3142",
            "f783d85d2f994a06ae683c0fb3c8f238",
            "2b4faf0d6a8b44e4a0885e8566a8e57f",
            "26ce14b3d4e7468082b91e1bc51ef2cd",
            "a53c6d547a754acebc433b2228f78eb7",
            "193b1d2d153c44368791d307ba2a00ae",
            "123f67e906954320a0eb8b33fd733072",
            "7657bb5d7c7949398e40f30f27b05fe7",
            "f7ebe1209a5744e289aa6a7e1078cebe",
            "09169452a2394c0a9617479f5f824ef7",
            "c035ffe6e8364d388a7c595b02ee2f36",
            "baf1a4a44ea24542977ffc2a58fb9ee2",
            "89759078cb4d47ab9c575e894d47e4c5",
            "dea076b99f1c475586353d956a5aeedb",
            "1d92f358011e4881a6925e596ba719a4",
            "fac62a2c1013405595c840c338942c9c",
            "2bffb103d9ab45039d26bbeb77849834",
            "af807567c76440d687f7a5ec693c22f6",
            "96dba14eeb2446988105809ed4ffe4bc",
            "f6a34075e37b4187bb682e940301327c",
            "f5a70856bde24dd487d4fc6b7dd6057c",
            "5e00e26744f74c22aff3b2f0e6a99639",
            "dedee8d8040a4c8497dd3a029445cf66",
            "c1797467e0b74584ae4aaa22831d2a7d",
            "2966ae172cf7437a9ab08a9f833a707a"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62f8962ea0694223aa7be1c112aa0cea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c318f848a4274861ac1857a817cd5989"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26ce14b3d4e7468082b91e1bc51ef2cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d92f358011e4881a6925e596ba719a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "Forbidden Siren is based upon the Siren 2 Playstation 2 (so many 2s) game. Like most video game turned movies, I would say the majority don't translate into a different medium really well. And that goes for this one too, painfully.<br /><br />There's a pretty long prologue which explains and sets the premise for the story, and the mysterious island on which a writer (Leo Morimoto) and his children, daughter Yuki (Yui Ichikawa) and son Hideo (Jun Nishiyama) come to move into. The villagers don't look all too friendly, and soon enough, sound advice is given about the siren on the island, to stay indoors once the siren starts wailing.<br /><br />Naturally and slowly, things start to go bump, and our siblings go on a mission beating around the bush to discover exactly what is happening on this unfriendly island with its strange inhabitants. But in truth, you will not bother with what's going on, as folklore and fairy tales get thrown in to convolute the plot even more. What was really pushing it into the realm of bad comedy are its unwittingly ill-placed-out-of-the-norm moments which just drew pitiful giggles at its sheer stupidity, until it's explained much later. It's one thing trying to come up and present something smart, but another thing doing it convincingly and with loopholes covered.<br /><br />Despite it clocking in under 90 minutes - I think it's a horror movie phenomenon to have that as a runtime benchmark - it gives that almost two hour feel with its slow buildup to tell what it wants to. Things begin to pick up toward the last 20 minutes, but it's a classic case of too little too late.<br /><br />What saves the movie is how it changes tack and its revelation at the end. Again this is a common device used to try and elevate a seemingly simple horror movie into something a little bit extra in the hope of wowing an audience. It turned out rather satisfactorily, but leaves a bad aftertaste as you'll feel cheated somewhat. There are two ways a twist will make you feel - it either elevates the movie to a memorable level, or provides you with that hokey feeling. Unfortunately Forbidden Siren belonged more to the latter.<br /><br />The saving grace will be its cinematography with its use of light, shadows and mirrors, but I will be that explicit - it's still not worth the time, so better to avoid this.\n"
          ]
        }
      ],
      "source": [
        "# 토크나이저 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# 데이터 로드\n",
        "train_dataset, val_dataset = load_data(file_path, tokenizer)\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    ## 예시 확인하기 ##\n",
        "df = pd.read_csv(file_path)\n",
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "texts = df['review'].tolist()\n",
        "labels = df['sentiment'].tolist()\n",
        "dataset = SentimentDataset(texts, labels, tokenizer, 128)\n",
        "\"\"\"\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            token_type_ids=batch['token_type_ids'],\n",
        "            labels=batch['labels'],\n",
        "            sentiment_tokens=batch['sentiment_tokens']\n",
        "\"\"\"\n",
        "print(\"Sample text:\", texts[3])\n",
        "sample_item = dataset[19]\n",
        "print(\"Tokenized input IDs:\", sample_item['input_ids'].tolist())\n",
        "print(\"Tokenized tokens:\", tokenizer.convert_ids_to_tokens(sample_item['input_ids'].tolist()))\n",
        "print(\"Tokenized attention mask:\", sample_item['attention_mask'].tolist())\n",
        "print(\"Tokenized token type IDs:\", sample_item['token_type_ids'].tolist())\n",
        "print(\"Label:\", sample_item['labels'].item())\n",
        "print('sentiment_words:', sample_item['sentiment_words'])\n",
        "print('Tokenized sentiment words:',sample_item['tokenized_sentiment_words'])\n",
        "\n",
        "# 토큰화된 감정 단어들 출력\n",
        "tokenized_sentiment_words = sample_item['tokenized_sentiment_words']\n",
        "all_tokens = []\n",
        "filtered_all_tokens = []\n",
        "\n",
        "for token_ids in tokenized_sentiment_words:\n",
        "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "        filtered_tokens = [token for token in tokens if not token.startswith('##')]\n",
        "        filtered_all_tokens.append(filtered_tokens)\n",
        "        all_tokens.append(tokens)\n",
        "\n",
        "print('## removed Tokenized sentiment words :', filtered_all_tokens)\n",
        "print(len(sample_item['input_ids']))\n",
        "print(len(sample_item['attention_mask']))\n",
        "print(len(sample_item['token_type_ids']))\n",
        "print(len(sample_item[\"attention_mask\"]))\n",
        "print(len(sample_item[\"sentiment_tokens\"]))\n",
        "    ##===============##"
      ],
      "metadata": {
        "id": "xnF-HxEmERu7",
        "outputId": "2c22c73a-9e6f-4ee8-910d-13919e3835ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample text: Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.\n",
            "Tokenized input IDs: [101, 2023, 3185, 2003, 2241, 2006, 1996, 2338, 1010, 1000, 1037, 2116, 11867, 7770, 7983, 2098, 2518, 1000, 2011, 7658, 10514, 25811, 1998, 10455, 3314, 1997, 2679, 4262, 2090, 4004, 2015, 1998, 12461, 1010, 1037, 8476, 2008, 3310, 2013, 7658, 1005, 1055, 3167, 6322, 2004, 2019, 23399, 3652, 2039, 1999, 2859, 1012, 2008, 4281, 1010, 1998, 1996, 3376, 4291, 4290, 10906, 1010, 3957, 2023, 2293, 2466, 1037, 4310, 1998, 2738, 15236, 7224, 2005, 2049, 2051, 1012, 2060, 2084, 2008, 1010, 1996, 2466, 2003, 1037, 12991, 27086, 7472, 2007, 1037, 13432, 2299, 2008, 2003, 3383, 2062, 4622, 2084, 1996, 3185, 2993, 1012, 1996, 3376, 7673, 3557, 3504, 1996, 2112, 1998, 3957, 1037, 6919, 1010, 7436, 4222, 2836, 2004, 1037, 3460, 1997, 3816, 8843, 2076, 1996, 13896, 1997, 15523, 102]\n",
            "Tokenized tokens: ['[CLS]', 'this', 'movie', 'is', 'based', 'on', 'the', 'book', ',', '\"', 'a', 'many', 'sp', '##len', '##dor', '##ed', 'thing', '\"', 'by', 'han', 'su', '##yin', 'and', 'tackles', 'issues', 'of', 'race', 'relations', 'between', 'asian', '##s', 'and', 'whites', ',', 'a', 'topic', 'that', 'comes', 'from', 'han', \"'\", 's', 'personal', 'experiences', 'as', 'an', 'eurasian', 'growing', 'up', 'in', 'china', '.', 'that', 'background', ',', 'and', 'the', 'beautiful', 'hong', 'kong', 'settings', ',', 'gives', 'this', 'love', 'story', 'a', 'unique', 'and', 'rather', 'daring', 'atmosphere', 'for', 'its', 'time', '.', 'other', 'than', 'that', ',', 'the', 'story', 'is', 'a', 'stereo', '##typical', 'romance', 'with', 'a', 'memorable', 'song', 'that', 'is', 'perhaps', 'more', 'remembered', 'than', 'the', 'movie', 'itself', '.', 'the', 'beautiful', 'jennifer', 'jones', 'looks', 'the', 'part', 'and', 'gives', 'a', 'wonderful', ',', 'oscar', 'nominated', 'performance', 'as', 'a', 'doctor', 'of', 'mixed', 'breed', 'during', 'the', 'advent', 'of', 'communism', '[SEP]']\n",
            "Tokenized attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Tokenized token type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Label: 1\n",
            "sentiment_words: ['growing', 'beautiful', 'love', 'daring', 'romance', 'beautiful', 'wonderful', 'better', 'playing', 'romantic', 'war', 'torn', 'top', 'lovers', 'affection', 'sure', 'romantically', 'lovers', 'sentimental', 'romances', 'enjoy', 'love']\n",
            "Tokenized sentiment words: [[3652], [3376], [2293], [15236], [7472], [3376], [6919], [2488], [2652], [6298], [2162], [7950], [2327], [10205], [12242], [2469], [6298, 3973], [10205], [23069], [7472, 2015], [5959], [2293]]\n",
            "## removed Tokenized sentiment words : [['growing'], ['beautiful'], ['love'], ['daring'], ['romance'], ['beautiful'], ['wonderful'], ['better'], ['playing'], ['romantic'], ['war'], ['torn'], ['top'], ['lovers'], ['affection'], ['sure'], ['romantic'], ['lovers'], ['sentimental'], ['romance'], ['enjoy'], ['love']]\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [모델 준비] #\n",
        "\n",
        "# 활성화 함수\n",
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "# 활성화 함수 매핑\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": torch.nn.functional.silu}\n",
        "\n",
        "# 모델 설정\n",
        "class Config:\n",
        "    vocab_size = 30522\n",
        "    hidden_size = 128\n",
        "    num_hidden_layers = 4\n",
        "    num_attention_heads = 4\n",
        "    intermediate_size = 512\n",
        "    hidden_act = \"gelu\"\n",
        "    hidden_dropout_prob = 0.1\n",
        "    attention_probs_dropout_prob = 0.1\n",
        "    max_position_embeddings = 512\n",
        "    type_vocab_size = 2\n",
        "    initializer_range = 0.02\n",
        "    layer_norm_eps = 1e-12\n",
        "    pad_token_id = 0\n",
        "    gradient_checkpointing = False\n",
        "    position_embedding_type = \"absolute\"\n",
        "    use_cache = True\n",
        "    is_decoder = False\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    #def __init__(self, config: Config):\n",
        "    def __init__(self, config: Config, sentiment_ratio_init: float = 2.0):\n",
        "        super().__init__()\n",
        "        # 단어 임베딩, 위치 임베딩, 토큰 타입 임베딩\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # 레이어 정규화와 드롭아웃\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False)\n",
        "        self.register_buffer(\"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False)\n",
        "\n",
        "        # 학습 가능한 sentiment_ratio 파라미터 추가 및 초기값 설정\n",
        "        self.sentiment_ratio = nn.Parameter(torch.tensor(sentiment_ratio_init))\n",
        "\n",
        "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0, sentiment_tokens=None):\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, past_key_values_length: seq_length + past_key_values_length]\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        # 입력 임베딩 생성\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "\n",
        "        for i in range(input_ids.size(0)):  # 배치의 각 문장에 대해\n",
        "            sentiment_token = sentiment_tokens[i]\n",
        "            sentiment_token_filtered = sentiment_token[(sentiment_token != 0) & (sentiment_token != 101) & (sentiment_token != 102)]\n",
        "            for j in range(input_ids.size(1)):  # 각 문장의 각 토큰에 대해\n",
        "                if input_ids[i, j] in sentiment_token_filtered:\n",
        "                    # 파라미터로 사용시\n",
        "                    embeddings[i, j] = self.sentiment_ratio * inputs_embeds[i, j] + token_type_embeddings[i, j]\n",
        "                    #print(\"sentiment token ratio : \",self.sentiment_ratio)\n",
        "                if j == 0 and i == 0:\n",
        "                    print(\"Sentiment ratio for this batch: \", self.sentiment_ratio.item())\n",
        "                    # 정해줄 때\n",
        "                    #embeddings[i, j] = 2 * inputs_embeds[i, j] + token_type_embeddings[i, j]\n",
        "\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# 셀프 어텐션 구현 클래스\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config, position_embedding_type=None):\n",
        "        super().__init__()\n",
        "        # hidden_size가 num_attention_heads의 배수가 아니면 오류 발생\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        # 어텐션 헤드의 수와 각 헤드의 크기, 전체 헤드 크기 설정\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(\n",
        "            config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        # Query, Key, Value 행렬 정의\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        # 드롭아웃 레이어 정의\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        # 위치 임베딩 유형 설정\n",
        "        self.position_embedding_type = position_embedding_type or getattr(\n",
        "            config, \"position_embedding_type\", \"absolute\"\n",
        "        )\n",
        "        # 상대적 위치 임베딩을 사용하는 경우, 위치 임베딩 레이어 정의\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(\n",
        "                2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        # 디코더인지 여부 설정\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # 텐서의 크기 변환\n",
        "        new_x_shape = x.size()[\n",
        "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        # 텐서의 차원 변경 [batch_size, num_heads, seq_len, head_size]\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        # Query 레이어 계산\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # 크로스 어텐션인지 여부 확인\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # 과거의 k, v 값을 재사용 (크로스 어텐션)\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            # 인코더의 키와 값을 사용하여 크로스 어텐션 수행\n",
        "            key_layer = self.transpose_for_scores(\n",
        "                self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(\n",
        "                self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            # 과거의 k, v 값을 현재의 k, v와 결합 (디코더의 셀프 어텐션)\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            # 현재의 히든 스테이트에서 키와 값을 계산 (셀프 어텐션)\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        # Query 레이어 변환\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # 캐시를 사용할지 여부 설정\n",
        "        use_cache = past_key_value is not None\n",
        "        if self.is_decoder:\n",
        "            # 디코더인 경우, 키와 값을 캐싱\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Query와 Key의 내적(dot product)을 통해 어텐션 스코어 계산\n",
        "        attention_scores = torch.matmul(\n",
        "            query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            # 상대적 위치 임베딩을 사용하는 경우\n",
        "            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
        "            if use_cache:\n",
        "                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n",
        "                    -1, 1\n",
        "                )\n",
        "            else:\n",
        "                position_ids_l = torch.arange(\n",
        "                    query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(\n",
        "                key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "\n",
        "            # 거리 임베딩 계산\n",
        "            positional_embedding = self.distance_embedding(\n",
        "                distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(\n",
        "                dtype=query_layer.dtype)  # fp16 호환성\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                # 상대적 위치 임베딩을 쿼리에 적용\n",
        "                relative_position_scores = torch.einsum(\n",
        "                    \"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                # 상대적 위치 임베딩을 쿼리와 키에 적용\n",
        "                relative_position_scores_query = torch.einsum(\n",
        "                    \"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\n",
        "                    \"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + \\\n",
        "                    relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        # 어텐션 스코어를 정규화\n",
        "        attention_scores = attention_scores / \\\n",
        "            math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # 어텐션 마스크 적용\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # 어텐션 스코어를 확률로 변환\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # 드롭아웃 적용\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # 헤드 마스크 적용\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        # 컨텍스트 레이어 계산\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        # 텐서의 크기 변환 및 재배치\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[\n",
        "            :-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        # 출력 생성\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (\n",
        "            context_layer,)\n",
        "\n",
        "        # 디코더인 경우, past_key_value를 출력에 포함\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# 셀프 어텐션 출력 처리 클래스\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # 드롭아웃, 레이어 정규화, 잔차 연결 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "# 어텐션 메커니즘 클래스\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        # 셀프 어텐션 및 출력 계산\n",
        "        self_outputs = self.self(\n",
        "            input_tensor,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], input_tensor)\n",
        "        outputs = (attention_output,) + self_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "# 중간 레이어 활성화 함수 클래스\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # 중간 레이어 활성화 함수 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "# 중간 레이어 출력 처리 클래스\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # 드롭아웃, 레이어 정규화, 잔차 연결 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.Layer\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # 드롭아웃, 레이어 정규화, 잔차 연결 적용\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "# 하나의 BERT 레이어를 구현하는 클래스\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        # 어텐션과 출력 계산\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        layer_output = self.output(self.intermediate(attention_output), attention_output)\n",
        "        outputs = (layer_output,) + self_attention_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "# 여러 BERT 레이어를 포함하는 인코더 클래스\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                hidden_states,\n",
        "                attention_mask,\n",
        "                layer_head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        return (hidden_states, all_hidden_states, all_attentions)\n",
        "\n",
        "# 첫 번째 토큰의 출력을 풀링하는 클래스\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # 첫 번째 토큰의 텐서를 사용해 풀링 출력 생성\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "# 전체 BERT 모델을 구현하는 클래스\n",
        "class  userBertModel(nn.Module):\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None, sentiment_tokens=None):\n",
        "        # 입력 텐서의 크기 확인\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"input_ids 혹은 inputs_embeds 둘 중 하나의 형식으로만 입력해야 합니다.\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"input_ids 또는 inputs_embeds의 형식이어야 합니다.\")\n",
        "\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(input_shape, device=device)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        # 임베딩 출력 계산\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            sentiment_tokens= sentiment_tokens,\n",
        "        )\n",
        "        # 인코더 출력 계산\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        return sequence_output, pooled_output"
      ],
      "metadata": {
        "id": "8RxEd2bIQyQ1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nAXhMfNkYbth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "from tqdm import tqdm\n",
        "# 위 구조를 취합하여 만든 이진 감정 분류를 위한 클래스\n",
        "class BertForSequenceClassification(nn.Module):\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.bert = userBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, sentiment_tokens=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            sentiment_tokens= sentiment_tokens,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# Pretrained BERT 모델 로드\n",
        "\n",
        "pretrained_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "pretrained_state_dict = pretrained_model.state_dict()\n",
        "\n",
        "# Custom 모델 초기화\n",
        "custom_config = BertConfig(\n",
        "    vocab_size=30522,\n",
        "    hidden_size=768,\n",
        "    num_hidden_layers=8,\n",
        "    num_attention_heads=12, # 수정된 부분\n",
        "    intermediate_size=3072, # 수정된 부분\n",
        "    hidden_act=\"gelu\",\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    max_position_embeddings=512,\n",
        "    type_vocab_size=2,\n",
        "    initializer_range=0.02,\n",
        "    layer_norm_eps=1e-12,\n",
        "    pad_token_id=0,\n",
        "    gradient_checkpointing=False,\n",
        "    position_embedding_type=\"absolute\",\n",
        "    use_cache=True\n",
        ")\n",
        "model = BertForSequenceClassification(custom_config, num_labels=2)\n",
        "\n",
        "# Pretrained 모델의 state_dict를 Custom 모델로 로드\n",
        "model.bert.load_state_dict(pretrained_state_dict, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# 옵티마이저 설정\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "# 학습 진행\n",
        "model.train()\n",
        "for epoch in tqdm(range(3)):\n",
        "    epoch_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            token_type_ids=batch['token_type_ids'],\n",
        "            labels=batch['labels'],\n",
        "            sentiment_tokens=batch['sentiment_tokens']\n",
        "        )\n",
        "        loss, logits = outputs\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # 예측값 계산\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct_predictions += (predicted == batch['labels']).sum().item()\n",
        "        total_predictions += batch['labels'].size(0)\n",
        "\n",
        "        if batch_idx % 10 == 0:  # 10번째 배치마다 현황 출력\n",
        "            batch_accuracy = correct_predictions / total_predictions\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}, Accuracy: {batch_accuracy}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Epoch: {epoch} finished with average loss: {avg_epoch_loss}, Accuracy: {epoch_accuracy}\")\n",
        "\n",
        "model.eval()\n",
        "epoch_loss = 0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(val_dataloader):\n",
        "        outputs = model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            token_type_ids=batch['token_type_ids'],\n",
        "            labels=batch['labels'],\n",
        "            sentiment_tokens=batch['sentiment_tokens']\n",
        "        )\n",
        "        loss, logits = outputs\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # 예측값 계산\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct_predictions += (predicted == batch['labels']).sum().item()\n",
        "        total_predictions += batch['labels'].size(0)\n",
        "\n",
        "        if batch_idx % 10 == 0:  # 10번째 배치마다 현황 출력\n",
        "            batch_accuracy = correct_predictions / total_predictions\n",
        "            print(f\"Epoch: TEST , Batch: {batch_idx}, Loss: {loss.item()}, Accuracy: {batch_accuracy}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "    print(f\"test iteration finished with average loss: {avg_epoch_loss}, Accuracy: {epoch_accuracy}\")\n",
        "\n",
        "# 모델 저장 경로 설정\n",
        "save_path = './my_finetuned_bert_model.pth'\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")"
      ],
      "metadata": {
        "id": "5aTojxNkPIxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bd18ca-a034-473e-f47a-e5834fc0e8f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment ratio for this batch:  2.0\n",
            "< test epoch > Batch: 0, Loss: 0.7101002335548401, Accuracy: 0.625\n",
            "Sentiment ratio for this batch:  1.9999948740005493\n",
            "Sentiment ratio for this batch:  1.9999909400939941\n",
            "Sentiment ratio for this batch:  1.999991536140442\n",
            "Sentiment ratio for this batch:  1.999990701675415\n",
            "Sentiment ratio for this batch:  1.9999924898147583\n",
            "Sentiment ratio for this batch:  1.9999940395355225\n",
            "Sentiment ratio for this batch:  1.9999940395355225\n",
            "Sentiment ratio for this batch:  1.9999935626983643\n",
            "Sentiment ratio for this batch:  1.9999932050704956\n",
            "Sentiment ratio for this batch:  1.9999933242797852\n",
            "< test epoch > Batch: 10, Loss: 0.7585178017616272, Accuracy: 0.48863636363636365\n",
            "Sentiment ratio for this batch:  1.9999923706054688\n",
            "Sentiment ratio for this batch:  1.9999905824661255\n",
            "Sentiment ratio for this batch:  1.99998939037323\n",
            "Sentiment ratio for this batch:  1.999988317489624\n",
            "Sentiment ratio for this batch:  1.9999867677688599\n",
            "Sentiment ratio for this batch:  1.9999858140945435\n",
            "Sentiment ratio for this batch:  1.999984622001648\n",
            "Sentiment ratio for this batch:  1.9999827146530151\n",
            "Sentiment ratio for this batch:  1.9999810457229614\n",
            "Sentiment ratio for this batch:  1.9999797344207764\n",
            "< test epoch > Batch: 20, Loss: 0.7419440150260925, Accuracy: 0.47619047619047616\n",
            "Sentiment ratio for this batch:  1.9999775886535645\n",
            "Sentiment ratio for this batch:  1.999975562095642\n",
            "Sentiment ratio for this batch:  1.9999752044677734\n",
            "Sentiment ratio for this batch:  1.9999746084213257\n",
            "Sentiment ratio for this batch:  1.999974250793457\n",
            "Sentiment ratio for this batch:  1.999972939491272\n",
            "Sentiment ratio for this batch:  1.9999725818634033\n",
            "Sentiment ratio for this batch:  1.9999723434448242\n",
            "Sentiment ratio for this batch:  1.9999719858169556\n",
            "Sentiment ratio for this batch:  1.999971628189087\n",
            "< test epoch > Batch: 30, Loss: 0.6535277962684631, Accuracy: 0.46774193548387094\n",
            "Sentiment ratio for this batch:  1.9999711513519287\n",
            "Sentiment ratio for this batch:  1.9999711513519287\n",
            "Sentiment ratio for this batch:  1.9999721050262451\n",
            "Sentiment ratio for this batch:  1.9999719858169556\n",
            "Sentiment ratio for this batch:  1.9999719858169556\n",
            "Sentiment ratio for this batch:  1.9999713897705078\n",
            "Sentiment ratio for this batch:  1.9999710321426392\n",
            "Sentiment ratio for this batch:  1.9999699592590332\n",
            "Sentiment ratio for this batch:  1.9999698400497437\n",
            "Sentiment ratio for this batch:  1.9999700784683228\n",
            "< test epoch > Batch: 40, Loss: 0.708954930305481, Accuracy: 0.4603658536585366\n",
            "Sentiment ratio for this batch:  1.999970555305481\n",
            "Sentiment ratio for this batch:  1.9999709129333496\n",
            "Sentiment ratio for this batch:  1.9999704360961914\n",
            "Sentiment ratio for this batch:  1.99997079372406\n",
            "Sentiment ratio for this batch:  1.99997079372406\n",
            "Sentiment ratio for this batch:  1.9999706745147705\n",
            "Sentiment ratio for this batch:  1.9999701976776123\n",
            "Sentiment ratio for this batch:  1.999969482421875\n",
            "Sentiment ratio for this batch:  1.999968409538269\n",
            "Sentiment ratio for this batch:  1.9999666213989258\n",
            "< test epoch > Batch: 50, Loss: 0.6348097920417786, Accuracy: 0.4852941176470588\n",
            "Sentiment ratio for this batch:  1.999964952468872\n",
            "Sentiment ratio for this batch:  1.9999628067016602\n",
            "Sentiment ratio for this batch:  1.9999603033065796\n",
            "Sentiment ratio for this batch:  1.9999576807022095\n",
            "Sentiment ratio for this batch:  1.999955415725708\n",
            "Sentiment ratio for this batch:  1.9999531507492065\n",
            "Sentiment ratio for this batch:  1.9999514818191528\n",
            "Sentiment ratio for this batch:  1.9999490976333618\n",
            "Sentiment ratio for this batch:  1.9999477863311768\n",
            "Sentiment ratio for this batch:  1.9999468326568604\n",
            "< test epoch > Batch: 60, Loss: 0.6805102825164795, Accuracy: 0.5020491803278688\n",
            "Sentiment ratio for this batch:  1.9999451637268066\n",
            "Sentiment ratio for this batch:  1.9999428987503052\n",
            "Sentiment ratio for this batch:  1.9999406337738037\n",
            "Sentiment ratio for this batch:  1.9999383687973022\n",
            "Sentiment ratio for this batch:  1.9999361038208008\n",
            "Sentiment ratio for this batch:  1.9999349117279053\n",
            "Sentiment ratio for this batch:  1.9999338388442993\n",
            "Sentiment ratio for this batch:  1.9999325275421143\n",
            "Sentiment ratio for this batch:  1.9999308586120605\n",
            "Sentiment ratio for this batch:  1.999929428100586\n",
            "< test epoch > Batch: 70, Loss: 0.7141251564025879, Accuracy: 0.5\n",
            "Sentiment ratio for this batch:  1.9999288320541382\n",
            "Sentiment ratio for this batch:  1.9999278783798218\n",
            "Sentiment ratio for this batch:  1.9999271631240845\n",
            "Sentiment ratio for this batch:  1.9999256134033203\n",
            "Sentiment ratio for this batch:  1.9999243021011353\n",
            "Sentiment ratio for this batch:  1.9999228715896606\n",
            "Sentiment ratio for this batch:  1.999922275543213\n",
            "Sentiment ratio for this batch:  1.9999216794967651\n",
            "Sentiment ratio for this batch:  1.9999209642410278\n",
            "Sentiment ratio for this batch:  1.9999206066131592\n",
            "< test epoch > Batch: 80, Loss: 0.6474105715751648, Accuracy: 0.5177469135802469\n",
            "Sentiment ratio for this batch:  1.9999197721481323\n",
            "Sentiment ratio for this batch:  1.999918818473816\n",
            "Sentiment ratio for this batch:  1.9999178647994995\n",
            "Sentiment ratio for this batch:  1.9999151229858398\n",
            "Sentiment ratio for this batch:  1.9999134540557861\n",
            "Sentiment ratio for this batch:  1.999912142753601\n",
            "Sentiment ratio for this batch:  1.9999115467071533\n",
            "Sentiment ratio for this batch:  1.9999111890792847\n",
            "Sentiment ratio for this batch:  1.9999107122421265\n",
            "Sentiment ratio for this batch:  1.9999096393585205\n",
            "< test epoch > Batch: 90, Loss: 0.7269982695579529, Accuracy: 0.5157967032967034\n",
            "Sentiment ratio for this batch:  1.9999088048934937\n",
            "Sentiment ratio for this batch:  1.9999078512191772\n",
            "Sentiment ratio for this batch:  1.9999076128005981\n",
            "Sentiment ratio for this batch:  1.9999072551727295\n",
            "Sentiment ratio for this batch:  1.9999065399169922\n",
            "Sentiment ratio for this batch:  1.9999057054519653\n",
            "Sentiment ratio for this batch:  1.9999043941497803\n",
            "Sentiment ratio for this batch:  1.999903678894043\n",
            "Sentiment ratio for this batch:  1.9999027252197266\n",
            "Sentiment ratio for this batch:  1.9999017715454102\n",
            "< test epoch > Batch: 100, Loss: 0.6610854864120483, Accuracy: 0.5167079207920792\n",
            "Sentiment ratio for this batch:  1.999900221824646\n",
            "Sentiment ratio for this batch:  1.9998987913131714\n",
            "Sentiment ratio for this batch:  1.9998974800109863\n",
            "Sentiment ratio for this batch:  1.9998968839645386\n",
            "Sentiment ratio for this batch:  1.999895453453064\n",
            "Sentiment ratio for this batch:  1.9998944997787476\n",
            "Sentiment ratio for this batch:  1.9998934268951416\n",
            "Sentiment ratio for this batch:  1.9998915195465088\n",
            "Sentiment ratio for this batch:  1.9998899698257446\n",
            "Sentiment ratio for this batch:  1.9998894929885864\n",
            "< test epoch > Batch: 110, Loss: 0.6741772890090942, Accuracy: 0.5213963963963963\n",
            "Sentiment ratio for this batch:  1.9998891353607178\n",
            "Sentiment ratio for this batch:  1.9998897314071655\n",
            "Sentiment ratio for this batch:  1.9998908042907715\n",
            "Sentiment ratio for this batch:  1.999892234802246\n",
            "Sentiment ratio for this batch:  1.9998935461044312\n",
            "Sentiment ratio for this batch:  1.9998942613601685\n",
            "Sentiment ratio for this batch:  1.9998939037322998\n",
            "Sentiment ratio for this batch:  1.999893069267273\n",
            "Sentiment ratio for this batch:  1.9998914003372192\n",
            "Sentiment ratio for this batch:  1.9998899698257446\n",
            "< test epoch > Batch: 120, Loss: 0.6053787469863892, Accuracy: 0.5278925619834711\n",
            "Sentiment ratio for this batch:  1.9998893737792969\n",
            "Sentiment ratio for this batch:  1.9998892545700073\n",
            "Sentiment ratio for this batch:  1.9998893737792969\n",
            "Sentiment ratio for this batch:  1.9998905658721924\n",
            "Sentiment ratio for this batch:  1.9998918771743774\n",
            "Sentiment ratio for this batch:  1.9998953342437744\n",
            "Sentiment ratio for this batch:  1.999898076057434\n",
            "Sentiment ratio for this batch:  1.9998998641967773\n",
            "Sentiment ratio for this batch:  1.9999014139175415\n",
            "Sentiment ratio for this batch:  1.9999020099639893\n",
            "< test epoch > Batch: 130, Loss: 0.6739412546157837, Accuracy: 0.5348282442748091\n",
            "Sentiment ratio for this batch:  1.9999016523361206\n",
            "Sentiment ratio for this batch:  1.999900221824646\n",
            "Sentiment ratio for this batch:  1.9998987913131714\n",
            "Sentiment ratio for this batch:  1.9998970031738281\n",
            "Sentiment ratio for this batch:  1.9998960494995117\n",
            "Sentiment ratio for this batch:  1.9998952150344849\n",
            "Sentiment ratio for this batch:  1.9998947381973267\n",
            "Sentiment ratio for this batch:  1.9998940229415894\n",
            "Sentiment ratio for this batch:  1.999893069267273\n",
            "Sentiment ratio for this batch:  1.9998923540115356\n",
            "< test epoch > Batch: 140, Loss: 0.6605874300003052, Accuracy: 0.5403368794326241\n",
            "Sentiment ratio for this batch:  1.9998921155929565\n",
            "Sentiment ratio for this batch:  1.9998921155929565\n",
            "Sentiment ratio for this batch:  1.9998914003372192\n",
            "Sentiment ratio for this batch:  1.9998911619186401\n",
            "Sentiment ratio for this batch:  1.999891757965088\n",
            "Sentiment ratio for this batch:  1.999890685081482\n",
            "Sentiment ratio for this batch:  1.9998900890350342\n",
            "Sentiment ratio for this batch:  1.9998884201049805\n",
            "Sentiment ratio for this batch:  1.9998877048492432\n",
            "Sentiment ratio for this batch:  1.999886155128479\n",
            "< test epoch > Batch: 150, Loss: 0.6889057755470276, Accuracy: 0.543046357615894\n",
            "Sentiment ratio for this batch:  1.999884843826294\n",
            "Sentiment ratio for this batch:  1.9998831748962402\n",
            "Sentiment ratio for this batch:  1.9998835325241089\n",
            "Sentiment ratio for this batch:  1.9998841285705566\n",
            "Sentiment ratio for this batch:  1.9998855590820312\n",
            "Sentiment ratio for this batch:  1.999886155128479\n",
            "Sentiment ratio for this batch:  1.999886155128479\n",
            "Sentiment ratio for this batch:  1.9998862743377686\n",
            "Sentiment ratio for this batch:  1.9998862743377686\n",
            "Sentiment ratio for this batch:  1.999886393547058\n",
            "< test epoch > Batch: 160, Loss: 0.5964028835296631, Accuracy: 0.5500776397515528\n",
            "Sentiment ratio for this batch:  1.9998862743377686\n",
            "Sentiment ratio for this batch:  1.9998855590820312\n",
            "Sentiment ratio for this batch:  1.9998854398727417\n",
            "Sentiment ratio for this batch:  1.9998843669891357\n",
            "Sentiment ratio for this batch:  1.9998843669891357\n",
            "Sentiment ratio for this batch:  1.9998841285705566\n",
            "Sentiment ratio for this batch:  1.9998836517333984\n",
            "Sentiment ratio for this batch:  1.9998828172683716\n",
            "Sentiment ratio for this batch:  1.9998815059661865\n",
            "Sentiment ratio for this batch:  1.9998785257339478\n",
            "< test epoch > Batch: 170, Loss: 0.6434156894683838, Accuracy: 0.5566520467836257\n",
            "Sentiment ratio for this batch:  1.9998747110366821\n",
            "Sentiment ratio for this batch:  1.9998712539672852\n",
            "Sentiment ratio for this batch:  1.999868392944336\n",
            "Sentiment ratio for this batch:  1.999864101409912\n",
            "Sentiment ratio for this batch:  1.9998600482940674\n",
            "Sentiment ratio for this batch:  1.9998550415039062\n",
            "Sentiment ratio for this batch:  1.9998496770858765\n",
            "Sentiment ratio for this batch:  1.999845027923584\n",
            "Sentiment ratio for this batch:  1.9998410940170288\n",
            "Sentiment ratio for this batch:  1.9998384714126587\n",
            "< test epoch > Batch: 180, Loss: 0.6065011620521545, Accuracy: 0.5638812154696132\n",
            "Sentiment ratio for this batch:  1.9998347759246826\n",
            "Sentiment ratio for this batch:  1.999830722808838\n",
            "Sentiment ratio for this batch:  1.9998283386230469\n",
            "Sentiment ratio for this batch:  1.9998254776000977\n",
            "Sentiment ratio for this batch:  1.9998228549957275\n",
            "Sentiment ratio for this batch:  1.9998191595077515\n",
            "Sentiment ratio for this batch:  1.99981689453125\n",
            "Sentiment ratio for this batch:  1.9998154640197754\n",
            "Sentiment ratio for this batch:  1.9998139142990112\n",
            "Sentiment ratio for this batch:  1.999812126159668\n",
            "< test epoch > Batch: 190, Loss: 0.5485479831695557, Accuracy: 0.5716623036649214\n",
            "Sentiment ratio for this batch:  1.999809980392456\n",
            "Sentiment ratio for this batch:  1.9998061656951904\n",
            "Sentiment ratio for this batch:  1.9998035430908203\n",
            "Sentiment ratio for this batch:  1.999800205230713\n",
            "Sentiment ratio for this batch:  1.9997966289520264\n",
            "Sentiment ratio for this batch:  1.9997941255569458\n",
            "Sentiment ratio for this batch:  1.999790906906128\n",
            "Sentiment ratio for this batch:  1.99978768825531\n",
            "Sentiment ratio for this batch:  1.99978506565094\n",
            "Sentiment ratio for this batch:  1.9997831583023071\n",
            "< test epoch > Batch: 200, Loss: 0.5619617104530334, Accuracy: 0.5774253731343284\n",
            "Sentiment ratio for this batch:  1.9997819662094116\n",
            "Sentiment ratio for this batch:  1.9997824430465698\n",
            "Sentiment ratio for this batch:  1.9997825622558594\n",
            "Sentiment ratio for this batch:  1.999781847000122\n",
            "Sentiment ratio for this batch:  1.9997807741165161\n",
            "Sentiment ratio for this batch:  1.9997799396514893\n",
            "Sentiment ratio for this batch:  1.999779462814331\n",
            "Sentiment ratio for this batch:  1.9997799396514893\n",
            "Sentiment ratio for this batch:  1.9997806549072266\n",
            "Sentiment ratio for this batch:  1.9997810125350952\n",
            "< test epoch > Batch: 210, Loss: 0.5762224197387695, Accuracy: 0.5850118483412322\n",
            "Sentiment ratio for this batch:  1.999780297279358\n",
            "Sentiment ratio for this batch:  1.9997798204421997\n",
            "Sentiment ratio for this batch:  1.9997793436050415\n",
            "Sentiment ratio for this batch:  1.9997782707214355\n",
            "Sentiment ratio for this batch:  1.999777913093567\n",
            "Sentiment ratio for this batch:  1.9997775554656982\n",
            "Sentiment ratio for this batch:  1.99977707862854\n",
            "Sentiment ratio for this batch:  1.9997776746749878\n",
            "Sentiment ratio for this batch:  1.9997777938842773\n",
            "Sentiment ratio for this batch:  1.9997789859771729\n",
            "< test epoch > Batch: 220, Loss: 0.41333118081092834, Accuracy: 0.5921945701357466\n",
            "Sentiment ratio for this batch:  1.999780297279358\n",
            "Sentiment ratio for this batch:  1.9997833967208862\n",
            "Sentiment ratio for this batch:  1.9997856616973877\n",
            "Sentiment ratio for this batch:  1.9997889995574951\n",
            "Sentiment ratio for this batch:  1.9997915029525757\n",
            "Sentiment ratio for this batch:  1.999794363975525\n",
            "Sentiment ratio for this batch:  1.9997981786727905\n",
            "Sentiment ratio for this batch:  1.9998016357421875\n",
            "Sentiment ratio for this batch:  1.9998040199279785\n",
            "Sentiment ratio for this batch:  1.9998060464859009\n",
            "< test epoch > Batch: 230, Loss: 0.5123773813247681, Accuracy: 0.5992965367965368\n",
            "Sentiment ratio for this batch:  1.999807357788086\n",
            "Sentiment ratio for this batch:  1.9998095035552979\n",
            "Sentiment ratio for this batch:  1.9998112916946411\n",
            "Sentiment ratio for this batch:  1.9998129606246948\n",
            "Sentiment ratio for this batch:  1.9998137950897217\n",
            "Sentiment ratio for this batch:  1.999815821647644\n",
            "Sentiment ratio for this batch:  1.9998172521591187\n",
            "Sentiment ratio for this batch:  1.999818205833435\n",
            "Sentiment ratio for this batch:  1.9998199939727783\n",
            "Sentiment ratio for this batch:  1.999820590019226\n",
            "< test epoch > Batch: 240, Loss: 0.534417450428009, Accuracy: 0.6055497925311203\n",
            "Sentiment ratio for this batch:  1.999820351600647\n",
            "Sentiment ratio for this batch:  1.999820590019226\n",
            "Sentiment ratio for this batch:  1.9998226165771484\n",
            "Sentiment ratio for this batch:  1.9998250007629395\n",
            "Sentiment ratio for this batch:  1.9998270273208618\n",
            "Sentiment ratio for this batch:  1.9998304843902588\n",
            "Sentiment ratio for this batch:  1.9998338222503662\n",
            "Sentiment ratio for this batch:  1.999837875366211\n",
            "Sentiment ratio for this batch:  1.9998390674591064\n",
            "Sentiment ratio for this batch:  1.9998401403427124\n",
            "< test epoch > Batch: 250, Loss: 0.4715498983860016, Accuracy: 0.6125498007968128\n",
            "Sentiment ratio for this batch:  1.9998397827148438\n",
            "Sentiment ratio for this batch:  1.9998403787612915\n",
            "Sentiment ratio for this batch:  1.9998435974121094\n",
            "Sentiment ratio for this batch:  1.9998469352722168\n",
            "Sentiment ratio for this batch:  1.999852180480957\n",
            "Sentiment ratio for this batch:  1.9998592138290405\n",
            "Sentiment ratio for this batch:  1.9998654127120972\n",
            "Sentiment ratio for this batch:  1.9998703002929688\n",
            "Sentiment ratio for this batch:  1.9998756647109985\n",
            "Sentiment ratio for this batch:  1.999881625175476\n",
            "< test epoch > Batch: 260, Loss: 0.3704594373703003, Accuracy: 0.6206896551724138\n",
            "Sentiment ratio for this batch:  1.9998878240585327\n",
            "Sentiment ratio for this batch:  1.9998927116394043\n",
            "Sentiment ratio for this batch:  1.9998968839645386\n",
            "Sentiment ratio for this batch:  1.9999027252197266\n",
            "Sentiment ratio for this batch:  1.999906301498413\n",
            "Sentiment ratio for this batch:  1.9999088048934937\n",
            "Sentiment ratio for this batch:  1.9999114274978638\n",
            "Sentiment ratio for this batch:  1.9999120235443115\n",
            "Sentiment ratio for this batch:  1.9999150037765503\n",
            "Sentiment ratio for this batch:  1.9999183416366577\n",
            "< test epoch > Batch: 270, Loss: 0.38590240478515625, Accuracy: 0.6266143911439115\n",
            "Sentiment ratio for this batch:  1.9999217987060547\n",
            "Sentiment ratio for this batch:  1.999923586845398\n",
            "Sentiment ratio for this batch:  1.9999265670776367\n",
            "Sentiment ratio for this batch:  1.9999291896820068\n",
            "Sentiment ratio for this batch:  1.9999312162399292\n",
            "Sentiment ratio for this batch:  1.9999338388442993\n",
            "Sentiment ratio for this batch:  1.9999366998672485\n",
            "Sentiment ratio for this batch:  1.9999399185180664\n",
            "Sentiment ratio for this batch:  1.9999425411224365\n",
            "Sentiment ratio for this batch:  1.9999452829360962\n",
            "< test epoch > Batch: 280, Loss: 0.46252086758613586, Accuracy: 0.6330071174377224\n",
            "Sentiment ratio for this batch:  1.9999473094940186\n",
            "Sentiment ratio for this batch:  1.99994957447052\n",
            "Sentiment ratio for this batch:  1.999951720237732\n",
            "Sentiment ratio for this batch:  1.9999542236328125\n",
            "Sentiment ratio for this batch:  1.9999548196792603\n",
            "Sentiment ratio for this batch:  1.9999526739120483\n",
            "Sentiment ratio for this batch:  1.999951958656311\n",
            "Sentiment ratio for this batch:  1.9999518394470215\n",
            "Sentiment ratio for this batch:  1.9999504089355469\n",
            "Sentiment ratio for this batch:  1.9999501705169678\n",
            "< test epoch > Batch: 290, Loss: 0.6018900275230408, Accuracy: 0.6372422680412371\n",
            "Sentiment ratio for this batch:  1.9999487400054932\n",
            "Sentiment ratio for this batch:  1.99994695186615\n",
            "Sentiment ratio for this batch:  1.9999446868896484\n",
            "Sentiment ratio for this batch:  1.9999415874481201\n",
            "Sentiment ratio for this batch:  1.9999388456344604\n",
            "Sentiment ratio for this batch:  1.9999386072158813\n",
            "Sentiment ratio for this batch:  1.999937653541565\n",
            "Sentiment ratio for this batch:  1.9999369382858276\n",
            "Sentiment ratio for this batch:  1.9999353885650635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 1/3 [04:18<08:37, 258.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 finished with average loss: 0.6188084517916044, Accuracy: 0.6404166666666666\n",
            "Sentiment ratio for this batch:  1.9999351501464844\n",
            "< test epoch > Batch: 0, Loss: 0.27216869592666626, Accuracy: 0.9375\n",
            "Sentiment ratio for this batch:  1.9999361038208008\n",
            "Sentiment ratio for this batch:  1.9999359846115112\n",
            "Sentiment ratio for this batch:  1.9999364614486694\n",
            "Sentiment ratio for this batch:  1.9999372959136963\n",
            "Sentiment ratio for this batch:  1.999937891960144\n",
            "Sentiment ratio for this batch:  1.999938726425171\n",
            "Sentiment ratio for this batch:  1.9999388456344604\n",
            "Sentiment ratio for this batch:  1.9999361038208008\n",
            "Sentiment ratio for this batch:  1.9999326467514038\n",
            "Sentiment ratio for this batch:  1.999929428100586\n",
            "< test epoch > Batch: 10, Loss: 0.412024587392807, Accuracy: 0.8181818181818182\n",
            "Sentiment ratio for this batch:  1.9999271631240845\n",
            "Sentiment ratio for this batch:  1.999924659729004\n",
            "Sentiment ratio for this batch:  1.999923825263977\n",
            "Sentiment ratio for this batch:  1.9999231100082397\n",
            "Sentiment ratio for this batch:  1.9999220371246338\n",
            "Sentiment ratio for this batch:  1.9999232292175293\n",
            "Sentiment ratio for this batch:  1.9999219179153442\n",
            "Sentiment ratio for this batch:  1.9999221563339233\n",
            "Sentiment ratio for this batch:  1.9999213218688965\n",
            "Sentiment ratio for this batch:  1.999918818473816\n",
            "< test epoch > Batch: 20, Loss: 0.3811834454536438, Accuracy: 0.7976190476190477\n",
            "Sentiment ratio for this batch:  1.9999167919158936\n",
            "Sentiment ratio for this batch:  1.9999171495437622\n",
            "Sentiment ratio for this batch:  1.9999173879623413\n",
            "Sentiment ratio for this batch:  1.9999171495437622\n",
            "Sentiment ratio for this batch:  1.999916672706604\n",
            "Sentiment ratio for this batch:  1.9999154806137085\n",
            "Sentiment ratio for this batch:  1.9999148845672607\n",
            "Sentiment ratio for this batch:  1.9999148845672607\n",
            "Sentiment ratio for this batch:  1.9999165534973145\n",
            "Sentiment ratio for this batch:  1.9999191761016846\n",
            "< test epoch > Batch: 30, Loss: 0.3637688159942627, Accuracy: 0.7963709677419355\n",
            "Sentiment ratio for this batch:  1.9999216794967651\n",
            "Sentiment ratio for this batch:  1.9999220371246338\n",
            "Sentiment ratio for this batch:  1.9999217987060547\n",
            "Sentiment ratio for this batch:  1.9999207258224487\n",
            "Sentiment ratio for this batch:  1.99991774559021\n",
            "Sentiment ratio for this batch:  1.9999146461486816\n",
            "Sentiment ratio for this batch:  1.9999126195907593\n",
            "Sentiment ratio for this batch:  1.9999102354049683\n",
            "Sentiment ratio for this batch:  1.9999057054519653\n",
            "Sentiment ratio for this batch:  1.9999021291732788\n",
            "< test epoch > Batch: 40, Loss: 0.6175029873847961, Accuracy: 0.7972560975609756\n",
            "Sentiment ratio for this batch:  1.9998981952667236\n",
            "Sentiment ratio for this batch:  1.999894142150879\n",
            "Sentiment ratio for this batch:  1.999890685081482\n",
            "Sentiment ratio for this batch:  1.9998846054077148\n",
            "Sentiment ratio for this batch:  1.999879002571106\n",
            "Sentiment ratio for this batch:  1.9998743534088135\n",
            "Sentiment ratio for this batch:  1.9998704195022583\n",
            "Sentiment ratio for this batch:  1.9998674392700195\n",
            "Sentiment ratio for this batch:  1.9998657703399658\n",
            "Sentiment ratio for this batch:  1.9998624324798584\n",
            "< test epoch > Batch: 50, Loss: 0.2813601493835449, Accuracy: 0.7941176470588235\n",
            "Sentiment ratio for this batch:  1.9998605251312256\n",
            "Sentiment ratio for this batch:  1.9998588562011719\n",
            "Sentiment ratio for this batch:  1.9998584985733032\n",
            "Sentiment ratio for this batch:  1.9998563528060913\n",
            "Sentiment ratio for this batch:  1.9998548030853271\n",
            "Sentiment ratio for this batch:  1.9998531341552734\n",
            "Sentiment ratio for this batch:  1.9998515844345093\n",
            "Sentiment ratio for this batch:  1.9998520612716675\n",
            "Sentiment ratio for this batch:  1.9998517036437988\n",
            "Sentiment ratio for this batch:  1.9998517036437988\n",
            "< test epoch > Batch: 60, Loss: 0.2771676182746887, Accuracy: 0.7858606557377049\n",
            "Sentiment ratio for this batch:  1.999852180480957\n",
            "Sentiment ratio for this batch:  1.9998526573181152\n",
            "Sentiment ratio for this batch:  1.9998527765274048\n",
            "Sentiment ratio for this batch:  1.9998512268066406\n",
            "Sentiment ratio for this batch:  1.999849796295166\n",
            "Sentiment ratio for this batch:  1.9998493194580078\n",
            "Sentiment ratio for this batch:  1.9998478889465332\n",
            "Sentiment ratio for this batch:  1.9998464584350586\n",
            "Sentiment ratio for this batch:  1.9998462200164795\n",
            "Sentiment ratio for this batch:  1.999846339225769\n",
            "< test epoch > Batch: 70, Loss: 0.3467978835105896, Accuracy: 0.7878521126760564\n",
            "Sentiment ratio for this batch:  1.9998455047607422\n",
            "Sentiment ratio for this batch:  1.9998434782028198\n",
            "Sentiment ratio for this batch:  1.9998409748077393\n",
            "Sentiment ratio for this batch:  1.99983811378479\n",
            "Sentiment ratio for this batch:  1.999837040901184\n",
            "Sentiment ratio for this batch:  1.999836802482605\n",
            "Sentiment ratio for this batch:  1.9998366832733154\n",
            "Sentiment ratio for this batch:  1.9998364448547363\n",
            "Sentiment ratio for this batch:  1.9998345375061035\n",
            "Sentiment ratio for this batch:  1.9998329877853394\n",
            "< test epoch > Batch: 80, Loss: 0.4912574887275696, Accuracy: 0.7893518518518519\n",
            "Sentiment ratio for this batch:  1.9998304843902588\n",
            "Sentiment ratio for this batch:  1.9998282194137573\n",
            "Sentiment ratio for this batch:  1.999826192855835\n",
            "Sentiment ratio for this batch:  1.9998241662979126\n",
            "Sentiment ratio for this batch:  1.9998220205307007\n",
            "Sentiment ratio for this batch:  1.9998211860656738\n",
            "Sentiment ratio for this batch:  1.9998201131820679\n",
            "Sentiment ratio for this batch:  1.9998198747634888\n",
            "Sentiment ratio for this batch:  1.9998197555541992\n",
            "Sentiment ratio for this batch:  1.999819040298462\n",
            "< test epoch > Batch: 90, Loss: 0.49659404158592224, Accuracy: 0.7912087912087912\n",
            "Sentiment ratio for this batch:  1.999818205833435\n",
            "Sentiment ratio for this batch:  1.9998164176940918\n",
            "Sentiment ratio for this batch:  1.9998148679733276\n",
            "Sentiment ratio for this batch:  1.9998136758804321\n",
            "Sentiment ratio for this batch:  1.999813437461853\n",
            "Sentiment ratio for this batch:  1.9998129606246948\n",
            "Sentiment ratio for this batch:  1.999812364578247\n",
            "Sentiment ratio for this batch:  1.9998105764389038\n",
            "Sentiment ratio for this batch:  1.99980890750885\n",
            "Sentiment ratio for this batch:  1.9998067617416382\n",
            "< test epoch > Batch: 100, Loss: 0.41416266560554504, Accuracy: 0.7902227722772277\n",
            "Sentiment ratio for this batch:  1.9998016357421875\n",
            "Sentiment ratio for this batch:  1.9997986555099487\n",
            "Sentiment ratio for this batch:  1.999795913696289\n",
            "Sentiment ratio for this batch:  1.9997929334640503\n",
            "Sentiment ratio for this batch:  1.9997878074645996\n",
            "Sentiment ratio for this batch:  1.9997831583023071\n",
            "Sentiment ratio for this batch:  1.9997795820236206\n",
            "Sentiment ratio for this batch:  1.9997769594192505\n",
            "Sentiment ratio for this batch:  1.9997732639312744\n",
            "Sentiment ratio for this batch:  1.999770164489746\n",
            "< test epoch > Batch: 110, Loss: 0.25524505972862244, Accuracy: 0.7944819819819819\n",
            "Sentiment ratio for this batch:  1.999767780303955\n",
            "Sentiment ratio for this batch:  1.9997652769088745\n",
            "Sentiment ratio for this batch:  1.9997636079788208\n",
            "Sentiment ratio for this batch:  1.9997626543045044\n",
            "Sentiment ratio for this batch:  1.999759316444397\n",
            "Sentiment ratio for this batch:  1.9997574090957642\n",
            "Sentiment ratio for this batch:  1.9997570514678955\n",
            "Sentiment ratio for this batch:  1.9997566938400269\n",
            "Sentiment ratio for this batch:  1.9997563362121582\n",
            "Sentiment ratio for this batch:  1.999756097793579\n",
            "< test epoch > Batch: 120, Loss: 0.36174502968788147, Accuracy: 0.8006198347107438\n",
            "Sentiment ratio for this batch:  1.9997563362121582\n",
            "Sentiment ratio for this batch:  1.9997564554214478\n",
            "Sentiment ratio for this batch:  1.9997557401657104\n",
            "Sentiment ratio for this batch:  1.999755859375\n",
            "Sentiment ratio for this batch:  1.9997553825378418\n",
            "Sentiment ratio for this batch:  1.9997543096542358\n",
            "Sentiment ratio for this batch:  1.9997532367706299\n",
            "Sentiment ratio for this batch:  1.9997516870498657\n",
            "Sentiment ratio for this batch:  1.9997506141662598\n",
            "Sentiment ratio for this batch:  1.9997485876083374\n",
            "< test epoch > Batch: 130, Loss: 0.28515347838401794, Accuracy: 0.7986641221374046\n",
            "Sentiment ratio for this batch:  1.999746561050415\n",
            "Sentiment ratio for this batch:  1.9997440576553345\n",
            "Sentiment ratio for this batch:  1.999741554260254\n",
            "Sentiment ratio for this batch:  1.9997390508651733\n",
            "Sentiment ratio for this batch:  1.9997366666793823\n",
            "Sentiment ratio for this batch:  1.99973464012146\n",
            "Sentiment ratio for this batch:  1.9997326135635376\n",
            "Sentiment ratio for this batch:  1.9997305870056152\n",
            "Sentiment ratio for this batch:  1.9997284412384033\n",
            "Sentiment ratio for this batch:  1.99972665309906\n",
            "< test epoch > Batch: 140, Loss: 0.38116565346717834, Accuracy: 0.8027482269503546\n",
            "Sentiment ratio for this batch:  1.9997254610061646\n",
            "Sentiment ratio for this batch:  1.9997228384017944\n",
            "Sentiment ratio for this batch:  1.9997212886810303\n",
            "Sentiment ratio for this batch:  1.9997204542160034\n",
            "Sentiment ratio for this batch:  1.9997197389602661\n",
            "Sentiment ratio for this batch:  1.9997191429138184\n",
            "Sentiment ratio for this batch:  1.9997179508209229\n",
            "Sentiment ratio for this batch:  1.9997166395187378\n",
            "Sentiment ratio for this batch:  1.999714970588684\n",
            "Sentiment ratio for this batch:  1.9997133016586304\n",
            "< test epoch > Batch: 150, Loss: 0.27686870098114014, Accuracy: 0.8038079470198676\n",
            "Sentiment ratio for this batch:  1.999711513519287\n",
            "Sentiment ratio for this batch:  1.999710202217102\n",
            "Sentiment ratio for this batch:  1.999708652496338\n",
            "Sentiment ratio for this batch:  1.9997069835662842\n",
            "Sentiment ratio for this batch:  1.9997050762176514\n",
            "Sentiment ratio for this batch:  1.999704360961914\n",
            "Sentiment ratio for this batch:  1.9997037649154663\n",
            "Sentiment ratio for this batch:  1.9997023344039917\n",
            "Sentiment ratio for this batch:  1.999700903892517\n",
            "Sentiment ratio for this batch:  1.9996994733810425\n",
            "< test epoch > Batch: 160, Loss: 0.36341768503189087, Accuracy: 0.8059006211180124\n",
            "Sentiment ratio for this batch:  1.999696969985962\n",
            "Sentiment ratio for this batch:  1.9996939897537231\n",
            "Sentiment ratio for this batch:  1.9996905326843262\n",
            "Sentiment ratio for this batch:  1.9996880292892456\n",
            "Sentiment ratio for this batch:  1.9996858835220337\n",
            "Sentiment ratio for this batch:  1.9996834993362427\n",
            "Sentiment ratio for this batch:  1.9996811151504517\n",
            "Sentiment ratio for this batch:  1.9996798038482666\n",
            "Sentiment ratio for this batch:  1.9996788501739502\n",
            "Sentiment ratio for this batch:  1.999678611755371\n",
            "< test epoch > Batch: 170, Loss: 0.3154250979423523, Accuracy: 0.8099415204678363\n",
            "Sentiment ratio for this batch:  1.999678134918213\n",
            "Sentiment ratio for this batch:  1.9996778964996338\n",
            "Sentiment ratio for this batch:  1.9996777772903442\n",
            "Sentiment ratio for this batch:  1.999678373336792\n",
            "Sentiment ratio for this batch:  1.9996788501739502\n",
            "Sentiment ratio for this batch:  1.9996792078018188\n",
            "Sentiment ratio for this batch:  1.9996782541275024\n",
            "Sentiment ratio for this batch:  1.9996758699417114\n",
            "Sentiment ratio for this batch:  1.9996731281280518\n",
            "Sentiment ratio for this batch:  1.9996716976165771\n",
            "< test epoch > Batch: 180, Loss: 0.6959319710731506, Accuracy: 0.8104281767955801\n",
            "Sentiment ratio for this batch:  1.999670147895813\n",
            "Sentiment ratio for this batch:  1.9996687173843384\n",
            "Sentiment ratio for this batch:  1.9996676445007324\n",
            "Sentiment ratio for this batch:  1.9996646642684937\n",
            "Sentiment ratio for this batch:  1.9996622800827026\n",
            "Sentiment ratio for this batch:  1.9996600151062012\n",
            "Sentiment ratio for this batch:  1.9996578693389893\n",
            "Sentiment ratio for this batch:  1.9996559619903564\n",
            "Sentiment ratio for this batch:  1.9996545314788818\n",
            "Sentiment ratio for this batch:  1.999653697013855\n",
            "< test epoch > Batch: 190, Loss: 0.23848430812358856, Accuracy: 0.8105366492146597\n",
            "Sentiment ratio for this batch:  1.9996527433395386\n",
            "Sentiment ratio for this batch:  1.9996525049209595\n",
            "Sentiment ratio for this batch:  1.9996521472930908\n",
            "Sentiment ratio for this batch:  1.9996509552001953\n",
            "Sentiment ratio for this batch:  1.999650001525879\n",
            "Sentiment ratio for this batch:  1.999647855758667\n",
            "Sentiment ratio for this batch:  1.999646544456482\n",
            "Sentiment ratio for this batch:  1.9996455907821655\n",
            "Sentiment ratio for this batch:  1.9996451139450073\n",
            "Sentiment ratio for this batch:  1.9996446371078491\n",
            "< test epoch > Batch: 200, Loss: 0.4003450572490692, Accuracy: 0.8090796019900498\n",
            "Sentiment ratio for this batch:  1.9996442794799805\n",
            "Sentiment ratio for this batch:  1.9996439218521118\n",
            "Sentiment ratio for this batch:  1.9996432065963745\n",
            "Sentiment ratio for this batch:  1.999642252922058\n",
            "Sentiment ratio for this batch:  1.9996416568756104\n",
            "Sentiment ratio for this batch:  1.999640703201294\n",
            "Sentiment ratio for this batch:  1.9996393918991089\n",
            "Sentiment ratio for this batch:  1.9996381998062134\n",
            "Sentiment ratio for this batch:  1.9996371269226074\n",
            "Sentiment ratio for this batch:  1.9996360540390015\n",
            "< test epoch > Batch: 210, Loss: 0.3399668037891388, Accuracy: 0.8104265402843602\n",
            "Sentiment ratio for this batch:  1.9996352195739746\n",
            "Sentiment ratio for this batch:  1.9996328353881836\n",
            "Sentiment ratio for this batch:  1.9996311664581299\n",
            "Sentiment ratio for this batch:  1.9996293783187866\n",
            "Sentiment ratio for this batch:  1.9996283054351807\n",
            "Sentiment ratio for this batch:  1.9996271133422852\n",
            "Sentiment ratio for this batch:  1.999626636505127\n",
            "Sentiment ratio for this batch:  1.9996260404586792\n",
            "Sentiment ratio for this batch:  1.9996250867843628\n",
            "Sentiment ratio for this batch:  1.999624252319336\n",
            "< test epoch > Batch: 220, Loss: 0.15825864672660828, Accuracy: 0.8133484162895928\n",
            "Sentiment ratio for this batch:  1.999623417854309\n",
            "Sentiment ratio for this batch:  1.9996228218078613\n",
            "Sentiment ratio for this batch:  1.999622106552124\n",
            "Sentiment ratio for this batch:  1.9996209144592285\n",
            "Sentiment ratio for this batch:  1.9996198415756226\n",
            "Sentiment ratio for this batch:  1.9996193647384644\n",
            "Sentiment ratio for this batch:  1.9996191263198853\n",
            "Sentiment ratio for this batch:  1.9996191263198853\n",
            "Sentiment ratio for this batch:  1.9996188879013062\n",
            "Sentiment ratio for this batch:  1.9996187686920166\n",
            "< test epoch > Batch: 230, Loss: 0.30886587500572205, Accuracy: 0.8162878787878788\n",
            "Sentiment ratio for this batch:  1.9996180534362793\n",
            "Sentiment ratio for this batch:  1.9996180534362793\n",
            "Sentiment ratio for this batch:  1.9996185302734375\n",
            "Sentiment ratio for this batch:  1.999618649482727\n",
            "Sentiment ratio for this batch:  1.999618649482727\n",
            "Sentiment ratio for this batch:  1.9996176958084106\n",
            "Sentiment ratio for this batch:  1.999617099761963\n",
            "Sentiment ratio for this batch:  1.9996166229248047\n",
            "Sentiment ratio for this batch:  1.9996159076690674\n",
            "Sentiment ratio for this batch:  1.9996140003204346\n",
            "< test epoch > Batch: 240, Loss: 0.2651178240776062, Accuracy: 0.816908713692946\n",
            "Sentiment ratio for this batch:  1.9996119737625122\n",
            "Sentiment ratio for this batch:  1.9996099472045898\n",
            "Sentiment ratio for this batch:  1.9996083974838257\n",
            "Sentiment ratio for this batch:  1.9996070861816406\n",
            "Sentiment ratio for this batch:  1.9996060132980347\n",
            "Sentiment ratio for this batch:  1.9996047019958496\n",
            "Sentiment ratio for this batch:  1.9996031522750854\n",
            "Sentiment ratio for this batch:  1.9996017217636108\n",
            "Sentiment ratio for this batch:  1.9996004104614258\n",
            "Sentiment ratio for this batch:  1.9995993375778198\n",
            "< test epoch > Batch: 250, Loss: 0.34705886244773865, Accuracy: 0.8184760956175299\n",
            "Sentiment ratio for this batch:  1.9995980262756348\n",
            "Sentiment ratio for this batch:  1.9995957612991333\n",
            "Sentiment ratio for this batch:  1.9995936155319214\n",
            "Sentiment ratio for this batch:  1.9995911121368408\n",
            "Sentiment ratio for this batch:  1.9995883703231812\n",
            "Sentiment ratio for this batch:  1.9995877742767334\n",
            "Sentiment ratio for this batch:  1.999587059020996\n",
            "Sentiment ratio for this batch:  1.9995863437652588\n",
            "Sentiment ratio for this batch:  1.9995852708816528\n",
            "Sentiment ratio for this batch:  1.9995849132537842\n",
            "< test epoch > Batch: 260, Loss: 0.3034569025039673, Accuracy: 0.819683908045977\n",
            "Sentiment ratio for this batch:  1.9995827674865723\n",
            "Sentiment ratio for this batch:  1.9995816946029663\n",
            "Sentiment ratio for this batch:  1.999581217765808\n",
            "Sentiment ratio for this batch:  1.9995814561843872\n",
            "Sentiment ratio for this batch:  1.999579668045044\n",
            "Sentiment ratio for this batch:  1.9995783567428589\n",
            "Sentiment ratio for this batch:  1.9995774030685425\n",
            "Sentiment ratio for this batch:  1.9995763301849365\n",
            "Sentiment ratio for this batch:  1.9995753765106201\n",
            "Sentiment ratio for this batch:  1.999574899673462\n",
            "< test epoch > Batch: 270, Loss: 0.24759094417095184, Accuracy: 0.8208025830258303\n",
            "Sentiment ratio for this batch:  1.9995758533477783\n",
            "Sentiment ratio for this batch:  1.9995759725570679\n",
            "Sentiment ratio for this batch:  1.9995763301849365\n",
            "Sentiment ratio for this batch:  1.9995765686035156\n",
            "Sentiment ratio for this batch:  1.999576449394226\n",
            "Sentiment ratio for this batch:  1.9995765686035156\n",
            "Sentiment ratio for this batch:  1.999577283859253\n",
            "Sentiment ratio for this batch:  1.9995777606964111\n",
            "Sentiment ratio for this batch:  1.999578833580017\n",
            "Sentiment ratio for this batch:  1.999579906463623\n",
            "< test epoch > Batch: 280, Loss: 0.3103797137737274, Accuracy: 0.8222864768683275\n",
            "Sentiment ratio for this batch:  1.999581217765808\n",
            "Sentiment ratio for this batch:  1.999583125114441\n",
            "Sentiment ratio for this batch:  1.999584674835205\n",
            "Sentiment ratio for this batch:  1.9995859861373901\n",
            "Sentiment ratio for this batch:  1.999586582183838\n",
            "Sentiment ratio for this batch:  1.999586820602417\n",
            "Sentiment ratio for this batch:  1.9995875358581543\n",
            "Sentiment ratio for this batch:  1.9995883703231812\n",
            "Sentiment ratio for this batch:  1.999589443206787\n",
            "Sentiment ratio for this batch:  1.9995901584625244\n",
            "< test epoch > Batch: 290, Loss: 0.40959909558296204, Accuracy: 0.8219501718213058\n",
            "Sentiment ratio for this batch:  1.9995919466018677\n",
            "Sentiment ratio for this batch:  1.9995936155319214\n",
            "Sentiment ratio for this batch:  1.9995943307876587\n",
            "Sentiment ratio for this batch:  1.9995942115783691\n",
            "Sentiment ratio for this batch:  1.9995942115783691\n",
            "Sentiment ratio for this batch:  1.9995930194854736\n",
            "Sentiment ratio for this batch:  1.9995924234390259\n",
            "Sentiment ratio for this batch:  1.9995925426483154\n",
            "Sentiment ratio for this batch:  1.9995911121368408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [08:35<04:17, 257.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 finished with average loss: 0.40124660710493726, Accuracy: 0.8216666666666667\n",
            "Sentiment ratio for this batch:  1.9995895624160767\n",
            "< test epoch > Batch: 0, Loss: 0.2055882215499878, Accuracy: 0.8125\n",
            "Sentiment ratio for this batch:  1.9995893239974976\n",
            "Sentiment ratio for this batch:  1.999588966369629\n",
            "Sentiment ratio for this batch:  1.999589204788208\n",
            "Sentiment ratio for this batch:  1.9995893239974976\n",
            "Sentiment ratio for this batch:  1.9995900392532349\n",
            "Sentiment ratio for this batch:  1.9995901584625244\n",
            "Sentiment ratio for this batch:  1.9995900392532349\n",
            "Sentiment ratio for this batch:  1.9995898008346558\n",
            "Sentiment ratio for this batch:  1.9995895624160767\n",
            "Sentiment ratio for this batch:  1.9995882511138916\n",
            "< test epoch > Batch: 10, Loss: 0.2259216606616974, Accuracy: 0.8863636363636364\n",
            "Sentiment ratio for this batch:  1.9995872974395752\n",
            "Sentiment ratio for this batch:  1.9995856285095215\n",
            "Sentiment ratio for this batch:  1.9995851516723633\n",
            "Sentiment ratio for this batch:  1.9995838403701782\n",
            "Sentiment ratio for this batch:  1.9995832443237305\n",
            "Sentiment ratio for this batch:  1.999583125114441\n",
            "Sentiment ratio for this batch:  1.9995828866958618\n",
            "Sentiment ratio for this batch:  1.9995827674865723\n",
            "Sentiment ratio for this batch:  1.9995808601379395\n",
            "Sentiment ratio for this batch:  1.9995784759521484\n",
            "< test epoch > Batch: 20, Loss: 0.249687060713768, Accuracy: 0.875\n",
            "Sentiment ratio for this batch:  1.9995766878128052\n",
            "Sentiment ratio for this batch:  1.9995759725570679\n",
            "Sentiment ratio for this batch:  1.9995743036270142\n",
            "Sentiment ratio for this batch:  1.9995704889297485\n",
            "Sentiment ratio for this batch:  1.9995670318603516\n",
            "Sentiment ratio for this batch:  1.99956476688385\n",
            "Sentiment ratio for this batch:  1.9995626211166382\n",
            "Sentiment ratio for this batch:  1.9995633363723755\n",
            "Sentiment ratio for this batch:  1.9995638132095337\n",
            "Sentiment ratio for this batch:  1.999564290046692\n",
            "< test epoch > Batch: 30, Loss: 0.20545905828475952, Accuracy: 0.8629032258064516\n",
            "Sentiment ratio for this batch:  1.9995648860931396\n",
            "Sentiment ratio for this batch:  1.9995654821395874\n",
            "Sentiment ratio for this batch:  1.9995661973953247\n",
            "Sentiment ratio for this batch:  1.9995665550231934\n",
            "Sentiment ratio for this batch:  1.9995664358139038\n",
            "Sentiment ratio for this batch:  1.9995663166046143\n",
            "Sentiment ratio for this batch:  1.9995663166046143\n",
            "Sentiment ratio for this batch:  1.9995663166046143\n",
            "Sentiment ratio for this batch:  1.9995648860931396\n",
            "Sentiment ratio for this batch:  1.9995630979537964\n",
            "< test epoch > Batch: 40, Loss: 0.7073137164115906, Accuracy: 0.8521341463414634\n",
            "Sentiment ratio for this batch:  1.9995611906051636\n",
            "Sentiment ratio for this batch:  1.999558925628662\n",
            "Sentiment ratio for this batch:  1.9995571374893188\n",
            "Sentiment ratio for this batch:  1.999553918838501\n",
            "Sentiment ratio for this batch:  1.9995510578155518\n",
            "Sentiment ratio for this batch:  1.9995487928390503\n",
            "Sentiment ratio for this batch:  1.9995468854904175\n",
            "Sentiment ratio for this batch:  1.9995453357696533\n",
            "Sentiment ratio for this batch:  1.9995441436767578\n",
            "Sentiment ratio for this batch:  1.9995416402816772\n",
            "< test epoch > Batch: 50, Loss: 0.1605304777622223, Accuracy: 0.8443627450980392\n",
            "Sentiment ratio for this batch:  1.9995399713516235\n",
            "Sentiment ratio for this batch:  1.9995381832122803\n",
            "Sentiment ratio for this batch:  1.9995368719100952\n",
            "Sentiment ratio for this batch:  1.9995347261428833\n",
            "Sentiment ratio for this batch:  1.99953293800354\n",
            "Sentiment ratio for this batch:  1.999531626701355\n",
            "Sentiment ratio for this batch:  1.9995301961898804\n",
            "Sentiment ratio for this batch:  1.9995291233062744\n",
            "Sentiment ratio for this batch:  1.9995286464691162\n",
            "Sentiment ratio for this batch:  1.999528408050537\n",
            "< test epoch > Batch: 60, Loss: 0.20003828406333923, Accuracy: 0.8432377049180327\n",
            "Sentiment ratio for this batch:  1.9995286464691162\n",
            "Sentiment ratio for this batch:  1.999528408050537\n",
            "Sentiment ratio for this batch:  1.9995274543762207\n",
            "Sentiment ratio for this batch:  1.9995262622833252\n",
            "Sentiment ratio for this batch:  1.9995249509811401\n",
            "Sentiment ratio for this batch:  1.9995242357254028\n",
            "Sentiment ratio for this batch:  1.9995228052139282\n",
            "Sentiment ratio for this batch:  1.9995208978652954\n",
            "Sentiment ratio for this batch:  1.999518871307373\n",
            "Sentiment ratio for this batch:  1.9995168447494507\n",
            "< test epoch > Batch: 70, Loss: 0.2507821023464203, Accuracy: 0.8450704225352113\n",
            "Sentiment ratio for this batch:  1.9995148181915283\n",
            "Sentiment ratio for this batch:  1.9995129108428955\n",
            "Sentiment ratio for this batch:  1.99951171875\n",
            "Sentiment ratio for this batch:  1.9995101690292358\n",
            "Sentiment ratio for this batch:  1.999509334564209\n",
            "Sentiment ratio for this batch:  1.9995092153549194\n",
            "Sentiment ratio for this batch:  1.999509334564209\n",
            "Sentiment ratio for this batch:  1.9995087385177612\n",
            "Sentiment ratio for this batch:  1.9995081424713135\n",
            "Sentiment ratio for this batch:  1.9995075464248657\n",
            "< test epoch > Batch: 80, Loss: 0.223555326461792, Accuracy: 0.8487654320987654\n",
            "Sentiment ratio for this batch:  1.999507188796997\n",
            "Sentiment ratio for this batch:  1.9995062351226807\n",
            "Sentiment ratio for this batch:  1.9995054006576538\n",
            "Sentiment ratio for this batch:  1.9995043277740479\n",
            "Sentiment ratio for this batch:  1.9995025396347046\n",
            "Sentiment ratio for this batch:  1.999501347541809\n",
            "Sentiment ratio for this batch:  1.9995009899139404\n",
            "Sentiment ratio for this batch:  1.9995009899139404\n",
            "Sentiment ratio for this batch:  1.9995002746582031\n",
            "Sentiment ratio for this batch:  1.9994992017745972\n",
            "< test epoch > Batch: 90, Loss: 0.4174514412879944, Accuracy: 0.8495879120879121\n",
            "Sentiment ratio for this batch:  1.9994983673095703\n",
            "Sentiment ratio for this batch:  1.999497890472412\n",
            "Sentiment ratio for this batch:  1.9994975328445435\n",
            "Sentiment ratio for this batch:  1.9994968175888062\n",
            "Sentiment ratio for this batch:  1.9994962215423584\n",
            "Sentiment ratio for this batch:  1.9994956254959106\n",
            "Sentiment ratio for this batch:  1.999495506286621\n",
            "Sentiment ratio for this batch:  1.9994946718215942\n",
            "Sentiment ratio for this batch:  1.9994940757751465\n",
            "Sentiment ratio for this batch:  1.99949312210083\n",
            "< test epoch > Batch: 100, Loss: 0.32116779685020447, Accuracy: 0.8452970297029703\n",
            "Sentiment ratio for this batch:  1.9994895458221436\n",
            "Sentiment ratio for this batch:  1.9994854927062988\n",
            "Sentiment ratio for this batch:  1.9994810819625854\n",
            "Sentiment ratio for this batch:  1.9994770288467407\n",
            "Sentiment ratio for this batch:  1.999473214149475\n",
            "Sentiment ratio for this batch:  1.9994701147079468\n",
            "Sentiment ratio for this batch:  1.999467372894287\n",
            "Sentiment ratio for this batch:  1.9994653463363647\n",
            "Sentiment ratio for this batch:  1.999462366104126\n",
            "Sentiment ratio for this batch:  1.9994597434997559\n",
            "< test epoch > Batch: 110, Loss: 0.22361677885055542, Accuracy: 0.847972972972973\n",
            "Sentiment ratio for this batch:  1.9994577169418335\n",
            "Sentiment ratio for this batch:  1.9994559288024902\n",
            "Sentiment ratio for this batch:  1.999454140663147\n",
            "Sentiment ratio for this batch:  1.9994536638259888\n",
            "Sentiment ratio for this batch:  1.9994524717330933\n",
            "Sentiment ratio for this batch:  1.9994513988494873\n",
            "Sentiment ratio for this batch:  1.9994521141052246\n",
            "Sentiment ratio for this batch:  1.9994529485702515\n",
            "Sentiment ratio for this batch:  1.9994536638259888\n",
            "Sentiment ratio for this batch:  1.999454140663147\n",
            "< test epoch > Batch: 120, Loss: 0.3115137815475464, Accuracy: 0.8512396694214877\n",
            "Sentiment ratio for this batch:  1.9994542598724365\n",
            "Sentiment ratio for this batch:  1.9994544982910156\n",
            "Sentiment ratio for this batch:  1.9994535446166992\n",
            "Sentiment ratio for this batch:  1.9994523525238037\n",
            "Sentiment ratio for this batch:  1.99945068359375\n",
            "Sentiment ratio for this batch:  1.9994471073150635\n",
            "Sentiment ratio for this batch:  1.9994432926177979\n",
            "Sentiment ratio for this batch:  1.9994404315948486\n",
            "Sentiment ratio for this batch:  1.9994380474090576\n",
            "Sentiment ratio for this batch:  1.9994357824325562\n",
            "< test epoch > Batch: 130, Loss: 0.12760166823863983, Accuracy: 0.8506679389312977\n",
            "Sentiment ratio for this batch:  1.9994335174560547\n",
            "Sentiment ratio for this batch:  1.9994308948516846\n",
            "Sentiment ratio for this batch:  1.9994291067123413\n",
            "Sentiment ratio for this batch:  1.9994267225265503\n",
            "Sentiment ratio for this batch:  1.9994239807128906\n",
            "Sentiment ratio for this batch:  1.9994213581085205\n",
            "Sentiment ratio for this batch:  1.9994170665740967\n",
            "Sentiment ratio for this batch:  1.9994133710861206\n",
            "Sentiment ratio for this batch:  1.999409794807434\n",
            "Sentiment ratio for this batch:  1.9994064569473267\n",
            "< test epoch > Batch: 140, Loss: 0.2639448344707489, Accuracy: 0.8537234042553191\n",
            "Sentiment ratio for this batch:  1.999403715133667\n",
            "Sentiment ratio for this batch:  1.9994003772735596\n",
            "Sentiment ratio for this batch:  1.9993979930877686\n",
            "Sentiment ratio for this batch:  1.9993964433670044\n",
            "Sentiment ratio for this batch:  1.9993953704833984\n",
            "Sentiment ratio for this batch:  1.999394416809082\n",
            "Sentiment ratio for this batch:  1.9993934631347656\n",
            "Sentiment ratio for this batch:  1.9993926286697388\n",
            "Sentiment ratio for this batch:  1.999391794204712\n",
            "Sentiment ratio for this batch:  1.9993911981582642\n",
            "< test epoch > Batch: 150, Loss: 0.21371592581272125, Accuracy: 0.8559602649006622\n",
            "Sentiment ratio for this batch:  1.9993908405303955\n",
            "Sentiment ratio for this batch:  1.9993902444839478\n",
            "Sentiment ratio for this batch:  1.9993889331817627\n",
            "Sentiment ratio for this batch:  1.9993878602981567\n",
            "Sentiment ratio for this batch:  1.9993869066238403\n",
            "Sentiment ratio for this batch:  1.9993865489959717\n",
            "Sentiment ratio for this batch:  1.999385952949524\n",
            "Sentiment ratio for this batch:  1.9993846416473389\n",
            "Sentiment ratio for this batch:  1.9993833303451538\n",
            "Sentiment ratio for this batch:  1.999381184577942\n",
            "< test epoch > Batch: 160, Loss: 0.25851988792419434, Accuracy: 0.8555900621118012\n",
            "Sentiment ratio for this batch:  1.9993780851364136\n",
            "Sentiment ratio for this batch:  1.9993747472763062\n",
            "Sentiment ratio for this batch:  1.9993714094161987\n",
            "Sentiment ratio for this batch:  1.9993687868118286\n",
            "Sentiment ratio for this batch:  1.999366283416748\n",
            "Sentiment ratio for this batch:  1.999363660812378\n",
            "Sentiment ratio for this batch:  1.9993611574172974\n",
            "Sentiment ratio for this batch:  1.9993586540222168\n",
            "Sentiment ratio for this batch:  1.999356746673584\n",
            "Sentiment ratio for this batch:  1.9993560314178467\n",
            "< test epoch > Batch: 170, Loss: 0.23180131614208221, Accuracy: 0.8578216374269005\n",
            "Sentiment ratio for this batch:  1.9993551969528198\n",
            "Sentiment ratio for this batch:  1.999354600906372\n",
            "Sentiment ratio for this batch:  1.9993541240692139\n",
            "Sentiment ratio for this batch:  1.9993536472320557\n",
            "Sentiment ratio for this batch:  1.9993535280227661\n",
            "Sentiment ratio for this batch:  1.9993534088134766\n",
            "Sentiment ratio for this batch:  1.999352216720581\n",
            "Sentiment ratio for this batch:  1.9993503093719482\n",
            "Sentiment ratio for this batch:  1.9993489980697632\n",
            "Sentiment ratio for this batch:  1.9993467330932617\n",
            "< test epoch > Batch: 180, Loss: 0.6059654951095581, Accuracy: 0.8594613259668509\n",
            "Sentiment ratio for this batch:  1.9993451833724976\n",
            "Sentiment ratio for this batch:  1.9993441104888916\n",
            "Sentiment ratio for this batch:  1.9993431568145752\n",
            "Sentiment ratio for this batch:  1.9993406534194946\n",
            "Sentiment ratio for this batch:  1.9993385076522827\n",
            "Sentiment ratio for this batch:  1.99933660030365\n",
            "Sentiment ratio for this batch:  1.9993352890014648\n",
            "Sentiment ratio for this batch:  1.9993342161178589\n",
            "Sentiment ratio for this batch:  1.9993338584899902\n",
            "Sentiment ratio for this batch:  1.999333381652832\n",
            "< test epoch > Batch: 190, Loss: 0.14377963542938232, Accuracy: 0.8599476439790575\n",
            "Sentiment ratio for this batch:  1.9993332624435425\n",
            "Sentiment ratio for this batch:  1.9993348121643066\n",
            "Sentiment ratio for this batch:  1.9993364810943604\n",
            "Sentiment ratio for this batch:  1.999336838722229\n",
            "Sentiment ratio for this batch:  1.9993361234664917\n",
            "Sentiment ratio for this batch:  1.9993343353271484\n",
            "Sentiment ratio for this batch:  1.9993327856063843\n",
            "Sentiment ratio for this batch:  1.9993312358856201\n",
            "Sentiment ratio for this batch:  1.9993298053741455\n",
            "Sentiment ratio for this batch:  1.9993289709091187\n",
            "< test epoch > Batch: 200, Loss: 0.28918033838272095, Accuracy: 0.8597636815920398\n",
            "Sentiment ratio for this batch:  1.999328374862671\n",
            "Sentiment ratio for this batch:  1.9993280172348022\n",
            "Sentiment ratio for this batch:  1.9993305206298828\n",
            "Sentiment ratio for this batch:  1.9993325471878052\n",
            "Sentiment ratio for this batch:  1.9993338584899902\n",
            "Sentiment ratio for this batch:  1.9993349313735962\n",
            "Sentiment ratio for this batch:  1.9993354082107544\n",
            "Sentiment ratio for this batch:  1.9993358850479126\n",
            "Sentiment ratio for this batch:  1.99933660030365\n",
            "Sentiment ratio for this batch:  1.9993376731872559\n",
            "< test epoch > Batch: 210, Loss: 0.2515028119087219, Accuracy: 0.8595971563981043\n",
            "Sentiment ratio for this batch:  1.9993382692337036\n",
            "Sentiment ratio for this batch:  1.9993377923965454\n",
            "Sentiment ratio for this batch:  1.9993377923965454\n",
            "Sentiment ratio for this batch:  1.9993377923965454\n",
            "Sentiment ratio for this batch:  1.9993380308151245\n",
            "Sentiment ratio for this batch:  1.999338150024414\n",
            "Sentiment ratio for this batch:  1.9993377923965454\n",
            "Sentiment ratio for this batch:  1.999337911605835\n",
            "Sentiment ratio for this batch:  1.9993377923965454\n",
            "Sentiment ratio for this batch:  1.9993377923965454\n",
            "< test epoch > Batch: 220, Loss: 0.09689836204051971, Accuracy: 0.8628393665158371\n",
            "Sentiment ratio for this batch:  1.9993376731872559\n",
            "Sentiment ratio for this batch:  1.9993382692337036\n",
            "Sentiment ratio for this batch:  1.9993391036987305\n",
            "Sentiment ratio for this batch:  1.99933922290802\n",
            "Sentiment ratio for this batch:  1.9993391036987305\n",
            "Sentiment ratio for this batch:  1.9993391036987305\n",
            "Sentiment ratio for this batch:  1.999338984489441\n",
            "Sentiment ratio for this batch:  1.999338984489441\n",
            "Sentiment ratio for this batch:  1.9993388652801514\n",
            "Sentiment ratio for this batch:  1.9993386268615723\n",
            "< test epoch > Batch: 230, Loss: 0.16707512736320496, Accuracy: 0.8660714285714286\n",
            "Sentiment ratio for this batch:  1.999337911605835\n",
            "Sentiment ratio for this batch:  1.9993374347686768\n",
            "Sentiment ratio for this batch:  1.999337077140808\n",
            "Sentiment ratio for this batch:  1.9993371963500977\n",
            "Sentiment ratio for this batch:  1.9993374347686768\n",
            "Sentiment ratio for this batch:  1.999337077140808\n",
            "Sentiment ratio for this batch:  1.99933660030365\n",
            "Sentiment ratio for this batch:  1.9993358850479126\n",
            "Sentiment ratio for this batch:  1.9993350505828857\n",
            "Sentiment ratio for this batch:  1.9993342161178589\n",
            "< test epoch > Batch: 240, Loss: 0.24392098188400269, Accuracy: 0.8674792531120332\n",
            "Sentiment ratio for this batch:  1.9993337392807007\n",
            "Sentiment ratio for this batch:  1.9993332624435425\n",
            "Sentiment ratio for this batch:  1.9993337392807007\n",
            "Sentiment ratio for this batch:  1.9993343353271484\n",
            "Sentiment ratio for this batch:  1.999334692955017\n",
            "Sentiment ratio for this batch:  1.9993351697921753\n",
            "Sentiment ratio for this batch:  1.9993351697921753\n",
            "Sentiment ratio for this batch:  1.9993350505828857\n",
            "Sentiment ratio for this batch:  1.9993358850479126\n",
            "Sentiment ratio for this batch:  1.9993364810943604\n",
            "< test epoch > Batch: 250, Loss: 0.30486249923706055, Accuracy: 0.8685258964143426\n",
            "Sentiment ratio for this batch:  1.9993360042572021\n",
            "Sentiment ratio for this batch:  1.999335527420044\n",
            "Sentiment ratio for this batch:  1.9993352890014648\n",
            "Sentiment ratio for this batch:  1.999334454536438\n",
            "Sentiment ratio for this batch:  1.9993342161178589\n",
            "Sentiment ratio for this batch:  1.999334454536438\n",
            "Sentiment ratio for this batch:  1.9993342161178589\n",
            "Sentiment ratio for this batch:  1.9993342161178589\n",
            "Sentiment ratio for this batch:  1.9993343353271484\n",
            "Sentiment ratio for this batch:  1.9993348121643066\n",
            "< test epoch > Batch: 260, Loss: 0.14683666825294495, Accuracy: 0.8697318007662835\n",
            "Sentiment ratio for this batch:  1.9993352890014648\n",
            "Sentiment ratio for this batch:  1.9993361234664917\n",
            "Sentiment ratio for this batch:  1.999337077140808\n",
            "Sentiment ratio for this batch:  1.9993382692337036\n",
            "Sentiment ratio for this batch:  1.9993388652801514\n",
            "Sentiment ratio for this batch:  1.9993394613265991\n",
            "Sentiment ratio for this batch:  1.999340295791626\n",
            "Sentiment ratio for this batch:  1.9993407726287842\n",
            "Sentiment ratio for this batch:  1.9993410110473633\n",
            "Sentiment ratio for this batch:  1.999341607093811\n",
            "< test epoch > Batch: 270, Loss: 0.17884290218353271, Accuracy: 0.8699261992619927\n",
            "Sentiment ratio for this batch:  1.9993420839309692\n",
            "Sentiment ratio for this batch:  1.9993425607681274\n",
            "Sentiment ratio for this batch:  1.999343991279602\n",
            "Sentiment ratio for this batch:  1.9993455410003662\n",
            "Sentiment ratio for this batch:  1.9993468523025513\n",
            "Sentiment ratio for this batch:  1.9993479251861572\n",
            "Sentiment ratio for this batch:  1.9993488788604736\n",
            "Sentiment ratio for this batch:  1.9993501901626587\n",
            "Sentiment ratio for this batch:  1.9993510246276855\n",
            "Sentiment ratio for this batch:  1.9993517398834229\n",
            "< test epoch > Batch: 280, Loss: 0.21516846120357513, Accuracy: 0.8707740213523132\n",
            "Sentiment ratio for this batch:  1.999351978302002\n",
            "Sentiment ratio for this batch:  1.9993523359298706\n",
            "Sentiment ratio for this batch:  1.999353051185608\n",
            "Sentiment ratio for this batch:  1.9993531703948975\n",
            "Sentiment ratio for this batch:  1.9993534088134766\n",
            "Sentiment ratio for this batch:  1.9993537664413452\n",
            "Sentiment ratio for this batch:  1.999355435371399\n",
            "Sentiment ratio for this batch:  1.99935781955719\n",
            "Sentiment ratio for this batch:  1.999359369277954\n",
            "Sentiment ratio for this batch:  1.9993616342544556\n",
            "< test epoch > Batch: 290, Loss: 0.3898455798625946, Accuracy: 0.8709192439862543\n",
            "Sentiment ratio for this batch:  1.999364972114563\n",
            "Sentiment ratio for this batch:  1.9993678331375122\n",
            "Sentiment ratio for this batch:  1.9993702173233032\n",
            "Sentiment ratio for this batch:  1.9993715286254883\n",
            "Sentiment ratio for this batch:  1.999372959136963\n",
            "Sentiment ratio for this batch:  1.9993740320205688\n",
            "Sentiment ratio for this batch:  1.999375343322754\n",
            "Sentiment ratio for this batch:  1.9993776082992554\n",
            "Sentiment ratio for this batch:  1.9993796348571777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [12:51<00:00, 257.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 finished with average loss: 0.3151939372221629, Accuracy: 0.871875\n",
            "Sentiment ratio for this batch:  1.9993816614151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Batch: 0, Loss: 0.5880629420280457, Accuracy: 0.75\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Epoch: 2, Batch: 10, Loss: 0.8388195037841797, Accuracy: 0.7784090909090909\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Epoch: 2, Batch: 20, Loss: 0.3928229808807373, Accuracy: 0.8154761904761905\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Epoch: 2, Batch: 30, Loss: 0.17847757041454315, Accuracy: 0.8366935483870968\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Epoch: 2, Batch: 40, Loss: 0.21817821264266968, Accuracy: 0.8323170731707317\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Epoch: 2, Batch: 50, Loss: 0.2513540983200073, Accuracy: 0.8321078431372549\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Epoch: 2, Batch: 60, Loss: 0.3594330847263336, Accuracy: 0.8381147540983607\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Epoch: 2, Batch: 70, Loss: 0.20729215443134308, Accuracy: 0.8327464788732394\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "Sentiment ratio for this batch:  1.9993816614151\n",
            "test iteration finished with average loss: 0.09965378635873397, Accuracy: 0.8325\n",
            "Model saved to ./my_finetuned_bert_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZsualEUMGMgq"
      }
    }
  ]
}